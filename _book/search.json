[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pkpd notes",
    "section": "",
    "text": "Preface\nThis page is from index page"
  },
  {
    "objectID": "PKFundamentals.html#pk-processes-adme",
    "href": "PKFundamentals.html#pk-processes-adme",
    "title": "1  Fundamentals of Pharmacokinetics- 103 OD",
    "section": "1.1 Pk processes: ADME",
    "text": "1.1 Pk processes: ADME\nADME Definitions:\nA: absorption; D: distribution, M: metabolism, E excretion\nElimination = metabolism + excretion\nDisposition = distribution + elimination\nADME describe the processes that a compound undergoes in a body and influences the performance of drug.\nElimination is another common word to describe the physiological processes of pharmacokinetics, which is a combination of metabolism and excretion.\n\n1.1.1 Absorption\nProcess by which drug moves from the site of administration to the systemic circulation.\nDrug can be delivered to the body in different ways and absotion process will take place for different rout of administration.\n\nOral absorption\nDermal absorption (skin)\nTopical absorption (ey, wond etc.)\nSubcutaneous absorption\nUnless the administration is IV, there is an absorption\n\nAbsorption has a rate (amount per time) and an extent (total amount) of absorption.\n\n1.1.1.1 Principle of absorption\nConcentration time profiles of both oral and IV administrition have same distribution, metabolism and excretion, but the difference is in absorption process.\nOral administartion has advantages of longer concentration coverage period and does not achive high concentration as IV that might be toxic for some person.\nConcentration time functions\nIntavenous equation:\n\\[\nC(t)= \\frac{Dose}{V}*e^{-k*t}\n\\] Extravascular equation:\n\\[\nC(t) = \\frac{F*D}{V}*(\\frac{k_a}{k_a - k})*(e^{-k*t}-e^{-k_a*t})\n\\] two new parameters, F and ka.\nAbsorption rate constant (ka) is first order rate constatn for drug absorption from the site of administration to the systemic circulation\n\nIntramascular follow first order absorption\nSubcauteneous often follow zero order absorption\n\n\n\n\n\n\n\n\n\n\n1.1.2 Distribution\nProcess by which drug moves from systemic circulation to other tissues in the body to take it’s action\nDrugs penetrate different tissues at different speeds depending on the unbound drug ability to cross membranes. This is an equilibrium process largly based on physiochemical characteristics of drug molecules and small extent on the transporters that exists in the body.\nDistribution occurs throughout the entire time that the drug is in the body. It is very difficult to quantify what distribution is, we can only say that the drug distribution is only small, average or large.\n\n\n\n\n\n\n\n\n\n\n1.1.3 Metabolism\nChemical or enzymatic transformation of parent drug to another chemical form (metabolite)\nMetabolites tend to be more polar and thus more water soluble, usually less active, which promotes excretion in the urine\nLiver is the primarily responsible for metabolism. Main enzymatic system is the Cytochrome P450 family of enzymes.\nSome metabolism also occurs in Kidney, intestine and lung\nWhen a drug is injected the body convert it into metabolites. This is the process of pharmacokinetics, also refer to clearance.\n\n\nConstant clearance is the result of first order kinetics.\n\n\n\n\n\n\n\n\n1.1.3.1 Hepatic metabolism\n\n\n\n\n\n\n\n\n\n\n\n1.1.4 Excretion\nProcess of removing drugs and metabolites from the body\nPrimarily occurs through the kidney (urinary excretion). The very polar and soluble metabolites end up in urine\nAlso, excreted though the lungs, saliva, breast milk, sweat, and bile\nElimination = metabolism + excretion (removal of active drug from the body)\n\n\n\n\n\n\n1.1.5 Pharmacokinetics and ADME\nPharmacokinetics encompasses all of ADME\nWhile ADME are discrete activities, they occur simultaneously\nImmediately after a molecule is absorbed, distribution, metabolism and excretion occurs\n\n1.1.5.1 Intravenous dosing\nADME processes can be observed from concentration time profile curves\nfor example, for Intravenous dosing\n\nNo absorption process, since the drug is already in blood\nDistribution is observed at early time points ( May be very rapid and not observed)\nElimination is observed at late time points (which encompasses metabolism + excretion)\n\n\n\n1.1.5.2 Extravascular dosing\n\nAbsorption predominates at early time points\nDistribution is difficult to observe before Cmax, but may be detected after Cmax\nElimination is observed at late time points (metabolism + excretion)\n\n\n\n\n1.1.6 IV administration\nPK principles for IV administration. IV administration can take many forms:\n\nBolus injection\nConstant intravenous infusion\nIntra-arterial administration (technically not IV, because drug is injected into an artery rather than a vein)\n\n\n1.1.6.1 Bolus injection\n\nDose is injected over 5-10 seconds\nEntire dose is avaialble to systemic circulation immediately\nUseful for acute conditoins that require drug immediately\nDistribution and elimination occur early\nOnly elimination occurs later.\nchanging dose reflects proportional change in plasma concentration, indicating linear kinetics.\n\n\n\n1.1.6.2 Rate of Elimination\nDrug elimination is an exponential decline\nIf we assume immediate distribution and equilibrium of drug throughout the body, only the exponential decline of the drug is observed. Equation for the elimination:Concentration at time is equal to some drug concentration at time zero times exponetial function of Clearance over Volume of distribution times t\n\\[\nC(t) = C(0)*e^{-\\frac{CL}{V}*t}\n\\] Concentration after the bolus dose:\n\\[\nC(0) = \\frac{Dose}{V}\n\\] V, the volume, is not a real volume in the body, it is a proportionality factor between the amount and concentration.\nIn this example, V is volume of distribution. But this is not the case if there was a noticible drug distribution phase.\nTaking natural logarithm of both sides:\n\\[\nln[C(t)] = ln[C(0)] - \\frac{CL}{V}*t\n\\] This equation is samilir to y = b + mx\nPlotting lnC(t) vs t will allow for linear regression to determine CL/V = k (elemination rate constant); Y-intercept is equal to C(0) = Dose/V\n\n\n1.1.6.3 Elimination rate constant\nThis is a secondary PK parameter and defined by the clearance and volume distribution.\nk can be estimated directly from the terminal elimination slope\nIn this simple case V = Vd. therefore,\n\\[\nk = \\frac{CL}{V_d}\n\\] Resulting slope of the regression is negative elemination rate constant. This is referred as k or ke or \\(\\lambda_z\\) depending on the method or model used to estimate the parameter.\n\n\n1.1.6.4 Half-life\n\\[\nln[C(t)] = ln[C(0)] - k*t \\\\\nln[\\frac{C(0)}{2}] = ln[C(0)] - k*t_{1/2} \\\\\nk*t_{1/2} = ln[2]\\\\\nt_{1/2} = \\frac{ln[2]}{k} = \\frac{0.693}{k}\n\\] Half-life and elimination rate is related to one another. t1/2 is derived from k, k is in turn derived from CL and V. Therefore, half-life is a tertiary parameter.\n\n\n1.1.6.5 Volume of Distribution\na proportionality factor between the amount of drug in the body and the concentration in the plasma/blood.\n\\[\nA(t) = V_d*C(t)\n\\] Vd is a hypothetical value\nPlasma = 3L, Extracellular water = 16 L, Total body water = 42 L\n\n\n1.1.6.6 Area under the curve\nMeasurement of total drug exposure over time. For IV bolus administration this can be defined by \\(AUC=\\frac{Dose}{CL}\\). CL is what defines the AUC, not the other way around. AUC is a secondary parameter.\nFor IV dosing and 1 compartment kinetics, \\(Dose = \\frac{C(0)}{V}\\) and \\(CL = \\frac{k}{V}\\). Therefore, \\(AUC = \\frac{C(0)}{k}\\)"
  },
  {
    "objectID": "NCAintroduction.html#observed-parameters",
    "href": "NCAintroduction.html#observed-parameters",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.1 Observed parameters",
    "text": "2.1 Observed parameters\nHere, we will explore concentration data and see how to obtain some of the parameters directly from the observed data.\nIt is important to note that NCA is not based on a model but is based on observed data and therefore some of the parameters are directly obtained from observations.\n\n2.1.1 Concentration:\namount of substance per unit volume\nUnits ng/mL, ug/mL, pg/mL, umol/L, nmol/L\nTerms used in concentration data in PK: LOQ, LLOQ, LOD, ULOQ or ULQ, Preision, Accuracy\nLOQ criteria: LOQ should be <3% (five half lives) of expected peak concentration. Highest conc will be Cmax and lowest will be 3% of Cmax.\nCmax: the maximum value observed in a set of data.\nTmax: The time of Cmax is the time axis was observed\nTlag: A lag time is the time of the last sample prior to the first sample with a measurable concentration. In IV administration there is no lag time observed because we are dosing directly into the vein. Lag times occur only with extra vascular administration. For example let’s imagine that we administer a drug in the form of a tablet after the tablet is swallowed it must dissolve in the stomach and then the dissolved drug must make its way to the small intestine to cross a membrane to reach the bloodstream\n\nCtrough: The trough concentration or Ctrough is the concentration measured at the end of a dose interval at steady state here you can see that the troughs occur at about 8 hours and 20 hours. These samples were collected just prior to administering the next dose therefore in this data set the Ctrough values are the concentrations measured at 8 and 20 hours which both appear to be about 10 ng/mL.\n\nCmin: a minimum concentration is the lowest concentration within a single dosing interval. it may be equal to the trough value but it may differ this example shows the case where a difference between the Cmin and Ctrough:\nThe dose was administered at 8 hours and you can see that the Ctough value is approximately 10 nanograms per milliliter but because there is a lag time in the absorption, the drug concentrations continue to fall after dosing at 9 hours. The concentration is fallen slightly to about 8 nanograms per milliliter after which the absorption process causes the concentration to increase therefore the minimum concentration is approximately 8 nanograms per milliliter but the trough concentration is 10 nanograms per milliliter. This is a very unique situation, it does not normally occur. Generally the minimum and trough concentrations are the same.\n\n\n\n2.1.2 Sampling Scheme\nthe following have significant effect on the observed parameters:\n\nthe times we choose to draw blood\nthe frequency of those blood draws\nthe scheduling of other events such as dosing relative to those blood draws\n\nhow the Cmax is affected by time point selection. The two graphs seen here contain the exact same time concentration data The only difference is in the blood sampling times. in the curve on the left we have more frequent sampling around the peak we have samples at 1 hour 1.5 hours 2 hours 3 hours and 4 hours this results in a c max of 12 nanograms per milliliter at a time of 2 hours The CMax\n\nYou can’t take blood samples at an infinite number of times invariably there’s going to be a space between two blood samples and the concentration will peak in the interval during which no blood is taken you want to ensure that you have frequent sampling around the expected time of c max to maximize your chance of observing near the peak concentration\neverything we discussed regarding c max also applies to T-Max because T-Max is defined as the time at which we observe c max\nthe Tlag is different in the two plots on the left t lag is at 1.5 hours and on the right it is at 1 hour you need to have sufficient sampling just after dosing to be able to determine if there is a lag time however lag time doesn’t occur for every drug and a very short lag time such as 10 to 15 minutes may not be relevant therefore you should select time points that will enable you to determine if there is a lag time on the time scale that is relevant for your drug Ttrough values are not really affected by the blood sample schedule as long as a sample is taken just prior to dose administration\n\nThe trough value is by definition the concentration just before the dose is administered usually it is appropriate to dose within 15 to 30 minutes of taking a trough sample if you wait much longer than 30 minutes you may not get a true trough however trying to dose too soon after collecting a trough sample may not be practical and it may lead to a cumbersome study design The minimum concentration can also be affected by the sampling schedule this can happen when there is a time lag in the absorption process if you collect a sample after dosing but within the time lag you may see a decrease in concentration after dosing\nthese plots illustrate this phenomenon on the left the doses administered at 8 hours with sample collection at 1 hour intervals because of the time lag the concentration continues to decrease and at 1 hour post dose we collect a sample with a lower concentration than the previous sample at the dose time if we do not collect a sample within 1 hour after dosing we will not observe the decrease in concentration process causes the concentration time at 8 hours keep in mind that understanding lag time from your single dose data will help you design a proper sampling scheme for your minimum concentration samples. In conclusion remember that because you’re observed parameters are taken directly from concentration time data nearly all of these parameters are dependent on the sampling scheme intensive sampling provides more accurate estimates for observed parameters"
  },
  {
    "objectID": "NCAintroduction.html#half-life-t12",
    "href": "NCAintroduction.html#half-life-t12",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.2 Half-life (t1/2)",
    "text": "2.2 Half-life (t1/2)\nThe time required for the observed concentration usually in plasma to decrease by 50%.\nlet’s take a look at half-life graphically. here we have a plot of the plasma concentration on the y-axis and time on the x-axis both axes are on a linear scale. in this plot on the figure we have indicated the concentration of 80 nanograms per milliliter with a dashed line it takes one half life for the concentration to drop to 40 nanograms per milliliter and then a second half life for to drop to 20 nanograms per milliliter\n\n\nThe half-life in this case is approximately 1 hour no matter where we start on the curve approximately an hour later the concentration will be cut in half here is the same data plotted on a semi logarithmic plot with the plasma concentration on a log scale. notice that on this scale it becomes a linear relationship which makes it very easy to determine the half-life here you see the same concentrations that we saw on the linear scale per milliliter. The time elapsed in each of these intervals. How do we determine the half-life in pharmacokinetics. First we assume there is a first order elimination during the terminal phase another words the change in concentration with respect to time can be represented by single term.\n\nThis how half-life in the elimination rate are related to one another if you have the elimination rate constant you can get the half-life and the inverse is also true if you have the half-life you can get the elimination rate constant\nwe are making an important assumption that we have first order elimination during the terminal phase generally this is a reasonable assumption But if you have not reached the terminal phase you are estimating a half-life this relationship may not hold The half-life can affect plasma concentration curb significantly Shorter the half life faster the drug is eliminated Concentration time profiles we can see here that the initial portion of this profile has one slope and then the terminal phase has a separate slope phasic because there are two different processes that are occurring resulting in two different slopes eliminated\n\nAfter one half life 50% drag its eliminated after two half-life 75% drug eliminated and after three half-life 88% druggies eliminated Is considered normally after 5 and 7 half-lives considered complete elimination Half-life can be used to estimate washout time needed\n\n\nHalf-life is the time required for a 50% reduction in the concentration of drug in the body\nHalf-life is related to elimination rate constant for first order elimination only\nHalf-life can be estimated from concentration time profiles\nComplete eliminatino (>95%) requires 5 half-lives\n\nMay not be a good estimate of the true concentration peak\n\n2.2.1 General Guidance\n\nwhen calculating half-life, plasma data is most often used however blood, urine or serum data can also be used to calculate terminal half-life values.\nWe will concentrate on plasma data in this lecture and we will consider urine data in the next lecture\neven when we observe multiple phases in the time concentration curve we only calculate the terminal or last half-life when doing non-compartmental analysis\nwe also assume first order elimination during the terminal elimination phase\n\nPhase should be considered and we need to transform that data by taking the log of the concentration second a minimum of three data points is required points should be\n\nAll concentration data in the terminal phase should be used (log-transformed)\nA minimum of 3 data points is required\nThe last measurable concentration should be used unless it is greater than the previous concentration measurement\nThe maximum observed concentration Cmax should not be used\nValues below BLQ should not be used\n\nExample:\nFirst plot the concentration time data on a semi-log scale and visually identify the terminal phase then perform a linear regression in the last three time points on your semilog plot. The slope is equal to the negative of the elimination rate constant. The intercept is not needed. r2 is the goodness of fit and values closer to 1 are better. finally we can use the equation of rate constant\n\nwe will plot the data using a log scale for the y axis as you see here. In this plot we need to identify the points that we will use to calculate the terminal elimination rate it looks like the terminal phase encompasses all but the first two data points which might be a different phase but generally it looks like a straight line from about 2 hours through 18 hours post dose In this first example we will simply use the last three points\nRegression we get a slope of negative 0.0949 and an r squared value of 0.930 which means that the line can explain 93% of the variability of the y-axis values. Remember that the elimination rate constant is the negative of the slope and therefore the rate constant is 0.949 hours to the -1 and then using the equation we saw previously we can calculate the half-life which turns out to be 7.3 hours. After second half life it approximately 15 hours the concentration should only be 25% of its original value And now on the log scale we can see that there appear to be two phases first a distribution phase for about the first 3 hours followed by an elimination phase from about 4 hours to 24 hours we will focus on the terminal portion and we will not use any points from the earlier distribution phase.\n\nAlthough there may be as many as 6 points available in the terminal phase in this example we will just use the last three points for deciding on the optimal number of points to use in the fit but for now we’re just going to use the last 3 points again we determine the slope by linear regression and get a slope of negative 0.07551 the r squared value is 999 indicating a high degree of correlation and explaining the variability we can determine the elimination rate constant and from that calculate the half-life which is 9.40 oral administration here you can see that the drug concentration increases over time\nTerminal elimination phase which appears to be between about two and eight hours. again we will use just the last three points although it does appear that there are more points we could consider using linear regression using the last 3 points allows us to determine the slope of negative 749 with an r squared value of 0.990 therefore the terminal elimination rate constant is 0.749 hours to the -1 and the half-life is 93 hours this is a very short half-life but it does appear to be correct because the concentration decreases from about 150 in about an hour\n\n\n2.2.2 Number of data points to use\n\nIn all of our examples we use just three points but perhaps we should select four or five or even more data points to get a better terminal elimination rate constant estimate. We will use:\n\nan iterative process where you start with the fewest number of data points to the maximum number of data points allowed up to but not including Cmax.\nWe make pairwise comparisons and select for the largest value of r2 or adjusted r2 . The r2 is the correlation coefficient from a linear regression and it shows how much variability is explained by the linear regression. An adjusted r2 gives a small penalty for adding data points, r2 tends to increase as you add data points so this penalty offsets that natural increase.\n\nThe equation for adjusted r2 is shown here where r squared is the correlation coefficient and n is the number of data points used in the linear fit. The adjusted r2 will always be slightly smaller than the r2 value although the difference gets smaller as more points are used.\nThe minimum acceptable value of r squared or adjusted r squared is 0.60. some companies have SOPs where they require even larger r squared values but generally 0.6 is an appropriate limit. Below this value you can’t have high confidence in your estimate of the terminal elimination rate constant\nIn conclusion here are a few things you should remember from this lecture\n\nthe terminal half life is calculated from the elimination rate constant and the elimination rate constant can be estimated from concentration time data\nYou can follow the basic guidelines we presented using linear regression of semi-logarithmic data\nyou want to use as much data as possible while maintaining the best correlation which is why we use the adjusted r squared to determine the number of points to use\none last note about the slope fit ideally you would like to be able to use two to three half-lives of data in the terminal phase to get a reliable estimate of the terminal slope in the terminal phase you will still be able to calculate a slope but the reliability of your estimate will decrease\n\n\n\n2.2.3 Half-life of Urine data\nWhen determining half-life from urine data:\n\nAs in plasma only the terminal or last half-life is calculated\nAs in plasma we assume first order elimination during the terminal phase\nWhat is different about your data is that we determine the renal elimination rate which we assume to be proportional to plasma concentration. That assumption is usually true although in rare cases you may get an inaccurate result if it is not proportional.\n\nlet’s consider the equations involved in renal elimination on the left we can describe the renal elimination rate in terms of the renal clearance and the plasma concentrations. One important difference between plasma and urine data is that plasma concentrations are determined at discreet points in time but urine data is collected over a time interval.\n\nWe can determine the renal elimination rate during the collection interval using the equation on the right where Vur is the volume of urine Cur is the concentration of the drug in the urine and delta t is the change in time.\nthe method that we will use is as follows\n\nfirst we calculate the renal elimination rate for each urine collection interval using the equation.\nwe plot the renal elimination rate versus the midpoint of the collection interval on a semi-logarithmic scale\nwe perform linear regression to determine the slope r squared and the half-life and as with plasma data we don’t need the intercept.\n\nlet’s take a look at an example of the renal elimination rate on this plot the y-axis is the renal elimination rate and milligrams per hour and the x-axis is the midpoint of the collection interval\n\nnotice that the intervals are about 2 hours apart at the beginning and then as long as 12 hours apart at the later times as with plasma data the semi-log plot is more useful let’s take a look at that one\n\nhere to help us visualize the data we have plotted the intervals as a series of steps and the midpoint of each interval is plotted with a diamond symbol The y-axis again is the renal elimination rate but now it is on a log scale if we perform a linear regression using the last three points we get a slope of minus 0.146 and an r squared value of 0.999 the rate constant for renal elimination is then the negative of the slope or 0.146 hours 1 and from that we calculate a half-life before 7 hours The linear regression for urine data is done the same way as with plasma data but the difference is that we plotted the renal elimination rate in the concentration on the and on the x-axis we plot the midpoint of the interval.\nThe rate constantly determine is the average renal elimination rate One question you might be asking is what else can we determine using the urine data Just as in plasma data we can estimate to wash out time which is given by the formula shown here wherein is the number of half-lives\n\nsame as plasma data issued expect 95% elimination within five have lives Finally remember that the half-life only predicts the time to steady state but it does not predict the plasma concentrations or the renal elimination rates that will be observed there are ways of predicting concentrations at steady state but those concentrations also depend on the dose amount in the interval of dosing\nIn conclusion the two major points of this lecture have been that\n\nthe terminal half-life can be estimated from urine data using the renal elimination rate\nhalf-life is primarily useful for estimating washout time and the time to achieve steady state this concludes the topic of half-life next let’s take a moment to review what you’ve learned about half-life in these three lectures and in the corresponding exercise"
  },
  {
    "objectID": "NCAintroduction.html#area-under-the-curve-or-auc",
    "href": "NCAintroduction.html#area-under-the-curve-or-auc",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.3 Area Under the Curve or AUC",
    "text": "2.3 Area Under the Curve or AUC\nArea under the curve or AUC is the total exposure to drug over time and can be calculated as the integral of all concentrations across all time points.\nMathematically it is expressed as shown in the equation. The AUC from 0 to infinity is the integral from time zero to infinity of the concentration time curve multiplied by dT which is the change in time.\nAUC can be calculated directly from concentration time profiles using simple algebra based on addition and multiplication. AUC can be calculated for any time interval for which concentrations are measurable those are called partial areas and we will talk about those a little bit later.\nHere’s an example of the area under the curve we have a set of plasma concentration time data shown here from time zero peeking about 1 hour post dose and then declining down to 6 hours post dose\n\nThe area under the curve is literally the area under that plasma concentration time curve as indicated here once the AUC has been calculated that number can be used to make comparisons in drug exposure. Next we are going to go through some mathematical derivations and we will end up with some equations that you can use to calculate the AUC. You don’t need to know the details of this derivation to calculate in AUC equations that result from it. But rather than just give you the end equations we will show you how they are derived. The first equation shows that the full AUC from 0 to infinity can be defined by two parts the AUC from 0 to t and the AUC from t to infinity or t is the time of the last measurable concentration. You can see here that AUC is additive so you can slice it up into pieces and add them back together to get the hole\nThe second equation\nSo you can slice it up into pieces and add them back together again to get the hole. The second equation shows that the AUC from 0 to T2 is the AUC from 0 to 1 + the AUC from t1 to T2 where t1 is a point somewhere between 0 and T2. You can divide your au season to pieces that are small as you want as long as you have. David to support that mathematically the AUC from time zero to time in is the integral from time zero to time n of the concentration as a function of time multiplied by DT which is the increment of time slices\nYou should also mention that the concentration at times 0 is a special case for oral administration. the initial concentration is assumed to be zero and for IV bolus administration the initial concentration is back extrapolated to zero time using the log linear fit. here is a visual illustration of this process using the example data set we calculate the area of each slice from data point to data point and then add them together to get the total AUC.\nNotice that this gives us the area from time zero up to our last observed concentration which occurs at 6 hours in this data set. in a later lecture we will see how to extrapolate to account for the AUC from the last observation to infinite time.\nsummary\n\nto calculate the AUC you determine the area of individual trapezoids using the adjacent concentration and time points and then you add the areas together to get the total AUC up to the last observation\nhow do you calculate the area of a trapezoid it just takes some There are two methods to interpolate between concentration for the calculation of area. The linear interpolation method and the log linear interpolation method. The linear interpolation method assumes that the intermediate values between two observed data points lie along a straight line. The log linear interpolation method assumes that there is a log relationship on the y-ax. This makes sense for decreasing concentration because we know that concentrations do not fall in the linear fashion but they do appear linear when viewed on the lag scale.\n\nYou’re a true options that are commonly used\nThe first is called linear up logged down when concentrations are increasing we use linear interpolation and when they are decreasing we use log linear interpolation\nThe second method uses linear interpolation only this is typically used in generic drug analysis ensuring all the data has been processed identically therefore the choice of method may be dictated by regulatory agencies and/or company policies so make sure you use an appropriate choice here are the equations that you will use to calculate AUC’s using the two methods.\n\n\n2.3.1 Extrapolation to infinity\nSo far we have only determined AUC up to the time of the last observation now we will consider how to extrapolate to determine the AUC from 0 to infinity\nWe have seen how to obtain the AUC from 0 to 6 hours using the trapezoid method, however we have left out an important part of the AUC calculation. When we stopped collecting data at 6 hours there was still a measurable concentration and therefore there is a contribution to AUC from 6 hours onward that we need food in our calculations\nremember that AUCs are additive so that we can subdivide the AUC calculation into different parts The full AUC from 0 to infinity is equal to the AUC from 0 to time t plus the AUC from time t to infinity or t is the time of the last measurable concentration also remember that we can subdivide an AUC as desired the AUC from 0 to T2 can be subdivided as the AUC from 0 to t1 plus the AUC from t1 to T2 where t1 is between 0 and T2 we have used this principle to subdivide the AUC into separate slices from 0 to the last observation as you see here in the yellow trapezoids The piece that we still need to determine is the red triangle which represents the AUC from 6 hours to infinity\n\n\nso all we need is an equation that will enable us to calculate the AUC from the last observation to infinity and by adding it to the AUC we obtained from the trapezoidal slices we can get the total AUC let’s see how we can drive an equation to calculate the AUC of that triangle consider the decline of concentration from the time of our last concentration measurement to infinity it is important to note that we are assuming that at t last absorption is complete and elimination is following first order kinetics The AUC from t last to infinity is represented by the integral from t last to infinity of c last important to know that in this equation the value of t will not be the time post dose but it is the time elapsed after t last therefore in this equation t ranges from 0 to infinity The integrated form of this equation is shown here to the power of\nThe term with zero drops out and therefore this big equation simplifies to see last divided by the elimination rate constant so now we have an equation we can use to determine the AUC of the extrapolation from t last to infinity as represented by the red triangle it is simply the concentration of the last observation c last divided by the elimination rate constant k e l all you will need to remember is this last equation The derivation is good to know because you can see what assumptions went into it mainly that absorption is complete and elimination kinetics are first order but the equation here is the only thing you need to know so to summarize to extrapolate to infinity all you need to know is the last observed concentration c at t last and the elimination rate constant k e l and you can determine the AUC from t last to infinity by doing a simple division\n\nfinally let us consider the confidence that we can place in our extrapolation we assess this by calculating the ratio of the extrapolated AUC to the total AUC if this ratio is less than 0.15 or 15% then the sampling interval from 0 to t covers at least 85% of the exposure and we can have good confidence in the extrapolation conversely if the ratio is greater than therefore the extrapolated AUC is more than 30% of the total then the sampling interval is probably not sufficient to estimate the total exposure now if the extrapolation percentages adequately will depend on the goals of the study and different criteria may be chosen depending on the application\n\nin conclusion we have seen how we can calculate the extrapolated AUC using the elimination rate constant and the last observed concentration the total AUC is simply the sum of the AUC up to the last observation we also saw that extrapolated AUC values that are greater than 30% of the total AUC are probably unreliable but with less than 15% extrapolation would be considered good anything between 15 and 30% may or may not be considered adequate depending on the application this concludes the second lecture\n\n\n\n2.3.2 AUC to make decision\nIn the third lecture on AUC we will see how to make decisions based on comparisons of drug exposure\nThe first thing that we should remember is that AUC is a measure of exposure to drug over time therefore AUC’s are used to make comparisons of formulations or to compare exposure under different conditions here are some applications that make use of AUC comparisons\n\ntwo different formulations of a drug\nthe effects of food on drug absorption\ndrug drug interactions\nthe effect of demographics on the PK of a drug\n\n\n2.3.2.1 Bioavailability\nbioavailability is defined as the fraction of administered drug that reaches systemic circulation. The bioavailability of a drug is the result of many processes. Let’s consider what happens when a drug is admistered and it moves down the digestive tract a fraction of the drug is absorbed in the small intestine and travels through the portal vein to the liver. The remainder of the drug that is not absorbed continues through the digestive tract and is eliminated in the feces, in the liver the drug is subjected to what is called first past metabolism in which the drug is chemically modified removing some of the drug before it reaches systemic circulation.\nThe portion of the drug that has not been metabolized enters the bloodstream.\nonce the drug has reached systemic circulation in the bloodstream it can then be eliminated by excretion through the kidneys and or further metabolism in the liver some drugs are eliminated via other organs that are not pictured here such as the lungs liver and/or other organs is what is responsible for the decline in the plasma concentration over time therefore the bioavailability is a single fraction that describes the many complex phenomena that lead to the drug reaching systemic circulation.\n\nIt has a possible range from 0 to 1 where a value of zero would mean that none of the administered systemic circulation and a value of one would mean that 100% of the dose reaches systemic circulation in contrast to oral administration when we administer a drug by IV either as a bolus or as an infusion 100% of the drug is introduced directly into systemic circulation and therefore the bioavailability is exactly one now that we have defined bioavailability\n\n\n2.3.2.2 Relation between AUC, Clearance and dose\nHow bioavailabity is related to the AUC, dose and clearance. AUC is a function of bioavailability, dose and clearance:\n\\[AUC = \\frac{F*Dose}{CL}\\]\nlet’s consider how we can use this equation to compare drug exposure\n\nif both dose and clearance are constant then dose is proportional to the bioavailability which is represented by F \\(AUC \\propto F\\)\nif F and clearance are constant then AUC will be proportional to dose \\(AUC \\propto Dose\\)\nand finally if dose and bioavailability or constant then AUC is inversely proportional to clearance \\(AUC \\propto \\frac{1}{CL}\\)\n\n\n\n2.3.2.3 Determinatino of Bioavailability\n\nThe equations for bioavailablity for both oral and IV administration is given above.\nif we divide these two equations by each other and do some algebra to solve for F we get the relationship shown here where F is equal to the AUC for oral administration times the IV dose divided by the AUCIV times the oral dose usually we can assume that the clearance following oral administration will be the same as the clearance following IV administration and therefore the clearance terms drop out.\nExample:\n\nWe administer the drug to the same subject by both IV and PO and then measure the plasma concentration over time we can use different doses for IV and PO but in this case we had doses of 5 mg per kilogram for both. We obtained the AUC values and then calculate the bioavailability. In the case shown in the plots here the AUC following oral administration was 47,000 and the AUC following IV administration was 133,000 both doses were five therefore the bioavailability for the subject was determined to be 0.352. The bioavailability can be determined for all subjects in the same way when comparing AUCs\nA few things to remember when comparing ACUs:\n\nCrossover studies have lower variability because the same individual receives both treatments.\nThe ratios of AUC are useful in drug studies. These are normally expressed as the AUC of the test divided by that of the reference. They are sometimes expressed as a percentage\nAUC is log-normally distributed. Log transform should be used before doing statistical tests and geometric mean is a better measure than arithmetic mean\n\nHere are some examples used to establish whether\ntwo formulations are equivalent in the rate and extent of the drug reaching the site of action In that case we compare the ratios of the AUCs of the two formulations and a similar comparison is done with the c max values\nfor a food effect study you might compare the AUC of the Fed arm to the AUC of the fasted arm for drug interactions you might compare the AUC for the drug plus the inhibitor to the AUC of the drug alone and for hepatic impairment you would compare the AUC with impairment to the AUC of subjects with normal hepatic function in each of these cases the denominator contains the reference The reference is the standard to which you are comparing the test notice that the test is in the numerator in each of these in bile equivalents the test is the test formulation in food effect the test is the fed state in drug interaction the test is the drug combination and in hepatic impairment the test is the impaired individuals\n\n\n\n2.3.3 how to interpret changes in AUC\nremember that AUC is directly proportional to bioavailability and inversely proportional to clearance\nif a u c increases either bioavailability has increased or clearance has decreased\nif AUC decreases then we have the reverse situation either bioavailability is decreased or clearance has increased\nso what constitutes a significant change? In AUC changes less than 20% are not considered clinically significant but changes greater than 50% are considered clinically significant in most situations.\n\n2.3.3.1 AUC and Dose\nChange in AUC should also be proportional to dose if clearance is constant that means that doubling the dose should double the AUC. You can test for proportionality using an exponential model:\n\\[\nAUC = \\alpha*Dose^\\beta\n\\]\nif the confidence interval for beta includes 1, then dose proportionality can be concluded.\nSummary\n\nwe have seen how AUC comparison should use ratios. We considered examples of bioequivalence food effects hepatic impairment and drug interactions\nWe can determine bioavailability by comparing AUC for extra vascular and IV administration and third\nAUC can also be used to evaluate dose proportionality using the power model"
  },
  {
    "objectID": "NCAintroduction.html#clearance",
    "href": "NCAintroduction.html#clearance",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.4 Clearance",
    "text": "2.4 Clearance\nClearance is how quickly the body removes drug from the bloodstream. It is also a proportionality factor between the plasma concentration and the rate of elimination. We can see in the equation that the rate of elimination is equal to the clearance times the concentration of drug is a function of time. It is expressed in units of volume divided by time (example, liters per hour).\n\\[Rate\\, of\\, Elimination =CL * C(t)\\] In most cases clearance is constant even though the concentration and rate both change over time. Clearance includes both metabolism and elimination.\nMetabolism is the conversion of drug from one chemical species to another.\nElimination or removal from the body through one of several organs that eliminates the compound through the body.\n\n2.4.1 Calculating CL\nThe rate of elimination is the change in amount (dA) divided by the change in time. From the definition of CL, the clearance is a proportionality factor between the concentration and rate of elimination.\n\\[\n\\int_0^{\\infty}dA = CL * \\int_0^{\\infty} C(t) *dt\n\\]\n\\[\nF * Dose = CL * AUC\n\\] The final equation to calculate CL:\n\\[\n\\frac{CL}{F} = \\frac{Dose}{AUC}\n\\]\n\n\n2.4.2 Clearance vs Apparent Clearance\n\nClearance can only be determined following an IV dose where F = 1 or if F is known from a previous IV dose.\nApparent clearance (CL/F) is the clearance adjusted for the bioavailability\nThe CL/F can be determined for any route of administration but is specific to that route of administration.\nThe CL/F cannot be extrapolated to other routes of administration or even across species. For example, because the bioavailability for oral administration will be different than the bioavailability for subcutaneous administration each route of administration will result in different CL/F.\n\n\n\n2.4.3 Clearance is additive\n\nClearance is an organ specific effect and individual organ clearances can be some together to provide overall drug clearance, as you can see in the equation\n\n\\[\nCL_{total} = CL_{hepatic} + CL_{renal} + CL_{others}\n\\]\n\nThe total body clearance is equal to the sum of clearance of the liver or hepatic clearance plus renal clearance plus other organ clearance. You can add those together to get the total clearance of the drug from the body making it very easy to work with clearances\n\n\n\n2.4.4 Calculating renal clearance\nAlthough it is not easy to calculate organ specific clearances, the renal clearance is much easier to determine.\n\nWe can determine the exact amount of drug that comes out in the urine and thus how much drug was eliminated by the kidneys.\nThe total amount of drug excreted in urine (Ae) can be determined by collecting urine for a long enough period of time and measuring the concentration of urine and those intervals then multiplying the concentration times the volume to get that amount.\nDivide the amount excreted, Ae, in the urine by the AUC in plasma that gives you the renal clearance or that portion of the drug that was cleared by the kidneys. The equation that describes that is shown here \\[\nCL_r = \\frac{Ae}{AUC}\n\\]\n\n\n\n2.4.5 Volume of distribution\nAnother term that we use in assessing the pharmacokinetics of drugs is the volume of distribution.\nThe volume of distribution is the apparent volume into which the drug is distributed.\nIt is the proportionality factor between the total drug in the body and the plasma concentration\nExpressed by an equation, dose divided by the volume of distribution is equal to the concentration in plasma\n\\[\nC_p = \\frac{Dose}{V_d}\n\\]Because we know the dose and we measure the plasma concentration it is a simple matter to calculate the volume of distribution.\nLet’s take a look at two examples in both cases we administer a dose of 1 mg which here we express is 1,000 micrograms on the left we determine a plasma concentration of 100 micrograms per liter and on the right the plasma concentration is 10 micrograms per liter solving the equation we get a volume of 10 liters to realize that the volume of distribution is not a real volume on average the human body only has approximately 5 l of whole blood and approximately half of that volume is plasma.\nFor a drug that is largely unbound, the volume of distribution may be as small as a few liters but when the drug is strongly bound to proteins in the blood the very small plasma concentration results in a volume of distribution as high as thousands of liters.\nSo even though the volume of distribution is not a real volume, it is a very useful parameter. Although this equation defines the volume of distribution in practice we use a different method to calculate it. We will see in another video.\n\n\n2.4.6 CL and other parameters\nYou can use clearance to estimate other parameters, for example:\nthe elimination rate constant is clearance divided by the volume of distribution\n\\[\nk = \\frac{CL}{V_d}\n\\] volume of distribution can be determined by dividing clearance by the elimination rate constant or \\(\\lambda_z\\)\n\\[\nV_z = \\frac{CL}{\\lambda_z}\n\\]\nhalf-life can be determined from volume and clearance the half-life is the natural log of two times the volume of distribution divided by the clearance this is also equal to the natural log of two divided by the elimination rate constant\n\\[\nt_{1/2} = \\frac{ln2 * V_z}{CL}\n\\]\n\n\n2.4.7 Clearance and Organs\nWe will consider how drugs are metabolized by the liver and see how metabolism affects the clearance. Here we see a diagram showing how drugs are eliminated via extraction in the kidneys and metabolism in the liver.\n\nRemember that some drugs are eliminated by other organs that are not pictured here such as the lungs. The total clearance is the sum of the individual organ clearances. We have already seen how to calculate the clearance from the dose and the AUC, so it is straight forward to obtain clearance values from the data using non-compartmental analysis however to be able to interpret the clearance values we obtain from NCA, especially cases where we observe changes in clearance under different conditions, we need to look at how these organs extract drug from circulation.\nOf all these organs that remove drug from circulation the most complicated is the liver. In this lecture we are going to concentrate on how the liver affects clearance values that we obtain from NCA.\n\n\n2.4.8 Clearance and Liver\n\n2.4.8.1 Rate of Extraction\nwe can think about clearance as the ability of an organ to extract drug from circulation.\n\nIn the middle of the screen is the picture of a liver. In the left is the artarial side and on the right is the venous side. The blood flow in the liver is given by Q. The rate of drug entry is given by Q times the arterial concentration. The rate that the drug returns to circulation is given by Q times the venous concentration. The rate of extraction is the difference between these two. Factoring out Q gives us the equation shown here where the rate of extraction is Q times the difference in two concentrations. When the drug is extracted by the liver, we see a decrease in concentration. Therefore the larger the concentration difference the greater the rate of extraction.\n\n\n2.4.8.2 Extraction ratio\n\\[\nE = \\frac{Rate\\,of\\,extraction}{Rate\\,of\\,drug\\,entry} = \\frac{Q*(C_{Arterial}-C_{Venous})}{C_{Arterial}} = \\frac{C_{Arterrial}-C_{Venous}}{C_{Arterial}}\n\\]\nYou can also calculate something called an extraction ratio which is the rate of extraction divided by the rate of drug entry. The rate of extraction we can replace with Q times the difference in concentration and the rate of drug entry can be replaced with Q times their arterial concentration. The arterial blood flow Q cancels out therefore the extraction ratio can be determined directly from the two different concentrations as shown here.\n\n\n2.4.8.3 Clearance and E\nNow, clearance is equal to the rate of extraction divided by the arterial concentration.\n\\[\nCL = \\frac{Rate \\, of \\, extraction}{Arterial \\, concentration} = \\frac{Q*(C_{Arterial}-C_{Venous}}{C_{Arterial}}\n\\]\n\\[\nCL = Q*E\n\\tag{2.1}\\]\n\\[\nE = \\frac{CL}{Q}\n\\]\nwe just determined that the rate of extraction is equal to q times the concentration difference so this gives us the first equation here we can substitute the extraction ratio for the concentration terms and therefore the clearance is the blood flow times the extraction ratio\n\nThe extraction ratio can take any value from 0 to 1 where 0 represents no extraction and 1 represents complete extraction.\nLet’s consider two extremes, as the extraction ratio approaches 1, clearance is equivalent to blood flow and as it approaches zero clearances completely independent of blood flow.\n\nextraction ratio helps us understand where the elimination is occurring. Some drugs are cleared almost entirely by metabolism in the liver other drugs are not metabolized but are largely cleared through other organs such as the kidneys. For a high extraction ratio drug where the extraction ratio is greater than about 7 the organ efficiently removes drug from the body and clearance can be approximated by blood flow. If blood flow changes clearance will change in the same direction so for a high extraction ratio drug if blood flow decreases the clearance will also decrease.\nDrugs that have low extraction ratios less than 3 have organs that are not very efficient at removing the drug from the body so the blood keeps passing through and a very small amount of the drug is actually cleared by the organ. Clearance then becomes independent of blood flow regardless of whether blood flow increases or decreases. It simply won’t change the clearance very much. In that case the organ function determines the clearance as opposed to blood flow.\n\n\n2.4.8.4 Plasma and Blood Clearance\n\nNormally clearance is based on plasma concentration data and this results in a plasma clearance value.\nHowever when you look at organ clearance you should base it on whole blood concentration data. To do that you need to convert it using the ratio of drug concentrations in plasma and blood. The clearance in whole blood is equal to the concentration in plasma divided by the concentration in blood times the plasma clearance. The ratio of the plasma concentration to the concentration in whole blood is called the fraction unbound and it is given by the symbol f u.\n\n\\[\nCL_{blood} = \\frac{C_{plasma}}{C_{blood}}*CL_{plasma}\n\\] \\[\nf_u = \\frac{C_{plasma}}{C_{blood}}\n\\]Intrinsic Clearance\nIntrinsic clearance is a measure of the metabolic activity of the liver. It tells you the efficiency of the liver at metabolizing drug. Extraction ratio for the liver is shown below\n\\[\nE_H = \\frac{f_u*CL_{int}}{Q_H + f_u * CL_{int}}\n\\]\nwhere EH is equal to the fraction unbound times the intrinsic clearance divided by the blood flow plus the fraction unbound times the intrinsic clearance. We know that the hepatic clearance is equal to the hepatic blood flow times the hepatic extraction ratio.\n\\[\nCL_H = Q_H * E_H\n\\]\nSo if we put those together blood flow times the ratio fraction unbound times the intrinsic clearance divided by the blood flow plus the fraction unbound times entrance clearance.\n\\[\nCL_H = Q_H * \\frac{f_u*CL_{int}}{Q_H + f_u * CL_{int}}\n\\]\nThere are two conditions that help simplify this equation:\n\n\n\n\n\n\n\n\nCondition\nResult\nApproximation\n\n\n\n\n\\(f_u*CL_{int} >> Q_H\\)\n\\(E_H \\rightarrow 1\\)\n\\(CL_H \\approx Q_H\\)\n\n\n\\(f_u * CL_{ing}<<Q_H\\)\n\\(E_H \\rightarrow 0\\)\n\\(CL_H \\approx f_u*CL{int}\\)\n\n\n\n\n\n\n\n\n\nIf the fraction unbound times the intrinsic clearance is much greater than the blood flow then the extraction ratio approaches 1 and the total clearance can be approximated by the blood flow to the liver\nIf The fraction unbound times the intrinsic clearance as much smaller than the blood flow, then the extraction ratio approaches zero and the hepatic clearance is approximately equal to fu times intrensic clearance.\n\nHow to interpret changes in clearance?\nDecreases in clearance suggest one of two things:\n\nMetabolic inhibition or the method of metabolizing the drug has been inhibited in some way.\norgan dysfunction if the organ simply is not working as well as it should you may see decreases in clearance\n\nIncreases in clearance suggest metabolic of regulation. For example, if your drug induces a cytochrome P450 that metabolizes your drug you may see changes in clearance over times with repeated dosing as the enzyme upregulates to start metabolizing the drug.\nClearance is derived from both dose and AUC.\nkeep in mind that comparisons of AUC are more accurate than comparisons of clearance. In theory, it should not make a difference but what we find is that AUC comparisons are more accurate. Because you are not introducing another potential variable dose which may vary slightly from person to person. Not only can the dose very slightly but the bioavailability can change from person to person as well. Remember that AUC is inversely proportional to clearance meaning that as AUC increases clearance decreases and as AUC decreases clearance increases.\n\n\n\n2.4.9 Conclusion\n\nthe extraction ratio represents the ability of an organ to remove drug from the body which means that if you have high extraction you have efficient removal if you have lower extraction it’s not efficient\nplasma and blood clearances are related by the ratio of plasma and blood drug concentrations this gives you a way to convert between plasma and blood clearances\nintrinsic clearance is really a measure of the ability of the liver to metabolize drugs\nclearance should be evaluated with the area under the curve as we describe previously"
  },
  {
    "objectID": "NCAintroduction.html#statistical-moments",
    "href": "NCAintroduction.html#statistical-moments",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.5 Statistical moments",
    "text": "2.5 Statistical moments\nStatistical moments are quantitative measures of the shape of a set of data points and they are used to describe probability distribution curves.\nFor example, here we have a probability distribution curve that is a normal distribution and we have a couple of moments or quantitative measures that describe the set of data. The first one is the mean given by the red line and a second moment is the width given by the green line so if you know the mean and the width and a certain place you could reproduce the curves.\n\nThese moments here describe the mean and the standard deviation for a normal distribution\n\n2.5.1 Statistical moments and PK\nIn pharmacokinetics, you can also have statistical moments. a statistical moment in pharmacokinetics is an alternative description of pharmacokinetic data. Normally we plot plasma concentration as a function of time but in this plot the residents time is on the x-axis and the relative frequency is on the y-axis. Therefore, the height is proportional to the number of molecules at a particular residence time.\n\nIn this plot, we can see that the distribution is different from the normal distribution. You still have a mean, the mean residence time, indicated with the red line but the distribution is no longer symmetrical. The peak is shifted to the left of the MRT line relative to the normal distribution and there is a long tail. As the residence time increases so a small number of molecules spend a very long time in the body.\n\n\n2.5.2 Half-life and MRT\nRemember that half-life is the time required to eliminate 50% of the drug molecules. Mean residence time is the average amount of time that a single molecule remains in the body. If all molecules enter the body at the same time, for example in an IV bolus dose, then the mean residence time is equal to the sum of the number of molecules (Ni) times the residence time (ti) divided by the number of molecules, so a simple average.\n\\[\nMRT = \\frac{\\sum (N_i*t_i)}{\\sum N_i}\n\\]\nThat is okay for a definition but we need to come up with an equation that will allow us to determine the MRT using values we can actually measure.\n\n\n2.5.3 Moment Curves in PK\nIn PK data, statistical moments of n order are described by the equation given below:\n\\[\n\\int_0^\\infty t^n*C(t)*dt\n\\]\nZero Moment Curve:\nLet’s start with the zero moment curve where n = 0, this is simply the concentration versus time curve that we have been using up to this point in our analyzes and as we have seen previously the interval of the zero moment curve gives us the AUC.\nThe first moment curve:\nWhen n in the equation takes on a value of 1. The integral of the first moment curve gives us a parameter called the area under the first moment curve or AUMC. It turns out that the AUC and AUMC values we determine in this way are useful for determining the mean residence time as we will see next.\n\n\n2.5.4 Calculating MRT\nwe will start with our original equation where the mean residence time is the sum of all the individual residence times divided by the number of individual molecules\n\nlet’s find a different way to express the summation in the denominator of the first equation. We can express the summation as an integral. The sum of the individual molecules is the integral from 0 to the dose of change in the amount is a function of time this interval defines the dose. So we can replace the denominator in the first equation with the dose. Now let’s take the second summation and recast it in terms of the integral from 0 to the dose of the individual times multiplied by the increment of the amount at that time. If we substitute these two equations into the first equation we get the fourth equation. Remember that the change in amount divided by the change in time is the clearance times the concentration as a function of time. So putting it all together the mean residence time is the ratio of these two integrals. We can substitute the clearance times the concentration for dA over dT, the clearance will cancel out so it simplifies to this ratio here. The denominator defines the area under the curve and the numerator defines the area under the moment curve for AUMC. Therefore, if we can determine both the AUMC and AUC we will be able to determine the mean residence time.\n\n\n2.5.5 MRT conditions\nThe equation that determines MRT is:\n\\[\nMRT = \\frac{AUMC}{AUC}\n\\]\nthe ratio is only valid if you have four specific conditions:\n\nThe PK is linear meaning that the clearance is independent of dose\nClearance does not change with time\nDrug is eliminated from the central compartment only\nDrug is administered into the central compartment as a bolus\n\nAll of these conditions have to be met for this equation to be valid\nConclusions\n\nStatistical moments can be used to describe PK curves\nThe mean residence time is the average time that a single drug molecule resides in the body\nMRT can be calculated by dividing the area under the moment curve by the area under the curve. Remember that this is only valid for an IV bolus dose.\n\n\n\n2.5.6 Non-IV bolus administration\nWhen the input is not instantaneous, the time for drug input must be considered with statistical moments.\n\\[\n\\frac{AUMC}{AUC} = MIT + MRT\n\\]\nBecause some molecules enter at time zero and some enters minutes or even after hours later. In that case the AUMC divided by the AUC is not equal to the mean residence time, it is equal to the mean input time plus the mean residence time.\n\\[\nMIT = \\frac{t_{dur}}{2}\n\\] For an infusion the mean input time is easy to determine when the infusion is delivered at a constant rate. Half way though the infusion half of the dose have been delivered. Therefore the MIT is the duration of infusion divided by 2.\n\n\n2.5.7 Other administration route\nFirst order input can be approximated using absorption rate constant\nlet’s consider first order input. The equation we have before is still valid where the ratio of the AUMC to the AUC is equal to the MIT plus the MRT. The mean input time is now different for first order input we can approximate the mean input time is one over the absorption rate constant:\n\\[\nMIT \\approx \\frac{1}{k_a}\n\\] Combining these two where the MRT can be approximated by the ratio of the AUMC to the AUC minus 1 over the absorption rate constant\n\\[\nMRT \\approx \\frac{AUMC}{AUC}- \\frac{1}{k_a}\n\\]\nFor oral administration, the MIT is given be equation below:\n\\[\nMIT_{oral}= MDT + MAT\n\\] where, MDT is mean dissolution time and MAT is mean absorption time\nMIT for oral administration includes times for dissolution and absorption\n\n\n2.5.8 Calculating AUMC\nTherefore it is even more complicated to determine the mean residence time following oral administration. To calculate the AUMC,\n\nwe have to first generate what is called the first moment curve. We calculate the term called c primed which is the time multiplied by the concentration value at that time.\n\n\\[\nC' = t_i *C(t_i)\n\\]\n\nCalculate the AUMC using linear or log-linear trapizoidal rule with the data pair (t, C’). This gives the AUMC from zero to the time of the last observation.\n\n\n\n\nFigure: Concentration curve and the first moment curve\n\n\nLet’s take a look at any illustration of this in blue we have a concentration versus time plot with c prime plotted in red on the second y ax remember that the c prime is the time multiplied by the concentration at that time let’s just recall how we get the AUC from the concentration by constructing a series of trapezoids from point to point as you see illustrated here remember that these trapezoids allow us to determine the AUC from 0 up to the time\nThe AUMC is just the area under this first moment curve just as the AUC was the area under the concentration time plot just as with the AUC we can determine the AUMC by a trapezoidal method as you see here in the red trapezoids.\n\nExtrapolate AUMC from the last observation to infinity\n\nNow that we have the AUMC up to the time of the last observation,the next part is to extrapolate it to infinity using equation below:\n\\[\nAUMC\\_{t-\\infty} = \\frac{C_{last}*t_{last}}{k_{el}} + \\frac{C_{last}}{(k_{el})^2}\n\\]\nThree variables go into this equation Clast, Tlast, and the elimination rate constant.\n\n\n2.5.9 Volume of distribution\nThe steady state volume of distribution can also be calculated using mean residence time. Steady state volume of distribution (Vss) is equal to the clearance times the mean residence time. In other words dose times AUMC over AUC squared. \\[\nV_{SS} = CL * MRT = \\frac{Dose *AUMC}{(AUC)^2}\n\\]\n\n\n2.5.10 Limitations of MRT\nThere are some limitations with mean residence time.\n\nMRT calculations are highly influenced by terminal samples. t*C can be large when t is large.\nInterpretation of MRT is very complicated when non-IV bolus modes of administration are used.\nThe mean residence time is not indicative of a duration of effect for most drugs. It is simply a way to describe the amount of time an average molecules spends in the body.\n\nConclusion, we’ve seen\n\nhow mean input time, mean disillusion time and mean absorption time are additional parameters that can be determined.\nhow to calculate AUMC based on the new data pairs of time and c prime\nhow the statistical moment parameters can be influenced by data at the terminal phase of the plasma concentration time curve"
  },
  {
    "objectID": "NCAintroduction.html#presenting-pk-results",
    "href": "NCAintroduction.html#presenting-pk-results",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.6 Presenting PK results",
    "text": "2.6 Presenting PK results\nInformation flow:\n\nIndividual observations\nCalculated PK parameter results\nStatistical results of comparisons\nDiscussion of results\nRelevant conclusions (that addresses the study objective)\n\n\n2.6.1 Presenting concentration data\n\nIndividual concentration values in listings\nIndividual concentration-time plots (both linear and semi-logarithomic)\nCombined plot of all individual concentration time data\nSummary statistics by nominal time points\n\nExample of individual listing:\n\n\n\n\n\n\n2.6.2 PK parameters\n\nIndividual parameter values in listing\nsummary statistics by treatment (for small data, include individual parameter values)\nGraphical presentation of the parameters are useful\n\n\n\n\n2.6.3 Proper use of descriptive statistics\nDiscrete variables: Variables that can only take on a certain number of values (eg whole number). Main statistics used are: N, minimum, maximum, median\n\nmean and SD don’t make sense for discrete variables\nTmax and Tlag are discrete variables because they can only take on values of the specific blood draw times.\n\n\n2.6.3.1 Continues variables\nVariables that can take on any value within a range of values. For example, body weight can take on any value it just depends on the precision of the instrument you are using. For a continuous variable that descriptive statistics that you want to use are:\n\nThe number of values\nThe minimum\nThe maximum\nThe median\nThe arithmetic or geometric mean\nThe standard deviation or the coefficient of variation\n\n\n\n2.6.3.2 Normally distributed variables\n\nthe arithmetic mean\nstandard deviation\n\nThe amount excreted in urine is an example of a normally distributed parameter\n\n\n2.6.3.3 Log normally distributed variables\n\nthe geometric mean\nthe coefficient of variation\n\nparameter such as Cmax, AUC and Clearance are log normally distributed\n\n\n\n2.6.4 Plots for PK parameters\na figure that is a useful way to display PK parameters graphically on the y-axis. We have Cmax on the x-axis, we have two groups a fasted group and then a non-fasted or fed group\n\nThe open circles represent individual values and the line connecting the pair of points signifies that it is the same individual so you can compare data from individual subjects.\nThe solid black circles with error bars represent the geometric least square mean with 95% confidence intervals. so it appears that there is an increase of food effect. this is not a statistical test but it’s just visual representation. The figure allows you to see that most of the lines slope upward from fasted to Fed with the exception of maybe two or three subjects. It looks like that increase in geometric mean may be skewed somewhat by one subject that had a very large Cmax. the plot makes it easier to detect trends in the data because it is very visual let’s take a look at the statistical results\nYou will want to include the full output of the statistical software in a listing so that all the details of your methodology are included. You will also want to include a well-organized summary table that presents the relevant data clearly and concisely. Finally graphical output can be very helpful so include graphics if it is appropriate for the data.\nLet’s take a look at some examples here’s an example of a statistical output for a linear mixed model and it’s from the same study that we’ve been examining in this video.\n\ntailed output but it has all the relevant details about how the analysis was done Readers probably won’t start examining your results here but they may come back to it if questions arise here’s a statistical summary\n\nwe have columns for parameters with units. The treatment the number of data points used the geometric lease squares mean for each of the treatments and the 90% confidence interval of that mean further to the right there are some comparisons between the treatments the pair of treatments is identified the ratio between the two treatments the confidence interval and p values are presented. This is a bio equivalence type analysis in the CMax comparison you can see that the ratio of Fed to fastid is 171% and the 90% confidence interval is from 141 to 208%. The p value is less than 0.05 and fasted treatments The AUC comparison show something different. The ratios for both AUC parameters are close to 100% and the 90% confidence interval is wholly contained within the 80 to 125% range are both greater than 05 therefore it appears that there is no significant difference in a UC between the Fed treatments but not.\nFor example here’s a graphical presentation of dose proportionality.\n\nThe individual observations are shown as open circles. The y-axis is the natural log of c max and the x-axis is the natural log of dose. The regression line from the power model is shown and the shaded blue areas represents the 90% confidence limits on that regression presenting it visually packs a lot of information into a single page. So graphical presentation of the relevant statistical information can help your audience understand the data much faster than they would otherwise be able.\nHere is another way to present statistical results graphically in a useful format called a forest pot:\n\nthis one is actually from a drug label. The top part of this plot shows the changes due to some parameter or co-administered compound. The PK parameters of interest the changes in those values plotted graphically including both mean and the 90% confidence interval and then dosing recommendations notice how for both ethanol and proton pump inhibitors the mean value indicated by a diamond is very close to one therefore there is no change relative to the reference and there is no dose adjustment needed but for sip 3a4 inhibitors like ketoconazole increased close to 1.5 * the reference and the recommendation is to have a maximum dose otherwise the patient might reach toxic levels and the section in the bottom shows the effect of demographics on PK parameters including age gender renal impairment or hepatic impairment so these forest plots are very useful way to communicate statistical results in a way that can be comprehended quickly Communication of scientific messages you want to focus the reader’s attention on clear tables and figures keep your text or minimum and lead the reader from observations to parameters to your discussion and to your conclusions that concludes the video on data presentation"
  },
  {
    "objectID": "NCAintroduction.html#bioequivalence",
    "href": "NCAintroduction.html#bioequivalence",
    "title": "2  Noncompartmental Data Analysis (105-OD)",
    "section": "2.7 Bioequivalence",
    "text": "2.7 Bioequivalence\n\nBioequivalence is defined as the absence of a significant difference in the rate and extent to which the active ingredient becomes available at the site of drug action.\nThe absence of a significant difference is understood to mean within 20% of the reference drug\nthe rate and the extent is understood to mean AUC and Cmax. Generally \\(AUC_{0-\\infty}\\) is used but sometimes \\(AUC_{0-t}\\) can be used.\n\n\n2.7.1 why, when and how we conduct equivalent study\n\nto assess whether two treatments are equivalent to each other\nthere are several contexts in which this might be done. The most common is the development of generic drugs in which the aim is to create a generic formulation that is bio-equivalent to an improved treatment. Bio equivalent studies can also be done during drug development to compare new formulations to one’s used in earlier phases of drug development. The same approach can be used for food effects studies and others.\nwe use a crossover study design in which all subjects receive both treatments and we will compare PK parameters such as Cmax and AUC to evaluate whether the treatments are bio-equivalent.\n\n\n\n2.7.2 Cross-over study design\n\nEach subject receives both treatments. We have a test treatment and a reference treatment (T for the test and R for the reference)\nsubjects are randomly assigned to one of two sequences. Either they get the reference first followed by the test and that is denoted RT or they get the test first and then the reference here denoted TR.\nThere is a washout period between the two doses, so that the first dose is completely gone before the second dose is administered.\nMost common is the two period crossover design in which subjects are either RT/TR but it is also possible to use other crossover design such as three period, RRT/RTR/TRR or four period, RTRT/TRTR crossover. The three and four period crossovers are used for drugs with a high degree of intra-subject variability.\nPK parameters that are used to compare the treatments are usually log transformed Cmax, ln(Cmax), \\(AUC_{0-\\infty}\\) and \\(AUC_{0-t}\\)\nlastly we use geometric means instead of arithmetic means. A geometric mean can be obtained one of two equivalent ways. You can multiply all the values and then take the n root where n is the number of subjects. There’s also an equivalent definition that uses natural logs and exponentials and that equation is also shown here.\n\n\n\n\n2.7.3 Example of Bio-equivalent study\nHere’s an example from bio-equivalent study with capsule and tablet formulations. In this case the tablet is the test and the capsule is the reference. In the top table these values here are the natural log of the\n$AUC$\n\nThe values for the tablets are in one column and the values for the capsules are in another. The ratio column has the value for the test divided by the value for the reference on a percent scale. The lower table takes the same approach except in this case the values of the natural log of Cmax.\n\n2.7.3.1 Bio-equivalence in practice\n\nIn practice log transformed perameters are evaluated in a mixed effects or two stage linear model.\nThe 90% confidence interval for the ratio of test to reference (T/R) should be within pre-specified limits.\nThe pre-specified limits are 80 to 125 on a percent scale unless a wider margin is acceptable based on clinical safety and efficacy data.\n\n\n\n2.7.3.2 Bio-equivalence results\nHere is a visual example of seven different bio-equivalence tests:\n\n\n\n\n\nThe square data points are the point estimates and the error bars indicate the 90% confidence intervals. Remember that the point estimate is the ratio of the test to the reference on a percent scale. The point estimate and the 90% confidence interval are both returned by the statistical model. The acceptable range is from 80 to 125 and there are dashed lines in both of these points. The ones inside the acceptable ranges are shown in green and the ones that fall outside or shown in red. Remember the entire range must fall within the limits of 80 to 125. If either the lower or upper bound of the confidence interval is not in this range then we cannot conclude bio equivalence. Notice the case in the middle although the point estimate is very close to 100% the confidence interval is very large. So that both the upper and lower bounds are outside the desirable range. Later in this lecture we will consider methods that can be used to assess bio-equivalence with highly variable drugs, such as this\n\n\n\n2.7.4 Average Bio-equivalence\nwe assess the bio-equivalence of the test formulation relative to the reference. This equation describes the statistical basis of the average bio-equivalence:\n\\[\n(\\mu_T - \\mu_R)^2 \\leq \\theta_A^2\n\\] The parameters in the equation are \\(\\mu_T\\) and \\(\\mu_R\\) and \\(\\theta_A\\). Let’s start by defining those terms. \\(\\mu_T\\) is the population average response of the log transformed measure either in AUC or Cmax for the test formulation. u sub r is the corresponding population average response of the logs transformed measure again either AUC or Cmax for the reference formulation sigma a is the equivalence margin which is plus or minus 20%. although a different margin might be used depending on safety and efficacy.\nThe equation states that the difference squared between the test and the reference must be less than or equal to the equivalence margin squared.\n\n\n2.7.5 Equivalence Margins\nLet’s take a look at the equivalence margins in more details\nEquivalence margins are designed to ensure clinical safety and efficacy or allowing for normal variations.\n\nMeasures of drug exposure are log normally distributed which is why we do comparisons of the log transformed PK parameters\nIf we assume difference between test/reference of 20% then the test/reference is 0.8 and this is the lower limit of acceptability\nbut the upper limit is symmetrical in log normal space. If you take the natural log of 0.8 you get minus 0.223 if you take the positive of that which is 0.223 and you exponentiate it you get 1.25\ntherefore you get the acceptable range in log normal space of minus 0.223 to + 0.223 that’s symmetrical which is plus or minus 20% but when you back transform it into linear space it moves to 80% on the lower end and 125%. So if you have been wondering where the 80 - 125 comes from, now you know."
  },
  {
    "objectID": "WinNonlinIntro.html#project-setup",
    "href": "WinNonlinIntro.html#project-setup",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.1 Project setup",
    "text": "3.1 Project setup\n\nCreate Project\n\nPhenoex Projects\n\ncontains all the data and calculations\nmultiple projects can be open at the same time\ncommon data file to import: excel, csv\nolder version can always be opened by newer version\nnewer version projects can not be opened by older ones\nCreate a project\nClick to history tab to see the project history\nClick to properties tab, where most of the work is done\n\n\nCreate Worksheets\n\n\nRight click on Data, select New, select Worksheet\nAdd columns, give column name, assign data type\nassign units: select time from list of columns, click Unit Builder button, specify h to the time, click Add button,click OK\nfor dose, specify mass prefex, click Add\nType numbers on the worksheet to add values to the cells\n\n\nImport Files\n\n\nfile type: xls, xlsx, csv, SAS\ntypical data file contains header and unit row.\nSelect the Import button, select the file\non the File import wizard, select appropriate options\nPreview area helps to see the changes\nIf units are in the column header, select “has units in the header”\n\nExcels with multiple worksheet:\n\nclick the arrow to move on the Wizard to the next worksheet\n\n\nSave Projects\n\n\nClick the save icon on the toolbar\nFile name can be completely different from the Project name\nNo auto-save options\nsharing project file will also share the embedded data files\nClose project by right clicking the project name\nAfter opening a saved project folder, expand the plus sign to see the contents.\n\n\nSet Project Preferences\n\n\nSelect the Edit menu -> preferences -> Projects\nCheck Autosave on execution\nupdate the save locations and hit apply before clicking on OK."
  },
  {
    "objectID": "WinNonlinIntro.html#create-and-modify-worksheets",
    "href": "WinNonlinIntro.html#create-and-modify-worksheets",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.2 Create and Modify Worksheets",
    "text": "3.2 Create and Modify Worksheets\n\nSort Rows\n\n\ndata can be sorted by subject, dose level,\nsort button on every worksheet\nUse the “sort worksheet” window to apply sort options\n\n\nMove Columns\n\n\nSelect a column from the column list\nClick the up or down arrow to move\n\n\nRename Columns\n\n\nClick the column name, type F2 or double click on it to edit the name\n\n\nApply Units\n\n\nSelect column\nClick the Unit Builder\nClick Clear Units\nAdd units\n\n\nConvert Units\n\n\nconvert amount column from microgram to miligram\nclick the Amount column\ntype mg in the New unit box,click OK\nTo convert ng/mL to nmole/mL, add nmol and then click the slash button, specify the volume unit, enter molecular weight, click OK.\nBetter way: use the Data Wizard to convert the units."
  },
  {
    "objectID": "WinNonlinIntro.html#plot-data",
    "href": "WinNonlinIntro.html#plot-data",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.3 Plot Data",
    "text": "3.3 Plot Data\n\nCreate Simple Plot\n\n\nData: Conc, Time, dose level: 16 mg, 10 subjects\nright click the worksheet\nSelect send to -> plotting -> xy plot\nXY object is created with the linked data source\nOn the mapping window, orange column headers are required mappings\nmap, x -> Time, y -> conc, Group -> subject\nclick execute\nOptions pan - Axes - Y - select log button\nOptions pan - Graphs - rename by typing F2\nGraph name and legend names are the same\n\n\nCreate Lattice Plot\n\n\nData: Conc, Time, Administration, dose level 4 mg for IV, 8 mg for PO, 10 subjects\nCreate a XY plot object same as above\nmap x - Time, y - conc, group - subject, lattice column - administration\nExecute and get two plots\nOptions - range - ‘auto scale best’ settings scales individual plots are independent\n\n\nUse Second Y Axis\n\n\nData: plasma conc, urine conc, time, 10 subjects\nCreat XY plot object same as above\nmap x - Time, y - plasma conc, y2 - urine conc, lattice condition, page (sort) - Subject\nplots are on a single page for each subject\nOptions - select plasma_conc vs Time, type F2, change the name to Plasma, do the same for Urine\nExecute\n\n\nCompute Descriptive Statistics\n\n\nNeeded to create a plot with mean and error bars\nright click on data sheet, send to - computation tools - Discriptive statistics\nmap summary - conc, sort - Time\nExecute\nOptions pannel - click Clear All - click basic statistics, check Mean and SD\n\n\nUse Error Bars\n\n\nDiscriptive Satistics object - Output data - right click on Statistics\nsend to plotting - XY plot\nmap x - Time, y - Mean, Error bars, lower - SD, Error bars, upper - SD\nExecute\nset Y axis to log scale\n\n\nCreate Overlay Plot\n\n\nduplicate the error bars plot from previous section\nOptions pan - Plot - Graphs tab - click Add button\nSelect the new second input from the setup tab\nLink the source data by clicking source button,\nmap x - Time, y - conc, group - subject\nExecute\nOptions pan - select Conc vs Time plot\nSelect Quick Styles\nUncheck Group by lines, uncheck Gourp by colors,\nSelect Apprance tab\nSpecify color to Silver\nUncheck Markers visible\nNow all the individual lines are silver color\nSelect Mean vs Time graph\nSelect Appearance, specify line colors to red, Marker border color - red, line weight 3\nUnder the Mean vs Time graph, select the Error bars\nSelect Appearance, color - red\nOptions pan - select Y axis - select Axis label and update\nOptions pan - select Legend - uncheck Visible\n\n\nCreate Box Plot\n\n\n\n\n\n\n\n20 subjects, AR: accumulation ratio (how much accumulated under repeated ss), dose level, 2 mg and 4 mg\nIs AR increases with increasing dose level?\nright click data, send to - plotting - box plot\nmap y - AR, group - dose level\nExecute\n\n\nCreate Plot with Categorical X Axis\n\n\nData: Severity (Mild, Moderate), dose level (1, 2, 4, 8, 16, 32 mg), frequency (numerical, i.e., 0, 0.2, 0.6)\nright click the data sheet, send to plotting - X-categorical XY plot\nmap x - severity, y - frequency, group - dose level\nOptions pan, select X axis, select Order tab, change order if needed\nOptions pan - Frequency vs Severity graph - check line visible - now points are connected by a line\n\n\nSet Plot Preferences\n\n\nOptions pan - Plot - Layout \nEdit menu bar, preferences, plotting details\nChanging prefernences affect all new plots"
  },
  {
    "objectID": "WinNonlinIntro.html#introduction-to-nca",
    "href": "WinNonlinIntro.html#introduction-to-nca",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.4 Introduction to NCA",
    "text": "3.4 Introduction to NCA\n\n3.4.1 About NCA\nNon-compartmental analysis or NCA is a method for quantifying drug exposure\n\nNCA determines a large number of pharmacokinetic descriptors or PK parameters for a drug\nThey are not really parameters as you would have in a model\nNCA does not use any kind of model other than assuming that the elimination can be described by first order kinetics\nbecause there is no model at the heart of the method we cannot really use it for predictions\nAn example plot of concentration over time following an extravascular dose NCA will give us two different measures of drug exposure:\n\nthe peak exposure to the drug concentration occurring after dosing\nThe overall exposure is measured by computing the area under the curve or AUC\nan extra vascular dose starts with a concentration of zero, the concentration rises rapidly reaches Cmax and then decreases\n\n\nwith extra vascular dosing there is an absorption process that leads to a maximum concentration followed by elimination\nIV bolus dosing: drug is directly injected all at once into a vein; the mixing and systemic circulation is very fast and by the time the first sample is taken after dosing the mixing is assumed to be complete. The concentration starts high and then decreases as the drug is eliminated.\nIV infusion: the concentration starts at zero and then rises if the infusion is continued for long enough the concentration approaches a plateau at steady state when the infusion stops the concentration then falls in the same manner as in ivy bolus dosing\nplotting on a log scale is useful because it usually shows linear elimination in each case regardless of the dosing root we could fit the linear portion with a straight line to predict what will happen to concentration after we’ve collected the last sample concentration on the log axis\n\n\n\nIt is useful to have both linear and log plots. Linear plots are useful for examining the peak concentration and log plots are useful for the low concentrations\nIn addition to an elimination phase many drugs also show a distribution phase in such cases there may be two distinctive straight line sections on the plot. Although sometimes the two phases blend into a general curvature in the plots we see here the distribution phase is apparent for all three dosing routs but it is most pronounced for the IV bolus dosing. For extravascular dosing the distribution phase may be obscured by the drug absorption.\nThe AUC can be determined no matter how complex the relationship between concentration and time.\nSummary:\nNCA is the primary method of assessing drug exposure.\nCmax is a measure of peak exposure\nAUC is a measure of the overall exposure to the drug\ndifferent dosing route leads to a curve with the distinctive shape that plotting on a log concentration scale usually shows linear elimination\nmany drugs show a distribution phase as well as an elimination phase\n\n\n\n\n3.4.2 Observe Parameters\n\nFrom the plot of concentration versus time, we can see that the maximum concentration is reached at about 1 hour, we call that time Tmax and the concentration at the peak is Cmax\nTmax and Cmax are listed in the output of NCA in Phoenix\nAt some point after dosing we will have our last observed concentration this may be because we have stopped collecting samples or the concentration may have dropped below the quantification limit for the analysis and therefore we were unable to get more values The point is at a time of t last and has a concentration of Tlast These observeed parameters are affected by the sampling schedule we can improve our chances by sampling richly around the expected time of c max if we have more points we have a better chance of capturing a concentration that is near the true maximum\n\nSummarize\n\nObserved parameters are TMax Cmax TLast and Clast. We call these observed parameters because they are found directly in the observations\nthe observed parameters are dependent on sampling times\nsample richly around the expected time of Cmax so you can have a better chance of capturing something close to the true maximum\n\n\n\n3.4.3 Half-Life\n\ntime it takes for the concentration to decrease by 50%.\na long half-life leads to a shallower slope and a short half-life leads to a steeper slope\nsome drugs exhibit two phases a distribution phase and an elimination phase each of these will have a half-life associated with it The shorter the half-life of the distribution phase the steeper the initial decline will be although we usually concentrate on the half-life of the elimination phase the effective half-life of the drug may very well depend on the half-lives of both of these processes\nIt takes five to seven half lives to eliminate the drug.\n\n\n\n3.4.4 Area Under the Curve (AUC)\nHow to calculate AUC?\n\nassume that the concentration follows a straight line between points\none triangle and several trapizoid\nAUC is calculated from concentration-time data\nTrapezoids are used to estimate AUC between two data points\nAUC is the sum of the areas of all the trapezoids plus one triangle\n\n\n\n3.4.5 Extrapolation to Infinity\n\nafter the Tlast there are still large quantity of drug in the plasma\nHow can we extrapolate to infinity?\nWe need a way to calculate the AUC Tlast - infinity.\nSlope of the elimination is the key, apparent terminal phase, magnitide of the slope is \\(\\lambda_Z\\)\n\n\\[\nAUC _{tlast - \\infty} = \\frac{C_{last}}{\\lambda_z}\n\\]\n\\[\nAUC _{0 - \\infty} = AUC_{last} + \\frac{C_{last}}{\\lambda_z}\n\\]\n\nextrapolatd area should be below 20%\n\nImportant NCA parameters:\n\nIndependent of least-squares fit: such as Cmax, Tmax, AUClast,\nDependent on the least-squares fit: \\(\\lambda_Z\\), AUC~ 0-inf~, %Extrapolation, terminal half-life, volume, clearance\n\n\n\n3.4.6 Volume of Distribution\n\\[\nC = \\frac{Dose}{V}\n\\]\n\nvolume of distribution relates to the dose and concentration\nDoes not corresponds to anything physiological\nExample, 100 ug dose to IV bolus and 2 ug/L concentration, volume is 50L.\ntypical human plasma volume is 5 L, why V is sometimes very large?\nDrugs that are strongly bound to protein has very high V\n\n\n\n3.4.7 Clearance\n\nClearance Quantifies how quickly drug is removed from the body\n\n\\[\nRate ofelimination = Cl * C(t)\n\\]\n\nIn most cases Cl is constant. If changes with concentration, suspect non linear kinetics (saturation). for this reason, different dose level is adminstered.\nClearance includes both Metabolism and Excretion\n\n\n\n\nit is difficult to obtain all the ratios, so the overall ratio is called Bioavailability.\n\nBioavailability\n\\[\nF = \\frac{AUC_{oral}/Dose_{oral}}{AUC_{IV}/Dose_{IV}}\n\\]\n\nIntravenous: NCA parameters are V and Cl (F = 1)\nExtravascular: NCA parameters are V/F and Cl/F (F<1)\nElimination = Metabolism (liver) + Excretion (Kidney)\nCltotal = Clhepatic + Clrenal + Clother\nClrenal = Ae (amount of drug excreted in the urine)/ AUCplasma\nCalculation of Clearance from NCA:\n\n\n\nYou get the following from NCA\n\n\n\n\n3.4.8 Linear vs Log"
  },
  {
    "objectID": "WinNonlinIntro.html#run-nca-on-plasma-data",
    "href": "WinNonlinIntro.html#run-nca-on-plasma-data",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.5 Run NCA on Plasma Data",
    "text": "3.5 Run NCA on Plasma Data\n\n3.5.1 Run NCA using best fit\n\nDrug: Gravitix, 10 subjects, a single ascending dose (SAD) study, 6 dose level (1, 2, 4, 8, 16, 32 mg), PO adminsitration\nFrom the plot each subject grouped by dose level, we see that as the dose increases so do the concentrations in plasma\nwe expect that the drug exposure should increase proportionally to the dose\nafter we run NCA we will assess dose proportionality by examining PK parameters returned by NCA\nTwo worksheets: Observations and Dosing. Data on Observatino worksheet: Conc, Time, Subject, Doselevel, Administration, Amount.\n\nPerforming NCA:\n\nRight click on observation worksheet\nselect sent to, non-compartmental analysis, NCA\nSelect plasma, which is default setting\nspecify the dose type the default is extravascular\nRequired mappings: Time to time, the conc to concentration, subject to sort, dose level to sort\nselect dosing. two options: a worksheet with the dozing information or an internal worksheet\nselect source\nclick okay\nMap: Time to time, amount to dose, subject and dose level to sort\nspecify the calculation method linear up logdown\nclick execute\ndouble click the final parameters pivoted worksheet to open in its own window\n\nViewing plots:\n\nselect observed y and predicted y versus x\nfor subject one dose level 1: 5 points were used in the \\(\\lambda_Z\\) calculation, from 8 hours to the last observation at 36 hours\nThe best fit method automatically determines the optimal least squares regression using at least three points\nr-squared is the correlation coefficient of the regression\nr squared adjusted is based on the r squared adjusted for the number of points in the regression\nThe number points with the best value of r squared adjusted is used.\nthe half-life based on the value of \\(\\lambda_Z\\) is also reported in this case the half life is about 22 hours\nPage 2: this plot is also for subject one but now the dose level is 2 mg\nin this case the best fit method used three points in the calculation\neven though it is the same subject but the half-life based on lambda z is much shorter than for the first dose at only about 15 hours\nPage 3: this plot is for subject one does level 4\nagain 3 points were used in the calculation\nthe half-life is about 28 hours\nPage 4: this is for subject one those level 8\nthis time five points were used in the half-life is 15 hours\nPage 5: now we are up to those level 16 for and half-life is 21 hours\nPage 6: this plot is now at the highest to those level 32 mg\nthe half life is lower this time It’s 18\nwe have seen it’s quite a variability between different dose levels but is there a systematic trend?\n\nlet’s create a box plot to see if there is appears to be a trend\n\nright click on final parameters pivoted\nsend to plotting: box plot\nmap HL_lambda_z to y\nnow we want to look for a trend across the dose group map to dose level\nclick execute button\nThe plot shows us the distribution of half-life across the different dose groups\nfrom the plots we can see that although there is a good deal of variation in the dose levels, there does not appear to be a systemic trend and all the boxes overlopped with each other\n\nBox plot of AUCs\n\nlet’s make the duplicate of this plot:\nright click box plot\nselect copy\nright click on the workflow\nselect paste\nselect the duplicated plot let’s change the mapping for the y map AUCINF_D_obs\nclick execute\nbecause the data are dose normalized AUC values extrapolated to infinity we would expect these values to be the same for all dose groups and they do appear to be very consistent\nmost values falling between 0.04 and 0.05\nthis does suggest that the overall drug exposure is proportional to the dose\nalthough this is not a statistical test for the dose proportionality, it does give us a quick visual impression\n\nBox plot of AUClast_D\n\nLet’s make duplicate of the second plot right click the plot select copy right click on workflow paste\nmap AUClast_D to y execute\n\n\n\n3.5.2 Customize rules and parameters\n\nGoal is to add \\(\\lambda_Z\\) acceptance rules to our NCA\nselect the rules tab\nenter 0.9 for r-squared adjusted\nenter 20 for percentage of extrapolation\nenter 2 for span, the span is the number of half lives spent by the regression.\nclick execute\nReview the flag column for r-squared adjusted, % extrapolation and span\nnote that the results are not removed from the output\nbut we can use data tools to do the filtering if we want to\n\nTo filter the data:\n\nRight click in the final parameters pivoted worksheet\nSelect sent to\nselect data management\nselect split worksheet\nmap the three flag columns to sort\nClick execute\n\nThe “unique values” worksheet shows how many rows there are for each combination of sort values - notice that there are 24 rows that passed all criteria\n\nlet’s look at the worksheet fully accepted here we see the walls that has all three criteria is\n\nUser defined parameters\n\nselect “user defined parameters” tab\nto compute the concentration at a time of 48 hours\ninter 48 in the box The other thing that we can do is user defined parameters\nthe NCA does include many parameters we can also define our own\nclick add button\nadd a parameter, i.e., AUC_2\nFor definition you see last / 2\nif you would like this values include in the final parameters pivoted worksheet turn on include the final parameters\nexecute the NCA object\n\n\n\n3.5.3 Customize slope selections\n\nstart with NCA used in previous section\nDuplicate the NCA object\nselect slopes selector from the Setup trab\nEach plot is on a separate page, on each plot we can see the points that were used select for best fit.\nclick the left point to change starting point for the linear fit\nto change the end of the fitting, holding down the shift key and clicking on the indicated point\nDon’t change the endpoint of the fitting unless you have reason to reset the last point\nto exclude points from the linear fit hold on the control key and click on the indicated point\n\nFaster way of modifying slopes:\n\nselect the slopes under the Setup tab\nthis worksheet has the same information we saw in the individual plots but all on one table\nto control the best fit method select rules tab\nwe have two options for customizing you can either limit the number of points used in the linear regression or you can specify your start time limit\nSince there is a distribution phase we might decide to make sure that we do not include from the 12 hours in linear regression\ntype of 12 in the option start not before\nlet’s look at the slopes and then see output settings\nselect slope settings\nNow, the start time is at least 12 hours\nComparing box plots from two NCA results ( best fit and coustom fit) it is seen that some of the outliers are removed.\nIt is good idea to examine each slopes and adjust the solpes to compare the data.\n\n\n\n3.5.4 Compute partial areas\nIn the previous section we saw how comparing AUClast was problematic.When different subjects had different Tlast, Computing partial areas is a way that can overcome that limitation.\n\n\nSelect the setup tab\nSelect Partial Areas\nCheck “use Internal Wroksheet”\nBeacause we will use the same partial areas for all subjects, uncheck subject checkbox and beacause we want to use the same settings for all dose groups, uncheck DoseLevel checkbox\nClick OK\nWe may want to compute more than one partial area, at the bottom of the option tab, we can specify how many we want.\nClick on the selector for number of maximum partial areas. Select 2\nOnce again, we are asked to specify the sorts, Uncheck subject and dose level checkbox.\nWe need to fill in the start and end times\nFor the first partial area, we will compute the partial area for the first 12 hours\nSpecify 0 for the start time and 12 for the end time.\nFor our second partial area, use 0 and 24 hours.\nIf desired, we can label for each partial area. However, label is not required and it will automatically generate for us. Leave the label field blank.\nClick Execute.\n\nThe partial areas are added as columns to the right of the final parameters pivoted.\nLet’s repeat the process for other NCA object in our project: Select NCA best fit Click Setup Click Partial Areas Check “Use Internal Worksheet”\nTurn off Subject and doselevel Specify the maximum number of partial areas\nThe plots are all read, because the plots are all out of date -WE can update by Selecting the workflow in the object browser, the Workflow is displayed, click the Execute butte and it will update. While the Workflow is selected, on the result tab, we have the output of the workflow. Let’s go back to the workflow diagram, select Diagram, notice how the red color gone\n\n\n3.5.5 Use Theraputic response\nTo choose a dose for a given drug we have consider both the efficacy and side effects. We can set upper and lower concentration limits in NCA and quantify the time and AUC between the limits as well as above and below.\n\nThe following plot is for gravitex SAD study: Let’s say we want concentration to be between 10 and 50 ug/L. At the lowest dose, all the concentrations are below the lower limit. For higher dose (32 mg), concentrations are largley between the upper and lower limit\n\nSelect “Therapeutic Response” option on the Setup tab of the NCA object\nuncheck both subject and dose boxes\nEnter 10 for lower limit and 50 for higher limit\nExecute\n\n\n\n3.5.6 Customize units and parameters names\nTo obtain units in the NCA results, we need to have units defined on the concentrations, the times and the doses\n\nconcentration units are microgram per liter, nanogram per liter and micromols per liter\ntimes units are typically either hours or minutes and in some cases days\nunits for doses are milligrams micrograms and micromols\nit is not recommended to mix mass units\nif you get no units on your results you may have missed the unit on the concentration, time or dose, you will see the message insufficient unit in the units output. If this happens to you check that you have proper units on the concentration time and dose\n\nDefine the units used in the NCA:\n\nselect the setup tab\nselect the units input\nThe default units depend on input dataset units because the time units in the input data set is hours all the time units in the NCA results will also be based on hours\nThe volume unit in our input data is liters and therefore all the results have liters in the default units\nif we decide we want to change the units on our results we can do so by changing the values in the columns labeled preferred\n\nlet’s do that\n\n\nChange the preferred unit for volume to milliliter\nspecify milliliters per minute for the clearance unit\nMake sure to match the case as shown\nPhoenix will attempt to convert but if Phoenix doesn’t know how to convert to your preferred unit it will revert to the default unit execute the NCA\nBy checking execute here you can see the appropriate units in the results.\n\nChanging Parameters Name\n\nClick setup tab select the parameter names input\nTurn on the Use internal worksheet checkbox\nLet’s say that we want to change the name of half life\nAnd executed the NCA and now the half-life column has been renamed in the result"
  },
  {
    "objectID": "WinNonlinIntro.html#run-urine-nca",
    "href": "WinNonlinIntro.html#run-urine-nca",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.6 Run Urine NCA",
    "text": "3.6 Run Urine NCA\n\n3.6.1 About Urine NCA\n\n\nUrine samples are collected over an interval. Four samples were collected and concentration and volume was measured, the data is compiled in the table.\nThe rate of drug excretion is calculated by:\n\n\\[\nRate = \\frac{A_{ur}}{t_{end}-t_{start}}\n\\]\n\nPlot the Rate of drug excretion vs midpoint of collection interval\nRate of excretion starts high and decreases over time.\n\n\n\nMost import parameters are Amount Recovered and Percent Recovered and Percent of extrapolation (AURC_%Extrap_red)\nUrine NCA also include lambdaZ and halflife, however, since the urine data only contains four data, it is better to use plasma halflife\nPercent of extrapolation should be as small as possible.\n\n\n\n3.6.2 Setup project\n\n10 subjects, dose level, 4 mg, conc in both plasma and urine,\n\n\n\n3.6.3 Exploratory Data Analysis\n\nXY plot of plasma conc vs time is created\nXY plot of urine conc vs time is created. Time is end of collection interval\n\n\n\n3.6.4 NCA of plasma data\n\nmap x to Time, y to conc, sort to subject\nselect the Dosing imput, map Time to Time, Dose to dose, sort to subject\nSelect linear up log down\nExcecute\n\n\n\n3.6.5 NCA of urine data\n\nspecify the model type: Urine\nRequired mapping: start time, end time, concentration, volume\nsort to subject\nselect the dosing file, sort subject\nExecute\nAutomatic data calculations was done. Three different rates are given: Max_rate, Rate_last, Rate_last_pred, Tmax_rate, AURC: Area Under the Rate Curve\nUrine NCA does not extrapolate for amount recovered and percent recovered. That’s why it is best to collect urine samples until no more drugs are detected in the urine sample\n\n\n\n3.6.6 Calculate Renal Clearance\n\n\nright click Final Parameter Pivited, send to “computation tools”, “ratio and differences”.\nmap sort to subject\nSelect the worksheet2 input, click to select source button, select “Final Parameter Pivited” from NCA plasma, sort to subject\nOn the options tab, update the X column to “Amount Recovered”, Y column: AUClast, new column name: Clr, unit: L/h,\nExecute\n\nCreate a box plot\n\nRight click Ratios Differences Stacked, send to box plot, map y to Clr,"
  },
  {
    "objectID": "WinNonlinIntro.html#sparse-and-steady-state-nca",
    "href": "WinNonlinIntro.html#sparse-and-steady-state-nca",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.7 Sparse and Steady-State NCA",
    "text": "3.7 Sparse and Steady-State NCA\n\npre-clinical study of Gravitex in rats, 20 rats, dose: 200 ug/kg,time of blood draws: group A(0.5, 1, 2, 4, 8 h), group B( 0.75, 2, 6, 12 h)\nNot possible to do full NCA with only four data points\nWe need to pool the data to do NCA\nData: Schedule ( group A or group B), Animal, Time, Conc\n\n\n3.7.1 EDA of Sparse Data\nPlot: Concentration vs time by schedule\n\nPlotting the data: XY plot, map x to Time, y to conc, group to schedule, group to animal\nOptions pan, select the graph Conc vs Time, select Quick Style tab, check “Each group to color”, select “Schedule”\nNow the lines are yellow for group A and purple for group B.\nTurn off “group by marker”, now all the markers are same\nTurn off “group by lines”,\nSelect the Legend and turn off the “visible” checkbox\n\nTable:\n\nright click observation worksheet, send to reporting, select table\nmap conc to Data, animal to Raw ID, Raw Stratification to Schedule, Column Stratification to Time.\nSelect “Precision/Alignment” from option, precision method: significantDigits, value: 3,\nFor Time, specify the precision method: DecimalPlaces, value: 2, execute\nSelect Animal column, specify precision method: DcimalPlaces, value: 0\nSelect statistic tab, turn of “mean” and “SD”\n\n\n\n3.7.2 NCA of Sparse Data\nKey Points on NCA of spares data:\n\nSpares Option often used in pre-clinical data, where there is insufficient data to compute a full NCA for each subject\nOnce set of PK parameters is returned for the pooled results\nPK parameters are identical to what you would get by running NCA on mean concentration data\nStandard errors on Cmax and AUClast . These are calculated using published methods\nStandard error calculation is only avaialbel with linear trapezoids\n\nPerform NCA\n\nSend the data to NCA, turn on “Sparse” button\nRequired mapping: Time to time, Concentration to Conc and Subject to Animal\nSpecify dosing file, time to Time and dose to dose_norm, execute\nThere is only one raw in the result sheet\nLinear Trapizoid method was used and SE was obtained for some of the PK parameters.\nSince the data is pooled, only one plot is obtained.\n\n\n\n3.7.3 EDA of Multiple-Dose Data\nStudy design:\n\nExample: Gravitex multiple dose study\n20 human subjects randomly divided into two groups (dose level 2 and 4), dosed at 0h, conc. determined at 48 h,2nd dose at 48h, next doses at every 24h, until 192 hours. The first dose is a Naive dose\nRichly sampled for 0 to 48 hours and the last dose from 192h to 240h. Only Trough conc were measured for other doses\n\n\nBlue lines are generated from PK model.\nPlots:\n\nMap x to Time, y to Conc, group to subject, lattice page to dose level, lattice column to profile\n\n\nAfter modifying (log) x and y axis:\n\nTable:\n\nSend to reporting, Table\nmap Data to Conc, subject to RowID, dose level to stratification row, stratification column to time, profile to stratification column.\nSelect Column/sort Order, select Row Stratification, select Column Stratification, move profile to the top,\n\n\n\n\n3.7.4 Split Data\n\nBefore running NCA of multiple dose data, we need to split data\nRight click the worksheet, send to Data Management, Split Worksheet\nmap profile to sort, Exicute.\nSplit the dose worksheet the same way as the observation worksheet\n\n\n\n3.7.5 NCA for First Dose\n\nSelect Split Observation, right click on Output data A, send to NCA\nmap subject to sort, dose level to sort, conc to concentration, time to Time\nlink dosing input for A profile\nmap Dose to Amount, Tau to Tau, sort to dose level and subject\nCalculation method LinearupLogdown\nSelect Dose level column and click freez pane icon\n\n\n\n3.7.6 NCA for Final Dose\n\nperform same operation as for First Dose by selecting profile C.\nTmin is the time where minimum concentration was found\nCtau is the concentration at the end of the dosing interval\nCavg is the average concentration during the dosing interval\nDifference between AUClast and AUCtau\nSlope correction of the linear fit is of concern only if we are extrapolating the results. Otherwise AUCtau will not be affected by the slope correction.\n\n\n\n3.7.7 Determine Accumulation Ratio\n\nNCA can only determine PK parameters of the first dose or the last dose,\nAccumulation ration (AR) is calculated with the equation below:\n\\[\nAR = \\frac{AUC_{\\tau,SS}} {AUC_{0-\\tau}}\n\\]\nwhere, \\(AUC_{\\tau,SS}\\) is AUC_TAU from steady-state NCA, last dose and \\(AUC_{0-\\tau}\\) is AUC0_24 partial area from first dose NCA\npartial area"
  },
  {
    "objectID": "WinNonlinIntro.html#use-data-tools",
    "href": "WinNonlinIntro.html#use-data-tools",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.8 Use Data Tools",
    "text": "3.8 Use Data Tools\n\n3.8.1 Append Worksheets\n\nCombine data from two worksheets that share the same general structure of columns\n\n\n\nexample, combine PK1 and PK2. The new column “source” tells which worksheet each set of rows came from.\nColumns do not have to be identical nor the same order\nRight click the worksheet, select Data Management, Append Worksheets\nmap source column to all the columns of worksheet 1\nClick worksheet2, link the source file, map all the columns to Source Column\nDouble click Results tab to view the results\nTo append more that one worksheet, set the number on the option section\n\n\n\n3.8.2 Cross-product Worksheets\n\nFor making combination of multiple columns,\n\n\n\nRight click the worksheet, send to Crossproduct Worksheet\nmap subject to sort\nLink the second worksheet and map the same way as before, Execute\n\n\n\n3.8.3 Join Worksheets\n\nTo combine data so that rows are matched by a common sort key. Merge option is identical to Join option.\n\n\n\n\n\nboth column has be to same name to join the columns.\nClick ’sort map”, turn on Internal Worksheet, cut and paste to the same row.\n\n\n\n\n3.8.4 Pivot Worksheet\n\nRearranging data to allow comparison, for example to compare the effect of treatment, we need to see the data:\n\n\n\n\n\n\n3.8.5 Stack Columns\n\nInverse of Pivoting . Stackers stacks two or more column into a single column\n\n\n\nChange the column names from the options section\n\n\n\n\n3.8.6 Split Worksheet\n\n\n3.8.7 Enumerate Worksheet\n\nTo convert text values to numbers,\n\n\n\n\n\n3.8.8 Make BQL Substitutions\n\n\nAny non-numeric data is ignored by the NCA object\n\nTo estimate Tlag we must replace BQL by zero"
  },
  {
    "objectID": "WinNonlinIntro.html#compute-ratios-and-differences",
    "href": "WinNonlinIntro.html#compute-ratios-and-differences",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.9 Compute Ratios and Differences",
    "text": "3.9 Compute Ratios and Differences\n\n3.9.1 Compute Ratios from Single Input\n\n10 subjects with 2 mg IV dose, after wash out period same subject was administered 4 mg PO, To compute Bio-availability, we need to do ratio from a single worksheet.\n\n\n\n\n\n3.9.2 Compute Ratios from Dual Inputs\n\nNCA for urine and plasma must be done in separate NCA object, we have to combine results from two different worksheets.\nRenal Clearance example, view clearance section above.\n\n\n\n\n3.9.3 Compute Ratios using Means\n\nDifference between cross-over and parallel study\n\n\n\n\n\n\n3.9.4 Compute Differences"
  },
  {
    "objectID": "WinNonlinIntro.html#use-data-wizard",
    "href": "WinNonlinIntro.html#use-data-wizard",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.10 Use Data Wizard",
    "text": "3.10 Use Data Wizard\n\n3.10.1 Create a Filter\n\nFilter by time values\nRight click, send to Data management, Data Wizard\n### Set Column Properties\n\n\n\nWe can: 1. Exclude whole column; 2. Exclude or include by values, 3. Filter individual cells or whole rows.\n\n\n\n3.10.2 Set column properties\n\nThis can be used to sort columns, rename columns, specify or convert units\n\n\n\n3.10.3 Transform Data (Arithmatic)\n\nused to do simple arethmatic; x and y are variables, n is constant, units inherited from source.\nsame as data normalization.\nSet baseline in Time based data, Requires Time and Data Columns to map, Substrat the initial values\n\n\n\n3.10.4 Transform Data (Custom)\n\nCeiling, random\n\n\n\n\n3.10.5 Transform Data (Functinos)\n\nthis works with only on single column such as X to Ln(X).\n\n\n\n3.10.6 Multi-step operation"
  },
  {
    "objectID": "WinNonlinIntro.html#applications-of-nca",
    "href": "WinNonlinIntro.html#applications-of-nca",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.11 Applications of NCA",
    "text": "3.11 Applications of NCA\n\n3.11.1 Using Cmax and AUC to Make Decisions\nIn regulatory guidance:\nRate of absorption : Cmax\nExtent of absorption: AUC\nCmax and AUC are used in the following type of studies:\n\nApplication of NCA\n\n\n\n\n\n\nStudies\nwhat compares\n\n\n\n\nFormulations\nCompare two formulations of a drug\n\n\nFood effects\nCompare effect of food on drug absorption\n\n\nDrug-drug interaction\nCompare effect of one drug on another\n\n\nDemographics\nCompare effect of demographics (age, weight, gender, etc) on a drug\n\n\nSpecial Patient Populations\nCompare effect of drug in two different groups\n\n\nHepatic Impairment\nCompare effect of drug in two different groups\n\n\nBio availability\nCompare drug exposure with and without absorption\n\n\nDose Propotionality\nTest of linear kinetics\n\n\n\nAUC Rations (Also for Cmax )\nBioequivalence \\(\\frac{AUC_{test\\, form}}{AUC_{ref\\, form}}\\)\nFood Effect \\(\\frac{AUC_{fed}}{AUC_{fasted}}\\)\nDrug-drug Interaction: \\(\\frac{AUC_{Drug\\, + Inhibitor}}{AUC_{Drug\\, alone}}\\)\nHepatic Impairment: \\(\\frac{AUC_{with\\, impairment}}{AUC_{normal\\, hepatic\\, function}}\\)\n\n\n3.11.2 \n\n\n3.11.3 Study Designs\nStudy design considerations:\n\nPopulation\nDuration\nSchedule\nProcedures\n\nCrossover: Each subject receives each treatment\nParallel: Each subject receives one treatment\nsingle dose\n\n\n3.11.4 Dose Proportionality\n\\[AUC = \\frac{F*Dose}{CL}\\]\n\nIf Dose and CL are constant, \\(AUC \\propto F\\)\nIf F and CL are constant, \\(AUC \\propto Dose\\)\nIf F and Dose are constant, \\(AUC \\propto \\frac{1}{CL}\\)\n\n\n\n3.11.5 Average Bioequivalence\n\n\n3.11.6 Food Effects\n\nThis study helps labeling. Important parameters for Food Effect study:\n\nTotal exposure (\\(AUC_{last}, AUC_{\\infty}\\))\nPeak exposure (Cmax)\nPartial exposure (for MR formulations) (pAUC)\nTime to peak exposure (Tmax)\nLag-time (Tlag)\nTerminal elimination half-life (t1/2)\nApparent clearance (CL/F)\nApparent volume of distribution (V/F)\n\nIf 90% confidence interval for the fed/fasted ratio is wholly contained withing 80-125% then there is no significant food effect.\n\n\n\n\n\n\n3.11.7 Drug-Drug Interactions"
  },
  {
    "objectID": "WinNonlinIntro.html#create-tables",
    "href": "WinNonlinIntro.html#create-tables",
    "title": "3  Introduction to WinNonlin (122-D)",
    "section": "3.12 Create Tables",
    "text": "3.12 Create Tables\n\n3.12.1 Create Simple Table\n\n\n3.12.2 Use Stratification\n\n\n3.12.3 Add Statistics to Table\n\n\n3.12.4 Change Column/Sort Order\n\n\n3.12.5 Set Precision/Alignment\n\n\n3.12.6 Change Table Text"
  },
  {
    "objectID": "WinNonlinModel.html",
    "href": "WinNonlinModel.html",
    "title": "4  Introduction to PK Modeling",
    "section": "",
    "text": "5 Summary"
  },
  {
    "objectID": "WinNonlinModel.html#pk-model-1",
    "href": "WinNonlinModel.html#pk-model-1",
    "title": "4  Introduction to PK Modeling",
    "section": "5.1 pk model 1",
    "text": "5.1 pk model 1\n\n5.1.1 Concentrations should be stacked\ndata format for PK models - observed concentration stacked into a single column - multiple analytes or metabolites concentration should be its own column\n \nSort variables\n\nFor multiple PK profiles, one or more sort variables are required.\nSort variables need to have a value on every row\nIn Phoenix text values, blank cells are not a problem\nAll non-numeric data is ignored by the model\n\n\n\namount oral column has a value at time zero and the rest of the rows are blank. The dose amount is given only on the row the corresponds to a dosing event\ndose level column has a value on every row\nunits: unlike NCA the model object does not translate units\nThe dose unit should always be the same as the unit in the concentration\n\nAnd your parameter values keep them the same so that the units will work out properly One more warning the model object allows you to either map the dosing amounts in the main input or the dosing input Make sure that you don’t map the dose amount in both places or your administered dose will be twice as large as you intended click next to continue let’s recap the section we saw how the model object requires stacked concentrations you should have a single concentration column for each observation second we use sort variables to define the individual PK profiles there’s nothing wrong with having more sort variables than you need third we saw how dosing events can be included in the data set remember that these are entered on the row at the time of the dosing event and finally we learned that text values and empty cells are okay in the input and we do not have to do anything to them this completes the section click"
  },
  {
    "objectID": "TechNotes.html#courses",
    "href": "TechNotes.html#courses",
    "title": "5  My Notes",
    "section": "5.1 Courses",
    "text": "5.1 Courses\n\n(103-OD) Fundamentals of Pharmacokinetics [Conceptual Course]\n(105-OD) Noncompartmental Data Analysis [Conceptual Course]\n(122-OD) Introduction to Phoenix WinNonlin (Part 1) - NCA\n(123-OD) Introduction to Phoenix WinNonlin (Part 2) - Modeling\n\n\n5.1.1 create a new repository on the command line\necho \"# test\" >> README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/drnazmul/test.git\ngit push -u origin main\n\n\n5.1.2 …or push an existing repository from the command line\ngit remote add origin https://github.com/drnazmul/test.git\ngit branch -M main\ngit push -u origin main\nAfter any change made on github repo but not on the local folder, do the following\ngit pull --rebase origin main\ngit push origin main\n\n\n5.1.3 Publishing Quarto site or book\nPublish from Git Provider\nhttps://quarto.org/docs/publishing/netlify.html#publish-from-git-provider\n\n\n5.1.4 Custom domain on Netlify with Namecheap\n\nGo to Namecheap’s account, click Manage on the active domain\non “Name Server” select “Custom DNS” option\nGo to “Domain Settings” of Netlify\nCustom Domain : Options: Edit site name\nClick on “Add custom domain” and add your domain\nUnder “primary domain” options, click “Setup Netlify DNS”\nClick verify tab\nClick Add domain tab\nClick Continue\ncopy all the four name servers and paste to the DNS server on Namecheap\nClick Done on Netlify"
  },
  {
    "objectID": "TechNotes.html#pbpk-notes",
    "href": "TechNotes.html#pbpk-notes",
    "title": "5  My Notes",
    "section": "5.2 PBPK notes",
    "text": "5.2 PBPK notes\nhttps://www.admescope.com/whats-new/blog/2016/pbpk-what-it-is-for\nhttps://www.fda.gov/regulatory-information/search-fda-guidance-documents\nhttps://www.boomer.org/\nhttps://www.pkpd168.com/\nhttps://bebac.at/\nData: Collection of observations or facts. Can be text, number, audio, video, pics,\n\nWe encounter data 1. purposefully 2. Unintentionally\nUnderstanding the data we encounter everyday is where the data literacy comes in.\nData Literacy: how a person interacts with data to make sense or the world around them\nData literate person:\n\nrecognize usefulness of the data\nInterrogate reliability: When, where and how the data was collected?\nDiscover meaning\nMake decision\nCommunicate findings\nDo everything ethically\n\n5.2.1 Recognize usefulness of the data\nwhen you encounter a data: ask: Is this data relevant and useful to me? If so, how?\n% increase = (final value - initial value)/initial value * 100\nQuestions to ask:\nwhat’s the story the author is trying to make\nWhat data are they presenting and how are the presenting it?\nWhat is the agenda behind the information being presented\nhow might this data help others to make decision\nwhat do you like and what would you do differently?\n\nData: which is most useful? Which is not relavant?\nMake a plan with the questions\nScaptacle eays and curious mind are essential for data literate person"
  },
  {
    "objectID": "TechNotes.html#courses-to-take",
    "href": "TechNotes.html#courses-to-take",
    "title": "5  My Notes",
    "section": "5.3 Courses to take",
    "text": "5.3 Courses to take\n\nhttps://www.coursera.org/specializations/precalculus-data-modelling\nNONMEM paper Keizer, Karlsson, and Hooker (2013)\nPM and ML paper Keutzer et al. (2022)"
  },
  {
    "objectID": "TechNotes.html#latex-in-rmarkdown",
    "href": "TechNotes.html#latex-in-rmarkdown",
    "title": "5  My Notes",
    "section": "5.4 latex in rmarkdown",
    "text": "5.4 latex in rmarkdown\nhttps://rpruim.github.io/s341/S19/from-class/MathinRmd.html\n\n\n\n\nKeizer, R J, M O Karlsson, and A Hooker. 2013. “Modeling and Simulation Workbench for NONMEM: Tutorial on Pirana, PsN, and Xpose.” CPT: Pharmacometrics & Systems Pharmacology 2 (6): e50. https://doi.org/10.1038/psp.2013.24.\n\n\nKeutzer, Lina, Huifang You, Ali Farnoud, Joakim Nyberg, Sebastian G. Wicha, Gareth Maher-Edwards, Georgios Vlasakakis, et al. 2022. “Machine Learning and Pharmacometrics for Prediction of Pharmacokinetic Data: Differences, Similarities and Challenges Illustrated with Rifampicin.” Pharmaceutics 14 (8): 1530. https://doi.org/10.3390/pharmaceutics14081530."
  },
  {
    "objectID": "Rnotes.html#helpful-syntax",
    "href": "Rnotes.html#helpful-syntax",
    "title": "6  R-notes",
    "section": "6.1 Helpful Syntax",
    "text": "6.1 Helpful Syntax\n\n#Returns the structure and information of a given object\n\nstr(iris)\n\n#Returns the class of a given object\n\nclass(iris)\n\n#Returns your current working directory\n\ngetwd() \n\n#Changes your current working directory to a desired file path\n\nsetwd(\"/c/Users/BABA/Documents\")\n\n\n6.1.1 Installing and loading\n\n#Install dplyr through tidyverse \n\ninstall.packages(\"tidyverse\")\n\n# Install it directly\n\ninstall.packages(\"dplyr\")\n\n# Load dplyr into R\n\nlibrary(\"dplyr\")\nlibrary(\"tidyverse\")\n\nLoad the packages:\n\nlibrary(datasets) # Load the datasets\nlibrary(gapminder) \nattach(iris) # Attach iris data to r search path\n\n\n\n6.1.2 The %>% operator\n\n# Without the %>% operator\n\nselect(df, a, b)\n\n# By using the %>% operator\n\ndf %>%\n  select(a, b)\n\n\n\n6.1.3 Preloaded datasets\n\ndata()\n\n#Load and print mtcars data as follow:\n\ndata(\"mtcars\")\nhead(mtcars, 6)\n\n#To learn more about the data set:\n\n?mtcars\nnrow(mtcars)\nncol(mtcars)\n\n#Other popular dataset:\n  \ndata(\"iris\")\ndata(\"ToothGrowth\")\ndata(\"PlantGrowth\")\ndata(\"USArrests\")\n\n\n\n6.1.4 Basic column operations with dplyr\n\n# Select one or more columns with select()\n\niris %>%\n  select(Petal.Length, Petal.Width)\n \n# Select all but one column (e.g., listing_id)\n\niris %>%\n  select(-Petal.Width)\n\n# Select all columns within a range\n\niris %>%\n  select(Species:Petal.Length)\n\n# Rename a column using rename()\n\niris %>%\n  rename(p_length = Petal.Length)"
  },
  {
    "objectID": "Rnotes.html#working-with-columns",
    "href": "Rnotes.html#working-with-columns",
    "title": "6  R-notes",
    "section": "6.2 Working with columns",
    "text": "6.2 Working with columns\n\n6.2.1 Mutate\n\n# Change Sepal.Length to be in millimeters\n\niris %>% \n  mutate(Sepal.Length = Sepal.Length*10)\n\n# Create a new column called s_length\n\niris %>% \n  mutate(SLMm = Sepal.Length*2)\n\niris %>%\n  filter(Species==\"Virginica\") %>%\n  mutate(SLMm=Sepal.Length*10) %>%\n  arrange(desc(SLMm))\n\n\n# Add the number of observations for a column (e.g., number of Specirs)\n\niris %>% \n  add_count(Species)"
  },
  {
    "objectID": "Rnotes.html#working-with-rows",
    "href": "Rnotes.html#working-with-rows",
    "title": "6  R-notes",
    "section": "6.3 Working with rows",
    "text": "6.3 Working with rows\n\n6.3.1 Filter\n\n# Filter rows on one condition (e.g., Select iris data of species \"virginica\")\n\niris %>%\n  filter(Species==\"virginica\")\n\n# Filter on two OR more conditions (Species OR Petal Length)\n\niris %>%\n  filter(Species==\"virginica\" | Sepal.Length > 6)\n\n# Filter on two AND more conditions (country AND number_of_rooms)\n\niris %>%\n  filter(Species==\"virginica\", Sepal.Length > 6)\n\n\n# Filter by checking if a value exists in another set of values\n\niris %>% \n  filter(Species %in% c(\"versicolor\", \"virginica\"))\n\n\n\n6.3.2 Arrange\n\n# Sort rows by values in a column in ascending order\n\niris %>%\narrange(Petal.Length)\n\n# Sort rows by values in a column in descending order\n\niris %>%\narrange(desc(Petal.Length))\n\n# Combine multiple dplyr verb\n\niris %>% \n  filter(Species == \"virginica\") %>% \n  arrange(desc(Sepal.Length))\n\n\n\n6.3.3 Others\n\n# Remove duplicate rows in all the dataset\n\niris %>% \n  distinct()\n\n# Find unique values in the country column\n\niris %>% \n  distinct(Species)\n\n# Select rows based on top-n values of a column (e.g., top 3 listings with the highest amount  of rooms)\n\niris %>% \n\ntop_n(3, Petal.Length)"
  },
  {
    "objectID": "Rnotes.html#aggregating-data-with-dplyr",
    "href": "Rnotes.html#aggregating-data-with-dplyr",
    "title": "6  R-notes",
    "section": "6.4 Aggregating data with dplyr",
    "text": "6.4 Aggregating data with dplyr\n\n6.4.1 Count\n\n# Count groups within a column (e.g., count number of species in Species)\n\niris %>% \n  count(Species)\n\n# Count groups within a column and return sorted\n\niris %>% \n  count(Species, sort=TRUE)\n\n\n\n6.4.2 Summerise\n\n# Return the total sum of values for a column (e.g., total petal length)\n\niris %>% \n  summarise(total_length=sum(Petal.Length))\n\n# Return the average of values for a column (e.g, average petal length)\n\niris %>% \n  summarise(avg_length=mean(Petal.Length))\n\n#group_by() allows you to summarize within groups instead of summarizing the entire dataset\n\n# Group by a variable and return counts of each group (e.g., number of listings by country)\n\niris %>% \n  group_by(Species) %>%\n  summarise(n=n())\n\n# Group by a variable and return the average value per group (e.g., average number of rooms in listings per city)\n\niris %>%\n  group_by(Species) %>%\n  summarise(avg_length=mean(Petal.Length))\n\n# Find median and max sepal length of each species\n\niris %>% \n  group_by(Species) %>% \n  summarise(medianSL = median(Sepal.Length),\n            maxSL = max(Sepal.Length))\n# Find median and max petal length of each species with sepal length > 6\n\niris %>% \n  filter(Sepal.Length >6) %>% \n  group_by(Species) %>% \n  summarise(medianPL = median(Petal.Length), \n            maxPL = max(Petal.Length))"
  },
  {
    "objectID": "Rnotes.html#ggplot2",
    "href": "Rnotes.html#ggplot2",
    "title": "6  R-notes",
    "section": "6.5 ggplot2",
    "text": "6.5 ggplot2\n\n6.5.1 Scatter Plots\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width))+\n  geom_point()\n\n# Add color with another variable\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width,\n                       color = Species))+\n  geom_point()\n\n# Add color and size with two other variables\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width,\n                       color = Species, \n                       size = Sepal.Length))+\n  geom_point()\n\n# Faceting\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width))+\n  geom_point()+\n  facet_wrap(~Species)\n\n\n\n6.5.2 Line plots\n\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length))+\n  geom_line()+\n  expand_limits(y=0)\n\n\n\n6.5.3 Bar plots\n\nby_species <- iris %>% \n  filter(Sepal.Length >6) %>% \n  group_by(Species) %>% \n  summarize(medianPL = median(Petal.Length))\n\nggplot(by_species, aes(x = Species, y = medianPL))+\n  geom_col()\n\n\n\n6.5.4 Histograms\n\nggplot(iris_small, aes(x= Petal.Length))+\n  geom_histogram()\n\n\n\n6.5.5 Box plots\n\nggplot(iris_small, aes(x=Species, y=Sepal.Width))+\n  geom_boxplot()"
  },
  {
    "objectID": "Rintro.html#introduction-to-r",
    "href": "Rintro.html#introduction-to-r",
    "title": "7  R for Data Analyst",
    "section": "7.1 Introduction to R",
    "text": "7.1 Introduction to R\n\n7.1.1 Vectors\n\nnumeric_vector <- c(1, 10, 49)\ncharacter_vector <- c(\"a\", \"b\", \"c\")\nboolean_vector <- c(TRUE, FALSE, TRUE)\n\n\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\n\n#Assign the names of the day to 'roulette_vector' and 'poker_vector'\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\ntotal_daily <- poker_vector+roulette_vector\nprint(total_daily)\n\ntotal_poker <- sum(poker_vector)\ntotal_roulette <- sum(roulette_vector)\ntotal_week <- total_poker+total_roulette\nprint(total_week)\n\n\n# Check if you realized higher total gains in poker than in roulette \nanswer <- total_poker > total_roulette\nanswer\n\n#sub-setting vectors by index, by name, by comparison.\n\npoker_wednesday <- poker_vector[3]\npoker_midweek <- poker_vector[c(2, 3, 4)]\nroulette_selection_vector <- roulette_vector[2:5]\n\n# Selection by name\npoker_start <- poker_vector[c(\"Monday\", \"Tuesday\", \"Wednesday\")]\naverage_midweek_gain <- mean( poker_start)\nprint(average_midweek_gain)\n\n# sub-setting by comparison\nselection_vector <- poker_vector>0\nprint(selection_vector)\n\n\n# Select from poker_vector these days\npoker_winning_days <- poker_vector[selection_vector]\nprint(poker_winning_days)\n\n\n\n7.1.2 Matrices\n\nmatrix(1:9, byrow = TRUE, nrow = 3)\n\n# Box office Star Wars (in millions!)\nnew_hope <- c(460.998, 314.4)\nempire_strikes <- c(290.475, 247.900)\nreturn_jedi <- c(309.306, 165.8)\n\n# Create box_office\nbox_office <- c(new_hope, empire_strikes, return_jedi)\n\n# Construct star_wars_matrix\nstar_wars_matrix <- matrix(box_office, byrow = TRUE, nrow = 3)\n\nprint(star_wars_matrix)\n\n\n# Vectors region and titles, used for naming\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \"The Empire Strikes Back\", \"Return of the Jedi\")\n\n# Name the columns with region\ncolnames(star_wars_matrix) <- region\n\n# Name the rows with titles\nrownames(star_wars_matrix) <- titles\n\n# Print out star_wars_matrix\nprint(star_wars_matrix)\n\n\n# Construct star_wars_matrix\nbox_office <- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \n                 \"The Empire Strikes Back\", \n                 \"Return of the Jedi\")\n               \nstar_wars_matrix <- matrix(box_office, \n                      nrow = 3, byrow = TRUE,\n                      dimnames = list(titles, region))\n\n# Calculate worldwide box office figures\nworldwide_vector <- rowSums(star_wars_matrix)\n\n# Bind the new variable worldwide_vector as a column to star_wars_matrix\nall_wars_matrix <- cbind(star_wars_matrix, worldwide_vector)\n\n\nstar_wars_matrix  \nstar_wars_matrix2 \n\n# Combine both Star Wars trilogies in one matrix\nall_wars_matrix <- rbind(star_wars_matrix, star_wars_matrix2)\n\n# Total revenue for US and non-US\ntotal_revenue_vector <- colSums(all_wars_matrix)\n  \n# Print out total_revenue_vector\nprint(total_revenue_vector)\n\n\n# sub-setting matrix\n# use a comma to separate the rows you want to select from the columns\n# all_wars_matrix is available in your workspace\nall_wars_matrix\n\n# Select the non-US revenue for all movies\nnon_us_all <- all_wars_matrix[,2]\n  \n# Average non-US revenue\nmean(non_us_all)\n  \n# Select the non-US revenue for first two movies\nnon_us_some <- all_wars_matrix[1:2,2]\n  \n# Average non-US revenue for first two movies\nmean(non_us_some)\n\n# Estimate the visitors\nvisitors <- all_wars_matrix/5\n  \n# Print the estimate to the console\nvisitors\n\n\n\n7.1.3 Factors\n\n# Sex vector\nsex_vector <- c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\")\n\n# Convert sex_vector to a factor\nfactor_sex_vector <-factor(sex_vector)\n\n# Print out factor_sex_vector\nfactor_sex_vector\n\n# Animals - Nominal categorical variable\nanimals_vector <- c(\"Elephant\", \"Giraffe\", \"Donkey\", \"Horse\")\nfactor_animals_vector <- factor(animals_vector)\nfactor_animals_vector\n\n# Temperature - Ordinal categorical variable\ntemperature_vector <- c(\"High\", \"Low\", \"High\",\"Low\", \"Medium\")\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE, levels = c(\"Low\", \"Medium\", \"High\"))\nfactor_temperature_vector\n\n# FACTOR LEVELS\n\n# Code to build factor_survey_vector\nsurvey_vector <- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector <- factor(survey_vector)\n\n# Specify the levels of factor_survey_vector\nlevels(factor_survey_vector) <-c(\"Female\", \"Male\")\n\nfactor_survey_vector\n\n#Summarizing a factor\n\n# Generate summary for factor_survey_vector\nsummary(factor_survey_vector)\n\n# Note: R returns NA when you try to compare values in a factor,\n\n# Create speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\n\n# Convert speed_vector to ordered factor vector\nfactor_speed_vector <-factor(speed_vector, ordered = TRUE, levels = c(\"slow\", \"medium\", \"fast\"))\n\n# Print factor_speed_vector\nfactor_speed_vector\nsummary(factor_speed_vector)\n\n\n# Create factor_speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\nfactor_speed_vector <- factor(speed_vector, ordered = TRUE, levels = c(\"slow\", \"medium\", \"fast\"))\n\n# Factor value for second data analyst\nda2 <- factor_speed_vector[2]\n\n# Factor value for fifth data analyst\nda5 <- factor_speed_vector[5]\n\n# Is data analyst 2 faster than data analyst 5?\nda2>da5\n\n\n\n7.1.4 Data Frame\n\nhead(mtcars)\nstr(mtcars)\n\n# Definition of vectors\nname <- c(\"Mercury\", \"Venus\", \"Earth\", \n          \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype <- c(\"Terrestrial planet\", \n          \"Terrestrial planet\", \n          \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\", \"Gas giant\")\ndiameter <- c(0.382, 0.949, 1, 0.532, \n              11.209, 9.449, 4.007, 3.883)\nrotation <- c(58.64, -243.02, 1, 1.03, \n              0.41, 0.43, -0.72, 0.67)\nrings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n# Create a data frame from the vectors\nplanets_df <-data.frame(name, type, diameter, rotation, rings)\nplanets_df\n\n# Print out diameter of Mercury (row 1, column 3)\nplanets_df[1,3]\n\n# Print out data for Mars (entire fourth row)\nplanets_df[4,]\n\n# Select first 5 values of diameter column\nplanets_df[1:5, 3]\n\n# Select an entire column\n\nplanets_df[,3]\nplanets_df[,\"diameter\"]\nplanets_df$diameter\n\n\n# Select planets with diameter < 1\nsubset(planets_df, subset = diameter<1)\n\n# Use order() to create positions\npositions <- order(planets_df$diameter)\n\n# Use positions to sort planets_df\nplanets_df[positions, ]\npwd"
  },
  {
    "objectID": "Rintro.html#introduction-to-sampling",
    "href": "Rintro.html#introduction-to-sampling",
    "title": "7  R for Data Analyst",
    "section": "7.2 Introduction to sampling",
    "text": "7.2 Introduction to sampling\n\n7.2.1 Point estimates\n\n\n7.2.2 Random number generation\n\nView(dataset)\n# Generate random numbers from ...\nrandoms <- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform = runif(n_numbers, min = -3, max = 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\nNotice how the histograms almost take the flat and bell curve shapes of the uniform and normal distributions, but there is a bit of random noise.\nSetting the seed to a particular value means that subsequent random code that generates random numbers will have the same answer each time you run it.\n\n\n7.2.3 Bootstrapping\nThe bootstrapping workflow is to generate\n\na resample of the same size as the population,\ncalculate a summary statistic,\nthen repeat this to get a distribution of summary statistics.\n\nThe key to deciding whether to sample without or with replacement is whether or not your dataset is best thought of as being the whole population or not.\nTo make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.\nFrom the smaller sample of Spotify songs, we can estimate the mean danceability statistic in the population. Since we have a distribution of statistics, we can even quantify how accurate our estimate is.\nIf the sample is not closely representative of the population, then the mean of the bootstrap distribution will not be representative of the population mean. This is less of a problem for standard errors.\n\n# Generate a sampling distribution\nmean_sampling_dist <- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the population\n    spotify_population %>% \n      # Sample 500 rows without replacement\n      slice_sample(n = 500) %>% \n      # Calculate the mean popularity as mean_popularity\n      summarise(mean_popularity = mean(popularity)) %>% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# See the result\nmean_popularity_2000_samp\n\n\n# Generate a bootstrap distribution\nmean_bootstrap_dist <- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the sample\n    spotify_sample %>% \n      # Sample same number of rows with replacement\n      slice_sample(prop = 1, replace = TRUE) %>% \n      # Calculate the mean popularity\n      summarise(mean_popularity = mean(popularity)) %>% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# Store the resamples in a tibble\nbootstrap_distn <- tibble(\n  resample_mean = mean_danceability_1000\n)\n\n# Draw a histogram of the resample means with binwidth 0.002\nggplot(bootstrap_distn, aes(resample_mean))+\n  geom_histogram(binwidth=0.002)\n\nThe sampling distribution mean is the best estimate of the true population mean; the bootstrap distribution mean is closest to the original sample mean.\nThe sampling distribution mean can be used to estimate the population mean, but that is not the case with the boostrap distribution.\n\n# Calculate std from sampling_distribution\n\nsamp_distn_sd <- sampling_distribution %>% \n  summarize(sd(sample_mean) * sqrt(500))\n\n# Calculate std from bootstrap_distribution\n\nboot_distn_sd <- bootstrap_distribution %>% \n  summarize(sd(resample_mean) * sqrt(500))\n\n# See the results\nc(sam_distn = samp_distn_sd, boot_distn = boot_distn_sd)\n\nWhen you don’t have all the values from the true population, you can use bootstrapping to get a good estimate of the population standard deviation\n\n\n7.2.4 Confidence interval\nWhen reporting results, it is common to provide a confidence interval alongside an estimate.\nWhat does that confidence interval provide?\n\nA range of plausible values for an unknown quantity.\n\nConfidence intervals account for uncertainty in our estimate of a population parameter by providing a range of possible values. We are confident that the true value lies somewhere in the interval specified by that range.\nThe standard error method for calculating the confidence interval assumes that the bootstrap distribution is normal. This assumption should hold if the sample size and number of replicates are sufficiently large.\n\n# Generate a 95% confidence interval using the quantile method\nconf_int_quantile <- bootstrap_distribution %>% \n  summarize(\n    lower = quantile(resample_mean, 0.025),\n    upper = quantile(resample_mean, 0.975)\n  )\n\n# See the result\nconf_int_quantile\n\n# Generate a 95% confidence interval using the std error method\nconf_int_std_error <- bootstrap_distribution %>% \n  summarize(\n    point_estimate = mean(resample_mean),\n    standard_error = sd(resample_mean),\n    lower = qnorm(0.025, point_estimate, standard_error),\n    upper = qnorm(0.975, point_estimate, standard_error)\n  )\n\n# See the result\nconf_int_std_error\n\n\nthe standard deviation of a bootstrap distribution statistic is a good approximation for the standard error of the sampling distribution.\nyou calculated confidence intervals for statistics using both the quantile method and the standard error method, and they gave very similar answers. That means that the normal distribution is a good approximation for the bootstrap distribution.\n\n\n\n7.2.5 t test\nHypothesis testing workflow for the one sample case where you compared a sample mean to a hypothesized value, and the two sample case where you compared two sample means. In both cases, the workflow follows this format.\n\n\n\n7.2.6 Two sample mean test statistic\nThe hypothesis test for determining if there is a difference between the means of two populations uses t-scores, and can be calculated from three values from each sample using this equation.\nWhy is t needed?\nThe process for calculating p-values is\n1. to start with the sample statistic,\n2. standardize it to get a test statistic,\n3. then transform it via a cumulative distribution function (CDF).\nIn Chapter 1, that final transformation was denoted z, and the CDF transformation used the (standard normal) z-distribution.\nIn the last video, the test statistic was denoted t, and the transformation used the t-distribution.\nUsing a sample standard deviation to estimate the standard error is computationally easier than using bootstrapping. However, to correct for the approximation, you need to use a t-distribution when transforming the test statistic to get the p-value.\n\n# Calculate the numerator of the test statistic\nnumerator <- xbar_no - xbar_yes\n\n# Calculate the denominator of the test statistic\ndenominator <- sqrt(s_no^2/n_no + s_yes^2/n_yes)\n\n# Calculate the test statistic\nt_stat <- numerator/denominator\n\n# See the result\nt_stat\n# Calculate the degrees of freedom\ndegrees_of_freedom <- n_no+n_yes-2\n\n# Calculate the p-value from the test stat\np_value <- pt(t_stat, df = degrees_of_freedom, lower.tail = TRUE)\n\n# See the result\np_value"
  },
  {
    "objectID": "Rintro.html#hypothesis-testing-in-r",
    "href": "Rintro.html#hypothesis-testing-in-r",
    "title": "7  R for Data Analyst",
    "section": "7.3 Hypothesis testing in R",
    "text": "7.3 Hypothesis testing in R\n\n7.3.1 Calculating a z-score\nSince variables have arbitrary ranges and units, we need to standardize them. For example, it would be silly if a hypothesis test gave a different answer if your variables were in Euros instead of US dollars. Standardization avoids that.\nOne standardized value of interest in a hypothesis test is called a z-score. To calculate it, we need three numbers:\n\nSample statistic (point estimate),\nHypothesized statistic,\nStandard error of the statistic (which we estimate from the bootstrap distribution).\n\n\n# View the late_shipments dataset\nView(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp <- late_shipments %>% \n  summarize(prop_late_shipments = mean(late == \"Yes\")) %>% \n  pull(prop_late_shipments)\n\n# See the results\nlate_prop_samp\n\n# Hypothesize that the proportion is 6%\nlate_prop_hyp <- 0.06\n\n# Calculate the standard error\nstd_error <- late_shipments_boot_distn %>% \n  summarize(sd_late_prop = sd(late_prop)) %>% \n  pull(sd_late_prop)\n\n# Find z-score of late_prop_samp\nz_score <- (late_prop_samp - late_prop_hyp) / std_error\n\n# See the results\nz_score\n\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic.\n\n\n7.3.2 p value\nA hypothesis is a statement about a population parameter. We don’t know the true value of this population parameter; we can only make inferences about it from the data. Hypothesis tests compare two competing hypotheses.\nRather than saying we accept the alternative hypothesis, the verdicts are rejecting the null hypothesis, or failing to reject the null hypothesis.\nThe hypothes is testing equivalent of “beyond a reasonable doubt” is known as the significance level.\nThe tails of the distribution that are relevant depend on whether the alternative hypothesis refers to “greater than”, “less than”, or “differences between”.\n\n# Calculate the p-value\np_value <- pnorm(z_score, lower.tail = FALSE)\n                 \n# See the result\np_value   \n\nThe p-value is calculated by transforming the z-score with the standard normal cumulative distribution function (CDF).\n\n\n7.3.3 Statistical significance\nWhat defines the cutoff point between a small p-value and a large one?\nSignificance level\nThe cutoff point is known as the significance level,\\(\\alpha\\). The appropriate significance level depends on the dataset and the discipline you are working in. Five percent is the most common choice, but ten percent and one percent are also popular. The significance level gives us a decision process for which hypothesis to support.\nIf the p-value is low, H0 must go (reject H0 )\nIf the p-value is high, H0 must fly (fail to reject H0)\nIt’s important that you decide what the appropriate significance level should be before you run your test. Otherwise there is a temptation to decide on a significance level that lets you choose the hypothesis you want.\n\n\n7.3.4 Confidence intervals\nTo get a sense as to potential values of the population parameter, it’s common to choose a confidence interval of one minus the significance level. For a significance level of 0.05 e, we’d use a 95% confidance interval. Here’s the calculation using the quantile method.\n\n# Calculate 95% confidence interval using quantile method\nconf_int_quantile <- late_shipments_boot_distn %>%\n  summarize(\n    lower = quantile(prop_late_shipments, 0.025),\n    upper = quantile(prop_late_shipments, 0.975)\n  )\n\n# See the result\nconf_int_quantile\n\nThe interval runs from 0.369 to 0.407 giving a range of plausible values for the proportion of data scientists starting programming as children.\nCalculating confidence intervals\nIf you give a single estimate of a sample statistic, you are bound to be wrong by some amount. For example, the hypothesized proportion of late shipments was 6%. Even if evidence suggests the null hypothesis that the proportion of late shipments is equal to this, for any new sample of shipments, the proportion is likely to be a little different. Consequently, it’s a good idea to state a confidence interval. That is, you say “we are 95% ‘confident’ the proportion of late shipments is between A and B” (for some value of A and B).\nSampling in R demonstrated two methods for calculating confidence intervals. Here, you’ll use quantiles of the bootstrap distribution to calculate the confidence interval.\nWhen you have a confidence interval width equal to one minus the significance level, if the hypothesized population parameter is within the confidence interval, you should fail to reject the null hypothesis.\n\n\n7.3.5 Proportion Tests\nThe t-test is needed for tests of mean(s) since you are estimating two unknown quantities, which leads to more variability.\n\n\n7.3.6 Non-parametric tests\nAssumptions in hypothesis testing\nEach hypothesis test makes assumptions about the data. It’s only when these assumptions are met that it is appropriate to use that hypothesis test.\nRandomness\nWhether it uses one or two samples, every hypothesis test assumes that each sample is randomly sourced from its population. If you don’t have a random sample, then it won’t be representative of the population. To check this assumption, you need to know where your data came from. There are no statistical or coding tests you can perform to check this. If in doubt, ask the people involved in collecting the data, or a domain expert that understands the population being sampled.\nIndependence of observations\nTests also assume that each observation is independent. There are some special cases like paired t-tests where dependencies between two samples are allowed, but these change the calculations so you need to understand where such dependencies occur. As you saw with the paired t-test, not accounting for dependencies results in an increased chance of false negative and false positive errors. This is also a difficult problem to diagnose after you have the data. It needs to be discussed before data collection.\nLarge sample size\nHypothesis tests also assume that your sample is big enough. Smaller samples incur greater uncertainty, and mean that the Central Limit Theorem doesn’t apply, which in turn means that the sampling distribution might not be normally distributed. The increased uncertainty means you get wider confidence intervals on the parameter you are trying to estimate. The Central Limit Theorem not applying means the calculations on the sample could be nonsense, which increases the chance of false negative and positive errors. The check for “big enough” depends on the test and that’s where we’ll head next.\nLarge sample size: t-test\nFor one sample t-tests, a popular heuristic is that you need at least thirty observations in your sample. For the two sample case or ANOVA, you need thirty observations in each. That means you can’t compensate for one small group sample by making the other one bigger. In the paired case, you need thirty pairs of observations.\nLarge sample size: proportion tests\nFor one sample proportion tests, the sample is considered big enough if it contains at least ten successes and ten failures. Notice that if the probability of success is close to zero or close to one, then you need a bigger sample. In the two sample case the size requirements apply to each sample separately.\nLarge sample size: chi-square tests\nThe chi-square test is slightly more forgiving and only requires five successes and failures in each group rather than ten.\nSanity check\nOne more check you can perform is to calculate a bootstrap distribution and visualize it with a histogram. If you don’t see a bell-shaped normal curve, then one of the assumptions hasn’t been met. In that case, you should revisit the data collection process, and see if any of the three assumptions of randomness, independence, and sample size do not hold."
  },
  {
    "objectID": "Rintro.html#foundations-of-inference",
    "href": "Rintro.html#foundations-of-inference",
    "title": "7  R for Data Analyst",
    "section": "7.4 Foundations of Inference",
    "text": "7.4 Foundations of Inference\n\n7.4.1 NHANES home owner’s data\n\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(infer)\n\nlibrary(NHANES)\n# What are the variables in the NHANES dataset?\n\ncolnames(NHANES)\n\nlet’s investigate the relationship between gender and home ownership calculate the original observed statistic\n\nhomes <- NHANES %>% \n  select(Gender, HomeOwn) %>% \n  filter(HomeOwn %in% c(\"Own\", \"Rent\"))\nhomes\n\n\ndiff_orig <- homes %>% \n  group_by(Gender) %>% \n  summarise(prop_org = mean(HomeOwn == \"Own\")) %>% \n  summarise(diff(prop_org)) %>% \n  pull()\n\nRandomized data under null model of independence\n\nhomeown_perm <- homes %>% \n  specify(HomeOwn ~ Gender, success = \"Own\") %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n#the distribution of randomized statistics\nggplot(homeown_perm, aes(x = stat))+\n  geom_dotplot(binwidth = 0.001)\n\ngeom density\nDo the data come from the population?\n\nggplot(homeown_perm, aes(x = stat))+\n  geom_density()+\n  geom_vline(aes(xintercept = diff_orig), color = \"red\")\n\nCompare permuted differences to observed difference\n\nhomeown_perm %>%\n  summarize(n_perm_le_obs = sum(stat <= diff_orig))\n\n223 permuted differences are more extreme than the observed difference. This only represents 21.2% of the null statistics, so you can conclude that the observed difference is consistent with the permuted distribution.\n\n\n7.4.2 Gender Discrimination\n\nlibrary(openintro)\n\ndisc <- sex_discrimination\n\ndisc %>% \n  count(sex, decision)\n\ndisc_orig <- disc %>% \n  group_by(sex) %>% \n  summarise(promoted_prop = mean(decision == \"promoted\")) %>% \n  summarise(diff(promoted_prop)) %>% \n  pull()\ndisc_orig\n\nAs the first step of any analysis, you should look at and summarize the data. Categorical variables are often summarized using proportions, and it is always important to understand the denominator of the proportion.\n\ndisc_perm <- disc %>% \n  specify(decision ~ sex, success = \"promoted\") %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\ndisc_perm\n\n\nggplot(disc_perm, aes(x = stat))+\n  geom_histogram(binwidth = 0.01)+\n  geom_vline(aes(xintercept = disc_orig), color = \"red\")\n\np-value measures the degree of disagreement between the data and the null hypothesis. here, you’re only interested in the one-sided hypothesis test here. That is, you’re trying to answer the question, “Are men more likely to be promoted than women?”\n\ndisc_perm %>% \n  visualise(obs_stat = disc_orig, direction = \"greater\")\n\ndisc_perm %>% \n  get_p_value(obs_stat = disc_orig, direction = \"greater\" )\n\nCalculating p values\n\ndisc_perm %>%\n  summarize(p_value = mean(disc_orig >= stat))\n\nhow the p-value is computed here. First we identify permuted differences that are larger than or equal to the observed statistic and label those situations with a 1, all other permutations receiving a zero. By averaging the 0s and 1s, the mean gives the proportion of times the permuted difference is larger than or equal to the observed difference. Because (point) 03 is less than (point) 05, we reject the null hypothesis and claim that men are promoted at a higher rate than women. That is, we conclude that it was not simply random variability which led to a higher proportion of men being promoted. A p-value of (point) 03 is reasonably close to (point) 05 which means we should be somewhat careful in making strong claims.\n\n\n7.4.3 Opportunity Cost\n\nopportunity_cost %>% \n  count(group, decision)\n\nopportunity_cost %>% \n  group_by(group) %>% \n  summarise(buy_prop = mean(decision == \"buy video\"))\n\nThe barplot better displays the results from the study. The treatment seems like it might have had an effect.\n\n# Plot group, filled by decision\nggplot(opportunity_cost, aes(x = group, fill = decision)) + \n  # Add a bar layer, with position \"fill\"\n  geom_bar(position = \"fill\")\n\n\n# Point estimate\n\n# Calculate the observed difference in purchase rate\ndiff_obs <- opportunity_cost %>%\n  # Group by group\n  group_by(group) %>%\n  # Calculate proportion deciding to buy a DVD\n  summarise(prop_buy = mean(decision == \"buy video\")) %>%\n  # Calculate difference between groups\n  summarise(stat = diff(prop_buy)) %>% \n  pull()\ndiff_obs\n\n\n# Create data frame of permuted differences in purchase rates\nopp_perm <- opportunity_cost %>%\n  # Specify decision vs. group, where success is buying a DVD\n  specify(decision ~ group, success = \"buy video\") %>%\n  # Set the null hypothesis to independence\n  hypothesise(null = \"independence\") %>%\n  # Generate 1000 reps of type permute\n  generate(reps = 1000, type = \"permute\") %>%\n  # Calculate the summary stat difference in proportions\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\"))\n    \n# Review the result\nopp_perm\n\n\n# Using the permuation data, plot stat\nggplot(opp_perm, aes(x = stat)) + \n  # Add a histogram layer with binwidth 0.005\n  geom_histogram(binwidth = 0.005) +\n  # Add a vline layer with intercept diff_obs\n  geom_vline(aes(xintercept = diff_obs), color = \"red\")\n\nyou’ll calculate the p-value to judge if the difference in proportions permuted is consistent with the observed difference.\nNow that you’ve created the randomization distribution, you’ll use it to assess whether the observed difference in proportions is consistent with the null difference. You will measure this consistency (or lack thereof) with a p-value, or the proportion of permuted differences less than or equal to the observed difference.\nvisualize and get_p_value using the built in infer functions. Remember that the null statistics are above the original difference, so the p-value (which represents how often a null value is more extreme) is calculated by counting the number of null values which are less than the original difference.\n\n# Visualize the statistic \nlibrary(infer)\nopp_perm %>%\n  visualize()+\n  shade_p_value(obs_stat =diff_obs, direction = \"left\" )\n\n# Calculate the p-value using `get_p_value`\nopp_perm %>%\n  get_p_value(obs_stat = diff_obs, direction = \"left\")\n\n# Calculate the p-value using `summarize`\nopp_perm %>%\n  summarize(p_value = mean(stat <= diff_obs))\n\nThe small p-value indicates that the observed data are inconsistent with the null hypothesis. We should reject the null claim and conclude that financial advice does affect the likelihood of purchase.\n\n\n7.4.4 Error in hypothesis testing\nthere are two possible decisions to make in hypothesis testing. 1. Either the observed data are inconsistent with the null hypothesis, in which case the null hypothesis is rejected.\n\nOr the observed data are consistent with the null hypothesis,in which case the null hypothesis is not rejected and no conclusion is made about a larger population.\n\nThere are also two possible “truth” states: either the null hypothesis is true or the alternative hypothesis is true.\n\nthe goal of the scientific study is to be in the bottom box where the alternative hypothesis is true and the data provide convincing evidence to reject the null hypothesis.\nHowever, any of the other three boxes are also possible. We cannot know which row has resulted, but we do know which conclusion has been made, thereby specifying the column.\nWhich is to say, if the null hypothesis is rejected, then either the science is correct or a type I error has been made. If the null hypothesis is not rejected, it has either been done so correctly or a type II error has been made. Recall that the decision being made controls the type I error rate, that is the false positive rate, at, for example, (point) 05, for both mathematical and historical reasons.\n\n\n7.4.5 Parameters and Confidence Interval\nUntil now, the research question at hand has been a question of comparison. What if, instead, the research question is one of estimation?\nFor example, “under which diet plan will participants lose more weight on average” is a comparative question and we use a hypothesis test. “How much should participants expect to lose on average” is an estimation question for which we use confidence intervals. Or\nanother example is the comparative question: “which of two car manufacturers are drivers more likely to recommend to their friends?” Hypothesis testing is used to analyze that question. But: “what percent of users are likely to recommend Subaru to their friends” is an estimation problem and we use confidence intervals to answer that question.\nOne more, the comparative question: “are education level and average income linearly related” is addressed with a hypothesis test. The estimation question: “for each additional year of education, what is the predicted average income” uses a confidence interval. OK, you see the pattern.\nParameter\nFor each of the estimation problems, we need to understand what a parameter is. A parameter is a numerical value from the population.\nSo in the first example, the parameter is the true average amount that all dieters will lose on a particular program. In the second example, the parameter is the proportion of individuals in the population who recommend Subaru cars. And the last parameter is the average income of all individuals in the population with a particular education level.\nConfidence interval\nA confidence interval is a range of numbers that hopefully captures the true parameter value of interest.\nFor example, at the end of the course, we’ll be able to make conclusions along the lines of “we are 95% confident that somewhere between 12% and 34% of the entire population recommends Subarus.” That is, the goal in creating a confidence interval is to calculate a range of plausible values for the parameter of interest.\n\n# Compute p-hat for each poll\nex1_props <- all_polls %>% \n  # Group by poll\n  group_by(poll) %>% \n  # Calculate proportion of yes votes\n  summarize(stat = mean(vote == \"yes\"))\n  \n# Review the result\nex1_props"
  },
  {
    "objectID": "Rintro.html#introduction-to-regression-with-r",
    "href": "Rintro.html#introduction-to-regression-with-r",
    "title": "7  R for Data Analyst",
    "section": "7.5 Introduction to Regression with R",
    "text": "7.5 Introduction to Regression with R\nRegression models are a class of statistical models that let you explore the relationship between a response variable and some explanatory variables.\nLinear regression is used when the response variable is numeric, like in the motor insurance dataset. Logistic regression is used when the response variable is logical. That is, it takes TRUE or FALSE values. We’ll limit the scope further to only consider simple linear regression and simple logistic regression. This means you only have a single explanatory variable.\nit’s a good idea to visualize your dataset. To visualize the relationship between two numeric variables, you can use a scatter plot.\nadd a trend line to the scatter plot. A trend line means fitting a line that follows the data points. In ggplot, trend lines are added using geom_smooth(). Setting the method argument to “lm”, for “linear model” gives a trend line calculated with a linear regression.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(fst)\n\ntaiwan_real_estate <- read.fst(\"data/taiwan_real_estate.fst\")\n\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq))+\n  geom_point(alpha = 0.5)+\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n\nStraight lines are completely defined by two properties. The intercept is the y value when x is zero. The slope is the steepness of the line, equal to the amount y increases if you increase x by one. \\[ y = intercept + slope * x \\]\n\nmdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\nThe intercept is positive, so a house with no convenience stores nearby still has a positive price. The coefficient for convenience stores is also positive, so as the number of nearby convenience stores increases, so does the price of the house.\n\n7.5.1 Categorical explanatory variables\nTo visualize the data, scatter plots aren’t ideal because species is categorical. Instead, we can draw a histogram for each of the species.\nLet’s calculate some summary statistics. First we group by species then we summarize to calculate their mean masses.\n\n# Using taiwan_real_estate, plot price_twd_msq\nggplot(taiwan_real_estate, aes(price_twd_msq)) +\n  # Make it a histogram with 10 bins\n  geom_histogram(bins = 10) +\n  # Facet the plot so each house age group gets its own panel\n  facet_wrap(~ house_age_years)\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\nCalculating means by category\n\nsummary_stats <- taiwan_real_estate %>% \n  # Group by house age\n  group_by(house_age_years) %>% \n  # Summarize to calculate the mean house price/area\n  summarize(mean_by_group = mean(price_twd_msq))\n\n# See the result\nsummary_stats\n\nlm() with a categorical explanatory variable\n\n# Run a linear regression of price_twd_msq vs. house_age_years\nmdl_price_vs_age <- lm(\n  price_twd_msq ~ house_age_years, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_age\n\n\n# Update the model formula to remove the intercept\nmdl_price_vs_age_no_intercept <- lm(\n  price_twd_msq ~ house_age_years +0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_age_no_intercept\n\nThis is a reassuringly boring result. When you only have a single, categorical explanatory variable, the linear regression coefficients are the means of each category.\n\n\n7.5.2 Predictions\nThe big benefit of running models rather than simply calculating descriptive statistics is that models let you make predictions. Before we can make predictions, we need a model.\nThe principle behind predicting is to ask questions of the form “if I set the explanatory variables to these values, what value would the response variable have?”.\n\nlibrary(tibble)\n# From previous steps\nexplanatory_data <- tibble(\n  n_convenience = 0:10\n)\n\n# Edit this, so predictions are stored in prediction_data\npredict(mdl_price_vs_conv, explanatory_data)\n\n# See the result\nprediction_data <- explanatory_data %>%\nmutate(\n  price_twd_msq = predict(mdl_price_vs_conv, explanatory_data)\n)\n\nHaving the predictions in a data frame will make it easier to visualize them.\n\n# Add to the plot\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add a point layer of prediction data, colored yellow\n  geom_point(\n    data = prediction_data, color = \"yellow\"\n  )\n\n\nsummary(mdl_price_vs_conv)\n\n\n\n7.5.3 Using Broom Package\n\nlibrary(broom)\ntidy(mdl_price_vs_conv)\naugment(mdl_price_vs_conv)\nglance(mdl_price_vs_conv)\n\n\n\n7.5.4 Assessing model fit\nQuantifyig model fit\n\nr-squared or the coefficient of determination\n\nIt is defined as the proportion of the variance in the response variable that is predictable from the explanatory variable.\nglance from broom and call r-squared\nFor simple linear regression, the interpretation of the coefficient of determination is straightforward. It is simply the correlation between the explanatory and response variables, squared.\n\nResidual standard error (RSE)\n\nThe RSE is, very roughly speaking, a measure of the typical size of the residuals. That is, how much the predictions are typically wrong by. It has the same unit as the response variable.\nResidual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it’s a measure of how badly wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.\n\nmdl_price_vs_conv %>% \n  glance() %>% \n  pull(sigma, r.squared)\n# sigma is RSE\n\n\nRoot-mean-square error (RMSE)\n\nYou need to be aware that RMSE exists, but typically you should use RSE instead.\n\nad_conversion <- read.fst(\"data/ad_conversion.fst\")\n\nmdl_nclick_impression <- lm(n_impressions ~ n_clicks, data = ad_conversion)\n\nmdl_nclick_impression %>% \n  glance() %>% \n  pull(sigma)\n\n\nlibrary(ggfortify)\nautoplot(mdl_nclick_impression, which= 1:3, nrow = 3, ncol =1)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Keizer, R J, M O Karlsson, and A Hooker. 2013. “Modeling and\nSimulation Workbench for NONMEM: Tutorial on Pirana, PsN, and\nXpose.” CPT: Pharmacometrics & Systems Pharmacology\n2 (6): e50. https://doi.org/10.1038/psp.2013.24.\n\n\nKeutzer, Lina, Huifang You, Ali Farnoud, Joakim Nyberg, Sebastian G.\nWicha, Gareth Maher-Edwards, Georgios Vlasakakis, et al. 2022.\n“Machine Learning and Pharmacometrics for Prediction of\nPharmacokinetic Data: Differences, Similarities and Challenges\nIllustrated with Rifampicin.” Pharmaceutics 14 (8):\n1530. https://doi.org/10.3390/pharmaceutics14081530."
  }
]