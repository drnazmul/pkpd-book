[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pkpd notes",
    "section": "",
    "text": "Preface\nThis page is from index page"
  },
  {
    "objectID": "NCAintroduction.html#presenting-pk-results",
    "href": "NCAintroduction.html#presenting-pk-results",
    "title": "1  Noncompartmental Data Analysis (105-OD)",
    "section": "1.1 Presenting PK results",
    "text": "1.1 Presenting PK results\n\nIndividual observations\nCalculated PK parameter results\nStatistical results of comparisons\nDiscussion of results\nRelevant conclusions (that addresses the study objective)\n\nHow to present concentration data\n\nIndividual concentration values in listings\nIndividual concentration-time plots (both linear and semi-logarithomic)\nCombined plot of all individual concentration time data\nSummary statistics by nominal time points\n\nExample of individual listing:\n\n\n\n\nPK parameters\n\nIndividual parameter values in listing\nsummary statistics by treatment (for small data, include individual parameter values)\nGraphical presentation of the parameters are useful\n\n\nProper use of descriptive statistics\nDiscrete variables: Variables that can only take on a certain number of values (eg whole number). Main statistics used are: N, minimum, maximum, median\n\nmean and SD don’t make sense for discrete variables\ntmax and tlag are discrete variables because they can only take on values of the specific blood draw times\n\nContinues variables\nVariables that can take on any value within a range of values (eg wright)"
  },
  {
    "objectID": "WinNonlinIntro.html#project-setup",
    "href": "WinNonlinIntro.html#project-setup",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.1 Project setup",
    "text": "2.1 Project setup\n\nCreate Project\n\nPhenoex Projects\n\ncontains all the data and calculations\nmultiple projects can be open at the same time\ncommon data file to import: excel, csv\nolder version can always be opened by newer version\nnewer version projects can not be opened by older ones\nCreate a project\nClick to history tab to see the project history\nClick to properties tab, where most of the work is done\n\n\nCreate Worksheets\n\n\nRight click on Data, select New, select Worksheet\nAdd columns, give column name, assign data type\nassign units: select time from list of columns, click Unit Builder button, specify h to the time, click Add button,click OK\nfor dose, specify mass prefex, click Add\nType numbers on the worksheet to add values to the cells\n\n\nImport Files\n\n\nfile type: xls, xlsx, csv, SAS\ntypical data file contains header and unit row.\nSelect the Import button, select the file\non the File import wizard, select appropriate options\nPreview area helps to see the changes\nIf units are in the column header, select “has units in the header”\n\nExcels with multiple worksheet:\n\nclick the arrow to move on the Wizard to the next worksheet\n\n\nSave Projects\n\n\nClick the save icon on the toolbar\nFile name can be completely different from the Project name\nNo auto-save options\nsharing project file will also share the embedded data files\nClose project by right clicking the project name\nAfter opening a saved project folder, expand the plus sign to see the contents.\n\n\nSet Project Preferences\n\n\nSelect the Edit menu -> preferences -> Projects\nCheck Autosave on execution\nupdate the save locations and hit apply before clicking on OK."
  },
  {
    "objectID": "WinNonlinIntro.html#create-and-modify-worksheets",
    "href": "WinNonlinIntro.html#create-and-modify-worksheets",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.2 Create and Modify Worksheets",
    "text": "2.2 Create and Modify Worksheets\n\nSort Rows\n\n\ndata can be sorted by subject, dose level,\nsort button on every worksheet\nUse the “sort worksheet” window to apply sort options\n\n\nMove Columns\n\n\nSelect a column from the column list\nClick the up or down arrow to move\n\n\nRename Columns\n\n\nClick the column name, type F2 or double click on it to edit the name\n\n\nApply Units\n\n\nSelect column\nClick the Unit Builder\nClick Clear Units\nAdd units\n\n\nConvert Units\n\n\nconvert amount column from microgram to miligram\nclick the Amount column\ntype mg in the New unit box,click OK\nTo convert ng/mL to nmole/mL, add nmol and then click the slash button, specify the volume unit, enter molecular weight, click OK.\nBetter way: use the Data Wizard to convert the units."
  },
  {
    "objectID": "WinNonlinIntro.html#plot-data",
    "href": "WinNonlinIntro.html#plot-data",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.3 Plot Data",
    "text": "2.3 Plot Data\n\nCreate Simple Plot\n\n\nData: Conc, Time, dose level: 16 mg, 10 subjects\nright click the worksheet\nSelect send to -> plotting -> xy plot\nXY object is created with the linked data source\nOn the mapping window, orange column headers are required mappings\nmap, x -> Time, y -> conc, Group -> subject\nclick execute\nOptions pan - Axes - Y - select log button\nOptions pan - Graphs - rename by typing F2\nGraph name and legend names are the same\n\n\nCreate Lattice Plot\n\n\nData: Conc, Time, Administration, dose level 4 mg for IV, 8 mg for PO, 10 subjects\nCreate a XY plot object same as above\nmap x - Time, y - conc, group - subject, lattice column - administration\nExecute and get two plots\nOptions - range - ‘auto scale best’ settings scales individual plots are independent\n\n\nUse Second Y Axis\n\n\nData: plasma conc, urine conc, time, 10 subjects\nCreat XY plot object same as above\nmap x - Time, y - plasma conc, y2 - urine conc, lattice condition, page (sort) - Subject\nplots are on a single page for each subject\nOptions - select plasma_conc vs Time, type F2, change the name to Plasma, do the same for Urine\nExecute\n\n\nCompute Descriptive Statistics\n\n\nNeeded to create a plot with mean and error bars\nright click on data sheet, send to - computation tools - Discriptive statistics\nmap summary - conc, sort - Time\nExecute\nOptions pannel - click Clear All - click basic statistics, check Mean and SD\n\n\nUse Error Bars\n\n\nDiscriptive Satistics object - Output data - right click on Statistics\nsend to plotting - XY plot\nmap x - Time, y - Mean, Error bars, lower - SD, Error bars, upper - SD\nExecute\nset Y axis to log scale\n\n\nCreate Overlay Plot\n\n\nduplicate the error bars plot from previous section\nOptions pan - Plot - Graphs tab - click Add button\nSelect the new second input from the setup tab\nLink the source data by clicking source button,\nmap x - Time, y - conc, group - subject\nExecute\nOptions pan - select Conc vs Time plot\nSelect Quick Styles\nUncheck Group by lines, uncheck Gourp by colors,\nSelect Apprance tab\nSpecify color to Silver\nUncheck Markers visible\nNow all the individual lines are silver color\nSelect Mean vs Time graph\nSelect Appearance, specify line colors to red, Marker border color - red, line weight 3\nUnder the Mean vs Time graph, select the Error bars\nSelect Appearance, color - red\nOptions pan - select Y axis - select Axis label and update\nOptions pan - select Legend - uncheck Visible\n\n\nCreate Box Plot\n\n\n\n\n\n\n\n20 subjects, AR: accumulation ratio (how much accumulated under repeated ss), dose level, 2 mg and 4 mg\nIs AR increases with increasing dose level?\nright click data, send to - plotting - box plot\nmap y - AR, group - dose level\nExecute\n\n\nCreate Plot with Categorical X Axis\n\n\nData: Severity (Mild, Moderate), dose level (1, 2, 4, 8, 16, 32 mg), frequency (numerical, i.e., 0, 0.2, 0.6)\nright click the data sheet, send to plotting - X-categorical XY plot\nmap x - severity, y - frequency, group - dose level\nOptions pan, select X axis, select Order tab, change order if needed\nOptions pan - Frequency vs Severity graph - check line visible - now points are connected by a line\n\n\nSet Plot Preferences\n\n\nOptions pan - Plot - Layout \nEdit menu bar, preferences, plotting details\nChanging prefernences affect all new plots"
  },
  {
    "objectID": "WinNonlinIntro.html#introduction-to-nca",
    "href": "WinNonlinIntro.html#introduction-to-nca",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.4 Introduction to NCA",
    "text": "2.4 Introduction to NCA\n\n2.4.1 About NCA\nNon-compartmental analysis or NCA is a method for quantifying drug exposure\n\nNCA determines a large number of pharmacokinetic descriptors or PK parameters for a drug\nThey are not really parameters as you would have in a model\nNCA does not use any kind of model other than assuming that the elimination can be described by first order kinetics\nbecause there is no model at the heart of the method we cannot really use it for predictions\nAn example plot of concentration over time following an extravascular dose NCA will give us two different measures of drug exposure:\n\nthe peak exposure to the drug concentration occurring after dosing\nThe overall exposure is measured by computing the area under the curve or AUC\nan extra vascular dose starts with a concentration of zero, the concentration rises rapidly reaches Cmax and then decreases\n\n\nwith extra vascular dosing there is an absorption process that leads to a maximum concentration followed by elimination\nIV bolus dosing: drug is directly injected all at once into a vein; the mixing and systemic circulation is very fast and by the time the first sample is taken after dosing the mixing is assumed to be complete. The concentration starts high and then decreases as the drug is eliminated.\nIV infusion: the concentration starts at zero and then rises if the infusion is continued for long enough the concentration approaches a plateau at steady state when the infusion stops the concentration then falls in the same manner as in ivy bolus dosing\nplotting on a log scale is useful because it usually shows linear elimination in each case regardless of the dosing root we could fit the linear portion with a straight line to predict what will happen to concentration after we’ve collected the last sample concentration on the log axis\n\n\n\nIt is useful to have both linear and log plots. Linear plots are useful for examining the peak concentration and log plots are useful for the low concentrations\nIn addition to an elimination phase many drugs also show a distribution phase in such cases there may be two distinctive straight line sections on the plot. Although sometimes the two phases blend into a general curvature in the plots we see here the distribution phase is apparent for all three dosing routs but it is most pronounced for the IV bolus dosing. For extravascular dosing the distribution phase may be obscured by the drug absorption.\nThe AUC can be determined no matter how complex the relationship between concentration and time.\nSummary:\nNCA is the primary method of assessing drug exposure.\nCmax is a measure of peak exposure\nAUC is a measure of the overall exposure to the drug\ndifferent dosing route leads to a curve with the distinctive shape that plotting on a log concentration scale usually shows linear elimination\nmany drugs show a distribution phase as well as an elimination phase\n\n\n\n\n2.4.2 Observe Parameters\n\nFrom the plot of concentration versus time, we can see that the maximum concentration is reached at about 1 hour, we call that time Tmax and the concentration at the peak is Cmax\nTmax and Cmax are listed in the output of NCA in Phoenix\nAt some point after dosing we will have our last observed concentration this may be because we have stopped collecting samples or the concentration may have dropped below the quantification limit for the analysis and therefore we were unable to get more values The point is at a time of t last and has a concentration of Tlast These observeed parameters are affected by the sampling schedule we can improve our chances by sampling richly around the expected time of c max if we have more points we have a better chance of capturing a concentration that is near the true maximum\n\nSummarize\n\nObserved parameters are TMax Cmax TLast and Clast. We call these observed parameters because they are found directly in the observations\nthe observed parameters are dependent on sampling times\nsample richly around the expected time of Cmax so you can have a better chance of capturing something close to the true maximum\n\n\n\n2.4.3 Half-Life\n\ntime it takes for the concentration to decrease by 50%.\na long half-life leads to a shallower slope and a short half-life leads to a steeper slope\nsome drugs exhibit two phases a distribution phase and an elimination phase each of these will have a half-life associated with it The shorter the half-life of the distribution phase the steeper the initial decline will be although we usually concentrate on the half-life of the elimination phase the effective half-life of the drug may very well depend on the half-lives of both of these processes\nIt takes five to seven half lives to eliminate the drug.\n\n\n\n2.4.4 Area Under the Curve (AUC)\nHow to calculate AUC?\n\nassume that the concentration follows a straight line between points\none triangle and several trapizoid\nAUC is calculated from concentration-time data\nTrapezoids are used to estimate AUC between two data points\nAUC is the sum of the areas of all the trapezoids plus one triangle\n\n\n\n2.4.5 Extrapolation to Infinity\n\nafter the Tlast there are still large quantity of drug in the plasma\nHow can we extrapolate to infinity?\nWe need a way to calculate the AUC Tlast - infinity.\nSlope of the elimination is the key, apparent terminal phase, magnitide of the slope is \\(\\lambda_Z\\)\n\n\\[\nAUC _{tlast - \\infty} = \\frac{C_{last}}{\\lambda_z}\n\\]\n\\[\nAUC _{0 - \\infty} = AUC_{last} + \\frac{C_{last}}{\\lambda_z}\n\\]\n\nextrapolatd area should be below 20%\n\nImportant NCA parameters:\n\nIndependent of least-squares fit: such as Cmax, Tmax, AUClast,\nDependent on the least-squares fit: \\(\\lambda_Z\\), AUC~ 0-inf~, %Extrapolation, terminal half-life, volume, clearance\n\n\n\n2.4.6 Volume of Distribution\n\\[\nC = \\frac{Dose}{V}\n\\]\n\nvolume of distribution relates to the dose and concentration\nDoes not corresponds to anything physiological\nExample, 100 ug dose to IV bolus and 2 ug/L concentration, volume is 50L.\ntypical human plasma volume is 5 L, why V is sometimes very large?\nDrugs that are strongly bound to protein has very high V\n\n\n\n2.4.7 Clearance\n\nClearance Quantifies how quickly drug is removed from the body\n\n\\[\nRate ofelimination = Cl * C(t)\n\\]\n\nIn most cases Cl is constant. If changes with concentration, suspect non linear kinetics (saturation). for this reason, different dose level is adminstered.\nClearance includes both Metabolism and Excretion\n\n\n\n\nit is difficult to obtain all the ratios, so the overall ratio is called Bioavailability.\n\nBioavailability\n\\[\nF = \\frac{AUC_{oral}/Dose_{oral}}{AUC_{IV}/Dose_{IV}}\n\\]\n\nIntravenous: NCA parameters are V and Cl (F = 1)\nExtravascular: NCA parameters are V/F and Cl/F (F<1)\nElimination = Metabolism (liver) + Excretion (Kidney)\nCltotal = Clhepatic + Clrenal + Clother\nClrenal = Ae (amount of drug excreted in the urine)/ AUCplasma\nCalculation of Clearance from NCA:\n\n\n\nYou get the following from NCA\n\n\n\n\n2.4.8 Linear vs Log"
  },
  {
    "objectID": "WinNonlinIntro.html#run-nca-on-plasma-data",
    "href": "WinNonlinIntro.html#run-nca-on-plasma-data",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.5 Run NCA on Plasma Data",
    "text": "2.5 Run NCA on Plasma Data\n\n2.5.1 Run NCA using best fit\n\nDrug: Gravitix, 10 subjects, a single ascending dose (SAD) study, 6 dose level (1, 2, 4, 8, 16, 32 mg), PO adminsitration\nFrom the plot each subject grouped by dose level, we see that as the dose increases so do the concentrations in plasma\nwe expect that the drug exposure should increase proportionally to the dose\nafter we run NCA we will assess dose proportionality by examining PK parameters returned by NCA\nTwo worksheets: Observations and Dosing. Data on Observatino worksheet: Conc, Time, Subject, Doselevel, Administration, Amount.\n\nPerforming NCA:\n\nRight click on observation worksheet\nselect sent to, non-compartmental analysis, NCA\nSelect plasma, which is default setting\nspecify the dose type the default is extravascular\nRequired mappings: Time to time, the conc to concentration, subject to sort, dose level to sort\nselect dosing. two options: a worksheet with the dozing information or an internal worksheet\nselect source\nclick okay\nMap: Time to time, amount to dose, subject and dose level to sort\nspecify the calculation method linear up logdown\nclick execute\ndouble click the final parameters pivoted worksheet to open in its own window\n\nViewing plots:\n\nselect observed y and predicted y versus x\nfor subject one dose level 1: 5 points were used in the \\(\\lambda_Z\\) calculation, from 8 hours to the last observation at 36 hours\nThe best fit method automatically determines the optimal least squares regression using at least three points\nr-squared is the correlation coefficient of the regression\nr squared adjusted is based on the r squared adjusted for the number of points in the regression\nThe number points with the best value of r squared adjusted is used.\nthe half-life based on the value of \\(\\lambda_Z\\) is also reported in this case the half life is about 22 hours\nPage 2: this plot is also for subject one but now the dose level is 2 mg\nin this case the best fit method used three points in the calculation\neven though it is the same subject but the half-life based on lambda z is much shorter than for the first dose at only about 15 hours\nPage 3: this plot is for subject one does level 4\nagain 3 points were used in the calculation\nthe half-life is about 28 hours\nPage 4: this is for subject one those level 8\nthis time five points were used in the half-life is 15 hours\nPage 5: now we are up to those level 16 for and half-life is 21 hours\nPage 6: this plot is now at the highest to those level 32 mg\nthe half life is lower this time It’s 18\nwe have seen it’s quite a variability between different dose levels but is there a systematic trend?\n\nlet’s create a box plot to see if there is appears to be a trend\n\nright click on final parameters pivoted\nsend to plotting: box plot\nmap HL_lambda_z to y\nnow we want to look for a trend across the dose group map to dose level\nclick execute button\nThe plot shows us the distribution of half-life across the different dose groups\nfrom the plots we can see that although there is a good deal of variation in the dose levels, there does not appear to be a systemic trend and all the boxes overlopped with each other\n\nBox plot of AUCs\n\nlet’s make the duplicate of this plot:\nright click box plot\nselect copy\nright click on the workflow\nselect paste\nselect the duplicated plot let’s change the mapping for the y map AUCINF_D_obs\nclick execute\nbecause the data are dose normalized AUC values extrapolated to infinity we would expect these values to be the same for all dose groups and they do appear to be very consistent\nmost values falling between 0.04 and 0.05\nthis does suggest that the overall drug exposure is proportional to the dose\nalthough this is not a statistical test for the dose proportionality, it does give us a quick visual impression\n\nBox plot of AUClast_D\n\nLet’s make duplicate of the second plot right click the plot select copy right click on workflow paste\nmap AUClast_D to y execute\n\n\n\n2.5.2 Customize rules and parameters\n\nGoal is to add \\(\\lambda_Z\\) acceptance rules to our NCA\nselect the rules tab\nenter 0.9 for r-squared adjusted\nenter 20 for percentage of extrapolation\nenter 2 for span, the span is the number of half lives spent by the regression.\nclick execute\nReview the flag column for r-squared adjusted, % extrapolation and span\nnote that the results are not removed from the output\nbut we can use data tools to do the filtering if we want to\n\nTo filter the data:\n\nRight click in the final parameters pivoted worksheet\nSelect sent to\nselect data management\nselect split worksheet\nmap the three flag columns to sort\nClick execute\n\nThe “unique values” worksheet shows how many rows there are for each combination of sort values - notice that there are 24 rows that passed all criteria\n\nlet’s look at the worksheet fully accepted here we see the walls that has all three criteria is\n\nUser defined parameters\n\nselect “user defined parameters” tab\nto compute the concentration at a time of 48 hours\ninter 48 in the box The other thing that we can do is user defined parameters\nthe NCA does include many parameters we can also define our own\nclick add button\nadd a parameter, i.e., AUC_2\nFor definition you see last / 2\nif you would like this values include in the final parameters pivoted worksheet turn on include the final parameters\nexecute the NCA object\n\n\n\n2.5.3 Customize slope selections\n\nstart with NCA used in previous section\nDuplicate the NCA object\nselect slopes selector from the Setup trab\nEach plot is on a separate page, on each plot we can see the points that were used select for best fit.\nclick the left point to change starting point for the linear fit\nto change the end of the fitting, holding down the shift key and clicking on the indicated point\nDon’t change the endpoint of the fitting unless you have reason to reset the last point\nto exclude points from the linear fit hold on the control key and click on the indicated point\n\nFaster way of modifying slopes:\n\nselect the slopes under the Setup tab\nthis worksheet has the same information we saw in the individual plots but all on one table\nto control the best fit method select rules tab\nwe have two options for customizing you can either limit the number of points used in the linear regression or you can specify your start time limit\nSince there is a distribution phase we might decide to make sure that we do not include from the 12 hours in linear regression\ntype of 12 in the option start not before\nlet’s look at the slopes and then see output settings\nselect slope settings\nNow, the start time is at least 12 hours\nComparing box plots from two NCA results ( best fit and coustom fit) it is seen that some of the outliers are removed.\nIt is good idea to examine each slopes and adjust the solpes to compare the data.\n\n\n\n2.5.4 Compute partial areas\nIn the previous section we saw how comparing AUClast was problematic.When different subjects had different Tlast, Computing partial areas is a way that can overcome that limitation.\n\n\nSelect the setup tab\nSelect Partial Areas\nCheck “use Internal Wroksheet”\nBeacause we will use the same partial areas for all subjects, uncheck subject checkbox and beacause we want to use the same settings for all dose groups, uncheck DoseLevel checkbox\nClick OK\nWe may want to compute more than one partial area, at the bottom of the option tab, we can specify how many we want.\nClick on the selector for number of maximum partial areas. Select 2\nOnce again, we are asked to specify the sorts, Uncheck subject and dose level checkbox.\nWe need to fill in the start and end times\nFor the first partial area, we will compute the partial area for the first 12 hours\nSpecify 0 for the start time and 12 for the end time.\nFor our second partial area, use 0 and 24 hours.\nIf desired, we can label for each partial area. However, label is not required and it will automatically generate for us. Leave the label field blank.\nClick Execute.\n\nThe partial areas are added as columns to the right of the final parameters pivoted.\nLet’s repeat the process for other NCA object in our project: Select NCA best fit Click Setup Click Partial Areas Check “Use Internal Worksheet”\nTurn off Subject and doselevel Specify the maximum number of partial areas\nThe plots are all read, because the plots are all out of date -WE can update by Selecting the workflow in the object browser, the Workflow is displayed, click the Execute butte and it will update. While the Workflow is selected, on the result tab, we have the output of the workflow. Let’s go back to the workflow diagram, select Diagram, notice how the red color gone\n\n\n2.5.5 Use Theraputic response\nTo choose a dose for a given drug we have consider both the efficacy and side effects. We can set upper and lower concentration limits in NCA and quantify the time and AUC between the limits as well as above and below.\n\nThe following plot is for gravitex SAD study: Let’s say we want concentration to be between 10 and 50 ug/L. At the lowest dose, all the concentrations are below the lower limit. For higher dose (32 mg), concentrations are largley between the upper and lower limit\n\nSelect “Therapeutic Response” option on the Setup tab of the NCA object\nuncheck both subject and dose boxes\nEnter 10 for lower limit and 50 for higher limit\nExecute\n\n\n\n2.5.6 Customize units and parameters names\nTo obtain units in the NCA results, we need to have units defined on the concentrations, the times and the doses\n\nconcentration units are microgram per liter, nanogram per liter and micromols per liter\ntimes units are typically either hours or minutes and in some cases days\nunits for doses are milligrams micrograms and micromols\nit is not recommended to mix mass units\nif you get no units on your results you may have missed the unit on the concentration, time or dose, you will see the message insufficient unit in the units output. If this happens to you check that you have proper units on the concentration time and dose\n\nDefine the units used in the NCA:\n\nselect the setup tab\nselect the units input\nThe default units depend on input dataset units because the time units in the input data set is hours all the time units in the NCA results will also be based on hours\nThe volume unit in our input data is liters and therefore all the results have liters in the default units\nif we decide we want to change the units on our results we can do so by changing the values in the columns labeled preferred\n\nlet’s do that\n\n\nChange the preferred unit for volume to milliliter\nspecify milliliters per minute for the clearance unit\nMake sure to match the case as shown\nPhoenix will attempt to convert but if Phoenix doesn’t know how to convert to your preferred unit it will revert to the default unit execute the NCA\nBy checking execute here you can see the appropriate units in the results.\n\nChanging Parameters Name\n\nClick setup tab select the parameter names input\nTurn on the Use internal worksheet checkbox\nLet’s say that we want to change the name of half life\nAnd executed the NCA and now the half-life column has been renamed in the result"
  },
  {
    "objectID": "WinNonlinIntro.html#run-urine-nca",
    "href": "WinNonlinIntro.html#run-urine-nca",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.6 Run Urine NCA",
    "text": "2.6 Run Urine NCA\n\n2.6.1 About Urine NCA\n\n\nUrine samples are collected over an interval. Four samples were collected and concentration and volume was measured, the data is compiled in the table.\nThe rate of drug excretion is calculated by:\n\n\\[\nRate = \\frac{A_{ur}}{t_{end}-t_{start}}\n\\]\n\nPlot the Rate of drug excretion vs midpoint of collection interval\nRate of excretion starts high and decreases over time.\n\n\n\nMost import parameters are Amount Recovered and Percent Recovered and Percent of extrapolation (AURC_%Extrap_red)\nUrine NCA also include lambdaZ and halflife, however, since the urine data only contains four data, it is better to use plasma halflife\nPercent of extrapolation should be as small as possible.\n\n\n\n2.6.2 Setup project\n\n10 subjects, dose level, 4 mg, conc in both plasma and urine,\n\n\n\n2.6.3 Exploratory Data Analysis\n\nXY plot of plasma conc vs time is created\nXY plot of urine conc vs time is created. Time is end of collection interval\n\n\n\n2.6.4 NCA of plasma data\n\nmap x to Time, y to conc, sort to subject\nselect the Dosing imput, map Time to Time, Dose to dose, sort to subject\nSelect linear up log down\nExcecute\n\n\n\n2.6.5 NCA of urine data\n\nspecify the model type: Urine\nRequired mapping: start time, end time, concentration, volume\nsort to subject\nselect the dosing file, sort subject\nExecute\nAutomatic data calculations was done. Three different rates are given: Max_rate, Rate_last, Rate_last_pred, Tmax_rate, AURC: Area Under the Rate Curve\nUrine NCA does not extrapolate for amount recovered and percent recovered. That’s why it is best to collect urine samples until no more drugs are detected in the urine sample\n\n\n\n2.6.6 Calculate Renal Clearance\n\n\nright click Final Parameter Pivited, send to “computation tools”, “ratio and differences”.\nmap sort to subject\nSelect the worksheet2 input, click to select source button, select “Final Parameter Pivited” from NCA plasma, sort to subject\nOn the options tab, update the X column to “Amount Recovered”, Y column: AUClast, new column name: Clr, unit: L/h,\nExecute\n\nCreate a box plot\n\nRight click Ratios Differences Stacked, send to box plot, map y to Clr,"
  },
  {
    "objectID": "WinNonlinIntro.html#sparse-and-steady-state-nca",
    "href": "WinNonlinIntro.html#sparse-and-steady-state-nca",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.7 Sparse and Steady-State NCA",
    "text": "2.7 Sparse and Steady-State NCA\n\npre-clinical study of Gravitex in rats, 20 rats, dose: 200 ug/kg,time of blood draws: group A(0.5, 1, 2, 4, 8 h), group B( 0.75, 2, 6, 12 h)\nNot possible to do full NCA with only four data points\nWe need to pool the data to do NCA\nData: Schedule ( group A or group B), Animal, Time, Conc\n\n\n2.7.1 EDA of Sparse Data\nPlot: Concentration vs time by schedule\n\nPlotting the data: XY plot, map x to Time, y to conc, group to schedule, group to animal\nOptions pan, select the graph Conc vs Time, select Quick Style tab, check “Each group to color”, select “Schedule”\nNow the lines are yellow for group A and purple for group B.\nTurn off “group by marker”, now all the markers are same\nTurn off “group by lines”,\nSelect the Legend and turn off the “visible” checkbox\n\nTable:\n\nright click observation worksheet, send to reporting, select table\nmap conc to Data, animal to Raw ID, Raw Stratification to Schedule, Column Stratification to Time.\nSelect “Precision/Alignment” from option, precision method: significantDigits, value: 3,\nFor Time, specify the precision method: DecimalPlaces, value: 2, execute\nSelect Animal column, specify precision method: DcimalPlaces, value: 0\nSelect statistic tab, turn of “mean” and “SD”\n\n\n\n2.7.2 NCA of Sparse Data\nKey Points on NCA of spares data:\n\nSpares Option often used in pre-clinical data, where there is insufficient data to compute a full NCA for each subject\nOnce set of PK parameters is returned for the pooled results\nPK parameters are identical to what you would get by running NCA on mean concentration data\nStandard errors on Cmax and AUClast . These are calculated using published methods\nStandard error calculation is only avaialbel with linear trapezoids\n\nPerform NCA\n\nSend the data to NCA, turn on “Sparse” button\nRequired mapping: Time to time, Concentration to Conc and Subject to Animal\nSpecify dosing file, time to Time and dose to dose_norm, execute\nThere is only one raw in the result sheet\nLinear Trapizoid method was used and SE was obtained for some of the PK parameters.\nSince the data is pooled, only one plot is obtained.\n\n\n\n2.7.3 EDA of Multiple-Dose Data\nStudy design:\n\nExample: Gravitex multiple dose study\n20 human subjects randomly divided into two groups (dose level 2 and 4), dosed at 0h, conc. determined at 48 h,2nd dose at 48h, next doses at every 24h, until 192 hours. The first dose is a Naive dose\nRichly sampled for 0 to 48 hours and the last dose from 192h to 240h. Only Trough conc were measured for other doses\n\n\nBlue lines are generated from PK model.\nPlots:\n\nMap x to Time, y to Conc, group to subject, lattice page to dose level, lattice column to profile\n\n\nAfter modifying (log) x and y axis:\n\nTable:\n\nSend to reporting, Table\nmap Data to Conc, subject to RowID, dose level to stratification row, stratification column to time, profile to stratification column.\nSelect Column/sort Order, select Row Stratification, select Column Stratification, move profile to the top,\n\n\n\n\n2.7.4 Split Data\n\nBefore running NCA of multiple dose data, we need to split data\nRight click the worksheet, send to Data Management, Split Worksheet\nmap profile to sort, Exicute.\nSplit the dose worksheet the same way as the observation worksheet\n\n\n\n2.7.5 NCA for First Dose\n\nSelect Split Observation, right click on Output data A, send to NCA\nmap subject to sort, dose level to sort, conc to concentration, time to Time\nlink dosing input for A profile\nmap Dose to Amount, Tau to Tau, sort to dose level and subject\nCalculation method LinearupLogdown\nSelect Dose level column and click freez pane icon\n\n\n\n2.7.6 NCA for Final Dose\n\nperform same operation as for First Dose by selecting profile C.\nTmin is the time where minimum concentration was found\nCtau is the concentration at the end of the dosing interval\nCavg is the average concentration during the dosing interval\nDifference between AUClast and AUCtau\nSlope correction of the linear fit is of concern only if we are extrapolating the results. Otherwise AUCtau will not be affected by the slope correction.\n\n\n\n2.7.7 Determine Accumulation Ratio\n\nNCA can only determine PK parameters of the first dose or the last dose,\nAccumulation ration (AR) is calculated with the equation below:\n\\[\nAR = \\frac{AUC_{\\tau,SS}} {AUC_{0-\\tau}}\n\\]\nwhere, \\(AUC_{\\tau,SS}\\) is AUC_TAU from steady-state NCA, last dose and \\(AUC_{0-\\tau}\\) is AUC0_24 partial area from first dose NCA\npartial area"
  },
  {
    "objectID": "WinNonlinIntro.html#use-data-tools",
    "href": "WinNonlinIntro.html#use-data-tools",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.8 Use Data Tools",
    "text": "2.8 Use Data Tools\n\n2.8.1 Append Worksheets\n\nCombine data from two worksheets that share the same general structure of columns\n\n\n\nexample, combine PK1 and PK2. The new column “source” tells which worksheet each set of rows came from.\nColumns do not have to be identical nor the same order\nRight click the worksheet, select Data Management, Append Worksheets\nmap source column to all the columns of worksheet 1\nClick worksheet2, link the source file, map all the columns to Source Column\nDouble click Results tab to view the results\nTo append more that one worksheet, set the number on the option section\n\n\n\n2.8.2 Cross-product Worksheets\n\nFor making combination of multiple columns,\n\n\n\nRight click the worksheet, send to Crossproduct Worksheet\nmap subject to sort\nLink the second worksheet and map the same way as before, Execute\n\n\n\n2.8.3 Join Worksheets\n\nTo combine data so that rows are matched by a common sort key. Merge option is identical to Join option.\n\n\n\n\n\nboth column has be to same name to join the columns.\nClick ’sort map”, turn on Internal Worksheet, cut and paste to the same row.\n\n\n\n\n2.8.4 Pivot Worksheet\n\nRearranging data to allow comparison, for example to compare the effect of treatment, we need to see the data:\n\n\n\n\n\n\n2.8.5 Stack Columns\n\nInverse of Pivoting . Stackers stacks two or more column into a single column\n\n\n\nChange the column names from the options section\n\n\n\n\n2.8.6 Split Worksheet\n\n\n2.8.7 Enumerate Worksheet\n\nTo convert text values to numbers,\n\n\n\n\n\n2.8.8 Make BQL Substitutions\n\n\nAny non-numeric data is ignored by the NCA object\n\nTo estimate Tlag we must replace BQL by zero"
  },
  {
    "objectID": "WinNonlinIntro.html#compute-ratios-and-differences",
    "href": "WinNonlinIntro.html#compute-ratios-and-differences",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.9 Compute Ratios and Differences",
    "text": "2.9 Compute Ratios and Differences\n\n2.9.1 Compute Ratios from Single Input\n\n10 subjects with 2 mg IV dose, after wash out period same subject was administered 4 mg PO, To compute Bio-availability, we need to do ratio from a single worksheet.\n\n\n\n\n\n2.9.2 Compute Ratios from Dual Inputs\n\nNCA for urine and plasma must be done in separate NCA object, we have to combine results from two different worksheets.\nRenal Clearance example, view clearance section above.\n\n\n\n\n2.9.3 Compute Ratios using Means\n\nDifference between cross-over and parallel study\n\n\n\n\n\n\n2.9.4 Compute Differences"
  },
  {
    "objectID": "WinNonlinIntro.html#use-data-wizard",
    "href": "WinNonlinIntro.html#use-data-wizard",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.10 Use Data Wizard",
    "text": "2.10 Use Data Wizard\n\n2.10.1 Create a Filter\n\nFilter by time values\nRight click, send to Data management, Data Wizard\n### Set Column Properties\n\n\n\nWe can: 1. Exclude whole column; 2. Exclude or include by values, 3. Filter individual cells or whole rows.\n\n\n\n2.10.2 Set column properties\n\nThis can be used to sort columns, rename columns, specify or convert units\n\n\n\n2.10.3 Transform Data (Arithmatic)\n\nused to do simple arethmatic; x and y are variables, n is constant, units inherited from source.\nsame as data normalization.\nSet baseline in Time based data, Requires Time and Data Columns to map, Substrat the initial values\n\n\n\n2.10.4 Transform Data (Custom)\n\nCeiling, random\n\n\n\n\n2.10.5 Transform Data (Functinos)\n\nthis works with only on single column such as X to Ln(X).\n\n\n\n2.10.6 Multi-step operation"
  },
  {
    "objectID": "WinNonlinIntro.html#applications-of-nca",
    "href": "WinNonlinIntro.html#applications-of-nca",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.11 Applications of NCA",
    "text": "2.11 Applications of NCA\n\n2.11.1 Using Cmax and AUC to Make Decisions\nIn regulatory guidance:\nRate of absorption : Cmax\nExtent of absorption: AUC\nCmax and AUC are used in the following type of studies:\n\nApplication of NCA\n\n\n\n\n\n\nStudies\nwhat compares\n\n\n\n\nFormulations\nCompare two formulations of a drug\n\n\nFood effects\nCompare effect of food on drug absorption\n\n\nDrug-drug interaction\nCompare effect of one drug on another\n\n\nDemographics\nCompare effect of demographics (age, weight, gender, etc) on a drug\n\n\nSpecial Patient Populations\nCompare effect of drug in two different groups\n\n\nHepatic Impairment\nCompare effect of drug in two different groups\n\n\nBio availability\nCompare drug exposure with and without absorption\n\n\nDose Propotionality\nTest of linear kinetics\n\n\n\nAUC Rations (Also for Cmax )\nBioequivalence \\(\\frac{AUC_{test\\, form}}{AUC_{ref\\, form}}\\)\nFood Effect \\(\\frac{AUC_{fed}}{AUC_{fasted}}\\)\nDrug-drug Interaction: \\(\\frac{AUC_{Drug\\, + Inhibitor}}{AUC_{Drug\\, alone}}\\)\nHepatic Impairment: \\(\\frac{AUC_{with\\, impairment}}{AUC_{normal\\, hepatic\\, function}}\\)\n\n\n2.11.2 \n\n\n2.11.3 Study Designs\nStudy design considerations:\n\nPopulation\nDuration\nSchedule\nProcedures\n\nCrossover: Each subject receives each treatment\nParallel: Each subject receives one treatment\nsingle dose\n\n\n2.11.4 Dose Proportionality\n\\[AUC = \\frac{F*Dose}{CL}\\]\n\nIf Dose and CL are constant, \\(AUC \\propto F\\)\nIf F and CL are constant, \\(AUC \\propto Dose\\)\nIf F and Dose are constant, \\(AUC \\propto \\frac{1}{CL}\\)\n\n\n\n2.11.5 Average Bioequivalence\n\n\n2.11.6 Food Effects\n\nThis study helps labeling. Important parameters for Food Effect study:\n\nTotal exposure (\\(AUC_{last}, AUC_{\\infty}\\))\nPeak exposure (Cmax)\nPartial exposure (for MR formulations) (pAUC)\nTime to peak exposure (Tmax)\nLag-time (Tlag)\nTerminal elimination half-life (t1/2)\nApparent clearance (CL/F)\nApparent volume of distribution (V/F)\n\nIf 90% confidence interval for the fed/fasted ratio is wholly contained withing 80-125% then there is no significant food effect.\n\n\n\n\n\n\n2.11.7 Drug-Drug Interactions"
  },
  {
    "objectID": "WinNonlinIntro.html#create-tables",
    "href": "WinNonlinIntro.html#create-tables",
    "title": "2  Introduction to WinNonlin (122-D)",
    "section": "2.12 Create Tables",
    "text": "2.12 Create Tables\n\n2.12.1 Create Simple Table\n\n\n2.12.2 Use Stratification\n\n\n2.12.3 Add Statistics to Table\n\n\n2.12.4 Change Column/Sort Order\n\n\n2.12.5 Set Precision/Alignment\n\n\n2.12.6 Change Table Text"
  },
  {
    "objectID": "WinNonlinModel.html",
    "href": "WinNonlinModel.html",
    "title": "3  Introduction to PK Modeling",
    "section": "",
    "text": "4 Summary"
  },
  {
    "objectID": "WinNonlinModel.html#pk-model-1",
    "href": "WinNonlinModel.html#pk-model-1",
    "title": "3  Introduction to PK Modeling",
    "section": "4.1 pk model 1",
    "text": "4.1 pk model 1\n\n4.1.1 Concentrations should be stacked\ndata format for PK models - observed concentration stacked into a single column - multiple analytes or metabolites concentration should be its own column\n \nSort variables\n\nFor multiple PK profiles, one or more sort variables are required.\nSort variables need to have a value on every row\nIn Phoenix text values, blank cells are not a problem\nAll non-numeric data is ignored by the model\n\n\n\namount oral column has a value at time zero and the rest of the rows are blank. The dose amount is given only on the row the corresponds to a dosing event\ndose level column has a value on every row\nunits: unlike NCA the model object does not translate units\nThe dose unit should always be the same as the unit in the concentration\n\nAnd your parameter values keep them the same so that the units will work out properly One more warning the model object allows you to either map the dosing amounts in the main input or the dosing input Make sure that you don’t map the dose amount in both places or your administered dose will be twice as large as you intended click next to continue let’s recap the section we saw how the model object requires stacked concentrations you should have a single concentration column for each observation second we use sort variables to define the individual PK profiles there’s nothing wrong with having more sort variables than you need third we saw how dosing events can be included in the data set remember that these are entered on the row at the time of the dosing event and finally we learned that text values and empty cells are okay in the input and we do not have to do anything to them this completes the section click"
  },
  {
    "objectID": "TechNotes.html#courses",
    "href": "TechNotes.html#courses",
    "title": "4  My Notes",
    "section": "4.1 Courses",
    "text": "4.1 Courses\n\n(103-OD) Fundamentals of Pharmacokinetics [Conceptual Course]\n(105-OD) Noncompartmental Data Analysis [Conceptual Course]\n(122-OD) Introduction to Phoenix WinNonlin (Part 1) - NCA\n(123-OD) Introduction to Phoenix WinNonlin (Part 2) - Modeling\n\n\n4.1.1 create a new repository on the command line\necho \"# test\" >> README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/drnazmul/test.git\ngit push -u origin main\n\n\n4.1.2 …or push an existing repository from the command line\ngit remote add origin https://github.com/drnazmul/test.git\ngit branch -M main\ngit push -u origin main\nAfter any change made on github repo but not on the local folder, do the following\ngit pull --rebase origin main\ngit push origin main\n\n\n4.1.3 Publishing Quarto site or book\nPublish from Git Provider\nhttps://quarto.org/docs/publishing/netlify.html#publish-from-git-provider\n\n\n4.1.4 Custom domain on Netlify with Namecheap\n\nGo to Namecheap’s account, click Manage on the active domain\non “Name Server” select “Custom DNS” option\nGo to “Domain Settings” of Netlify\nCustom Domain : Options: Edit site name\nClick on “Add custom domain” and add your domain\nUnder “primary domain” options, click “Setup Netlify DNS”\nClick verify tab\nClick Add domain tab\nClick Continue\ncopy all the four name servers and paste to the DNS server on Namecheap\nClick Done on Netlify"
  },
  {
    "objectID": "TechNotes.html#pbpk-notes",
    "href": "TechNotes.html#pbpk-notes",
    "title": "4  My Notes",
    "section": "4.2 PBPK notes",
    "text": "4.2 PBPK notes\nhttps://www.admescope.com/whats-new/blog/2016/pbpk-what-it-is-for\nhttps://www.fda.gov/regulatory-information/search-fda-guidance-documents\nhttps://www.boomer.org/\nhttps://www.pkpd168.com/\nhttps://bebac.at/\nData: Collection of observations or facts. Can be text, number, audio, video, pics,\n\nWe encounter data 1. purposefully 2. Unintentionally\nUnderstanding the data we encounter everyday is where the data literacy comes in.\nData Literacy: how a person interacts with data to make sense or the world around them\nData literate person:\n\nrecognize usefulness of the data\nInterrogate reliability: When, where and how the data was collected?\nDiscover meaning\nMake decision\nCommunicate findings\nDo everything ethically\n\n4.2.1 Recognize usefulness of the data\nwhen you encounter a data: ask: Is this data relevant and useful to me? If so, how?\n% increase = (final value - initial value)/initial value * 100\nQuestions to ask:\nwhat’s the story the author is trying to make\nWhat data are they presenting and how are the presenting it?\nWhat is the agenda behind the information being presented\nhow might this data help others to make decision\nwhat do you like and what would you do differently?\n\nData: which is most useful? Which is not relavant?\nMake a plan with the questions\nScaptacle eays and curious mind are essential for data literate person"
  },
  {
    "objectID": "TechNotes.html#courses-to-take",
    "href": "TechNotes.html#courses-to-take",
    "title": "4  My Notes",
    "section": "4.3 Courses to take",
    "text": "4.3 Courses to take\n\nhttps://www.coursera.org/specializations/precalculus-data-modelling\nNONMEM paper Keizer, Karlsson, and Hooker (2013)\nPM and ML paper Keutzer et al. (2022)"
  },
  {
    "objectID": "TechNotes.html#data-camp-courses",
    "href": "TechNotes.html#data-camp-courses",
    "title": "4  My Notes",
    "section": "4.4 Data Camp Courses",
    "text": "4.4 Data Camp Courses\n\n4.4.1 Introduction to the Tidyverse\nDavid Robinson\nhttps://www.datacamp.com/cheat-sheet/tidyverse-cheat-sheet-for-beginners\n\n\n4.4.2 Data Manipulation with dplyr\nJames Chapman\n\n\n4.4.3 Introduction to Statistics in R\nMaggie Matsui\n\n\n4.4.4 Hypothesis Testing in R\nRichie Cotto\n\n\n4.4.5 Sampling in R\nRichie Cotto\nhttps://rpubs.com/ArifYetik/887228\n\n\n\n\nKeizer, R J, M O Karlsson, and A Hooker. 2013. “Modeling and Simulation Workbench for NONMEM: Tutorial on Pirana, PsN, and Xpose.” CPT: Pharmacometrics & Systems Pharmacology 2 (6): e50. https://doi.org/10.1038/psp.2013.24.\n\n\nKeutzer, Lina, Huifang You, Ali Farnoud, Joakim Nyberg, Sebastian G. Wicha, Gareth Maher-Edwards, Georgios Vlasakakis, et al. 2022. “Machine Learning and Pharmacometrics for Prediction of Pharmacokinetic Data: Differences, Similarities and Challenges Illustrated with Rifampicin.” Pharmaceutics 14 (8): 1530. https://doi.org/10.3390/pharmaceutics14081530."
  },
  {
    "objectID": "Rnotes.html#helpful-syntax",
    "href": "Rnotes.html#helpful-syntax",
    "title": "5  R-notes",
    "section": "5.1 Helpful Syntax",
    "text": "5.1 Helpful Syntax\n\n#Returns the structure and information of a given object\n\nstr(iris)\n\n#Returns the class of a given object\n\nclass(iris)\n\n#Returns your current working directory\n\ngetwd() \n\n#Changes your current working directory to a desired file path\n\nsetwd(\"/c/Users/BABA/Documents\")\n\n\n5.1.1 Installing and loading\n\n#Install dplyr through tidyverse \n\ninstall.packages(\"tidyverse\")\n\n# Install it directly\n\ninstall.packages(\"dplyr\")\n\n# Load dplyr into R\n\nlibrary(\"dplyr\")\nlibrary(\"tidyverse\")\n\nLoad the packages:\n\nlibrary(datasets) # Load the datasets\nlibrary(gapminder) \nattach(iris) # Attach iris data to r search path\n\n\n\n5.1.2 The %>% operator\n\n# Without the %>% operator\n\nselect(df, a, b)\n\n# By using the %>% operator\n\ndf %>%\n  select(a, b)\n\n\n\n5.1.3 Preloaded datasets\n\ndata()\n\n#Load and print mtcars data as follow:\n\ndata(\"mtcars\")\nhead(mtcars, 6)\n\n#To learn more about the data set:\n\n?mtcars\nnrow(mtcars)\nncol(mtcars)\n\n#Other popular dataset:\n  \ndata(\"iris\")\ndata(\"ToothGrowth\")\ndata(\"PlantGrowth\")\ndata(\"USArrests\")\n\n\n\n5.1.4 Basic column operations with dplyr\n\n# Select one or more columns with select()\n\niris %>%\n  select(Petal.Length, Petal.Width)\n \n# Select all but one column (e.g., listing_id)\n\niris %>%\n  select(-Petal.Width)\n\n# Select all columns within a range\n\niris %>%\n  select(Species:Petal.Length)\n\n# Rename a column using rename()\n\niris %>%\n  rename(p_length = Petal.Length)"
  },
  {
    "objectID": "Rnotes.html#working-with-columns",
    "href": "Rnotes.html#working-with-columns",
    "title": "5  R-notes",
    "section": "5.2 Working with columns",
    "text": "5.2 Working with columns\n\n5.2.1 Mutate\n\n# Change Sepal.Length to be in millimeters\n\niris %>% \n  mutate(Sepal.Length = Sepal.Length*10)\n\n# Create a new column called s_length\n\niris %>% \n  mutate(SLMm = Sepal.Length*2)\n\niris %>%\n  filter(Species==\"Virginica\") %>%\n  mutate(SLMm=Sepal.Length*10) %>%\n  arrange(desc(SLMm))\n\n\n# Add the number of observations for a column (e.g., number of Specirs)\n\niris %>% \n  add_count(Species)"
  },
  {
    "objectID": "Rnotes.html#working-with-rows",
    "href": "Rnotes.html#working-with-rows",
    "title": "5  R-notes",
    "section": "5.3 Working with rows",
    "text": "5.3 Working with rows\n\n5.3.1 Filter\n\n# Filter rows on one condition (e.g., Select iris data of species \"virginica\")\n\niris %>%\n  filter(Species==\"virginica\")\n\n# Filter on two OR more conditions (Species OR Petal Length)\n\niris %>%\n  filter(Species==\"virginica\" | Sepal.Length > 6)\n\n# Filter on two AND more conditions (country AND number_of_rooms)\n\niris %>%\n  filter(Species==\"virginica\", Sepal.Length > 6)\n\n\n# Filter by checking if a value exists in another set of values\n\niris %>% \n  filter(Species %in% c(\"versicolor\", \"virginica\"))\n\n\n\n5.3.2 Arrange\n\n# Sort rows by values in a column in ascending order\n\niris %>%\narrange(Petal.Length)\n\n# Sort rows by values in a column in descending order\n\niris %>%\narrange(desc(Petal.Length))\n\n# Combine multiple dplyr verb\n\niris %>% \n  filter(Species == \"virginica\") %>% \n  arrange(desc(Sepal.Length))\n\n\n\n5.3.3 Others\n\n# Remove duplicate rows in all the dataset\n\niris %>% \n  distinct()\n\n# Find unique values in the country column\n\niris %>% \n  distinct(Species)\n\n# Select rows based on top-n values of a column (e.g., top 3 listings with the highest amount  of rooms)\n\niris %>% \n\ntop_n(3, Petal.Length)"
  },
  {
    "objectID": "Rnotes.html#aggregating-data-with-dplyr",
    "href": "Rnotes.html#aggregating-data-with-dplyr",
    "title": "5  R-notes",
    "section": "5.4 Aggregating data with dplyr",
    "text": "5.4 Aggregating data with dplyr\n\n5.4.1 Count\n\n# Count groups within a column (e.g., count number of species in Species)\n\niris %>% \n  count(Species)\n\n# Count groups within a column and return sorted\n\niris %>% \n  count(Species, sort=TRUE)\n\n\n\n5.4.2 Summerise\n\n# Return the total sum of values for a column (e.g., total petal length)\n\niris %>% \n  summarise(total_length=sum(Petal.Length))\n\n# Return the average of values for a column (e.g, average petal length)\n\niris %>% \n  summarise(avg_length=mean(Petal.Length))\n\n#group_by() allows you to summarize within groups instead of summarizing the entire dataset\n\n# Group by a variable and return counts of each group (e.g., number of listings by country)\n\niris %>% \n  group_by(Species) %>%\n  summarise(n=n())\n\n# Group by a variable and return the average value per group (e.g., average number of rooms in listings per city)\n\niris %>%\n  group_by(Species) %>%\n  summarise(avg_length=mean(Petal.Length))\n\n# Find median and max sepal length of each species\n\niris %>% \n  group_by(Species) %>% \n  summarise(medianSL = median(Sepal.Length),\n            maxSL = max(Sepal.Length))\n# Find median and max petal length of each species with sepal length > 6\n\niris %>% \n  filter(Sepal.Length >6) %>% \n  group_by(Species) %>% \n  summarise(medianPL = median(Petal.Length), \n            maxPL = max(Petal.Length))"
  },
  {
    "objectID": "Rnotes.html#ggplot2",
    "href": "Rnotes.html#ggplot2",
    "title": "5  R-notes",
    "section": "5.5 ggplot2",
    "text": "5.5 ggplot2\n\n5.5.1 Scatter Plots\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width))+\n  geom_point()\n\n# Add color with another variable\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width,\n                       color = Species))+\n  geom_point()\n\n# Add color and size with two other variables\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width,\n                       color = Species, \n                       size = Sepal.Length))+\n  geom_point()\n\n# Faceting\n\niris_small <- iris %>% \n  filter(Sepal.Length >5)\nggplot(iris_small, aes(x = Petal.Length, # Requires ggplot package\n                       y = Petal.Width))+\n  geom_point()+\n  facet_wrap(~Species)\n\n\n\n5.5.2 Line plots\n\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length))+\n  geom_line()+\n  expand_limits(y=0)\n\n\n\n5.5.3 Bar plots\n\nby_species <- iris %>% \n  filter(Sepal.Length >6) %>% \n  group_by(Species) %>% \n  summarize(medianPL = median(Petal.Length))\n\nggplot(by_species, aes(x = Species, y = medianPL))+\n  geom_col()\n\n\n\n5.5.4 Histograms\n\nggplot(iris_small, aes(x= Petal.Length))+\n  geom_histogram()\n\n\n\n5.5.5 Box plots\n\nggplot(iris_small, aes(x=Species, y=Sepal.Width))+\n  geom_boxplot()"
  },
  {
    "objectID": "Rintro.html#introduction-to-r",
    "href": "Rintro.html#introduction-to-r",
    "title": "6  R for Data Analyst",
    "section": "6.1 Introduction to R",
    "text": "6.1 Introduction to R\n\n6.1.1 Vectors\n\nnumeric_vector <- c(1, 10, 49)\ncharacter_vector <- c(\"a\", \"b\", \"c\")\nboolean_vector <- c(TRUE, FALSE, TRUE)\n\n\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\n\n#Assign the names of the day to 'roulette_vector' and 'poker_vector'\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\ntotal_daily <- poker_vector+roulette_vector\nprint(total_daily)\n\ntotal_poker <- sum(poker_vector)\ntotal_roulette <- sum(roulette_vector)\ntotal_week <- total_poker+total_roulette\nprint(total_week)\n\n\n# Check if you realized higher total gains in poker than in roulette \nanswer <- total_poker > total_roulette\nanswer\n\n#sub-setting vectors by index, by name, by comparison.\n\npoker_wednesday <- poker_vector[3]\npoker_midweek <- poker_vector[c(2, 3, 4)]\nroulette_selection_vector <- roulette_vector[2:5]\n\n# Selection by name\npoker_start <- poker_vector[c(\"Monday\", \"Tuesday\", \"Wednesday\")]\naverage_midweek_gain <- mean( poker_start)\nprint(average_midweek_gain)\n\n# sub-setting by comparison\nselection_vector <- poker_vector>0\nprint(selection_vector)\n\n\n# Select from poker_vector these days\npoker_winning_days <- poker_vector[selection_vector]\nprint(poker_winning_days)\n\n\n\n6.1.2 Matrices\n\nmatrix(1:9, byrow = TRUE, nrow = 3)\n\n# Box office Star Wars (in millions!)\nnew_hope <- c(460.998, 314.4)\nempire_strikes <- c(290.475, 247.900)\nreturn_jedi <- c(309.306, 165.8)\n\n# Create box_office\nbox_office <- c(new_hope, empire_strikes, return_jedi)\n\n# Construct star_wars_matrix\nstar_wars_matrix <- matrix(box_office, byrow = TRUE, nrow = 3)\n\nprint(star_wars_matrix)\n\n\n# Vectors region and titles, used for naming\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \"The Empire Strikes Back\", \"Return of the Jedi\")\n\n# Name the columns with region\ncolnames(star_wars_matrix) <- region\n\n# Name the rows with titles\nrownames(star_wars_matrix) <- titles\n\n# Print out star_wars_matrix\nprint(star_wars_matrix)\n\n\n# Construct star_wars_matrix\nbox_office <- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \n                 \"The Empire Strikes Back\", \n                 \"Return of the Jedi\")\n               \nstar_wars_matrix <- matrix(box_office, \n                      nrow = 3, byrow = TRUE,\n                      dimnames = list(titles, region))\n\n# Calculate worldwide box office figures\nworldwide_vector <- rowSums(star_wars_matrix)\n\n# Bind the new variable worldwide_vector as a column to star_wars_matrix\nall_wars_matrix <- cbind(star_wars_matrix, worldwide_vector)\n\n\nstar_wars_matrix  \nstar_wars_matrix2 \n\n# Combine both Star Wars trilogies in one matrix\nall_wars_matrix <- rbind(star_wars_matrix, star_wars_matrix2)\n\n# Total revenue for US and non-US\ntotal_revenue_vector <- colSums(all_wars_matrix)\n  \n# Print out total_revenue_vector\nprint(total_revenue_vector)\n\n\n# sub-setting matrix\n# use a comma to separate the rows you want to select from the columns\n# all_wars_matrix is available in your workspace\nall_wars_matrix\n\n# Select the non-US revenue for all movies\nnon_us_all <- all_wars_matrix[,2]\n  \n# Average non-US revenue\nmean(non_us_all)\n  \n# Select the non-US revenue for first two movies\nnon_us_some <- all_wars_matrix[1:2,2]\n  \n# Average non-US revenue for first two movies\nmean(non_us_some)\n\n# Estimate the visitors\nvisitors <- all_wars_matrix/5\n  \n# Print the estimate to the console\nvisitors\n\n\n\n6.1.3 Factors\n\n# Sex vector\nsex_vector <- c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\")\n\n# Convert sex_vector to a factor\nfactor_sex_vector <-factor(sex_vector)\n\n# Print out factor_sex_vector\nfactor_sex_vector\n\n# Animals - Nominal categorical variable\nanimals_vector <- c(\"Elephant\", \"Giraffe\", \"Donkey\", \"Horse\")\nfactor_animals_vector <- factor(animals_vector)\nfactor_animals_vector\n\n# Temperature - Ordinal categorical variable\ntemperature_vector <- c(\"High\", \"Low\", \"High\",\"Low\", \"Medium\")\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE, levels = c(\"Low\", \"Medium\", \"High\"))\nfactor_temperature_vector\n\n# FACTOR LEVELS\n\n# Code to build factor_survey_vector\nsurvey_vector <- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector <- factor(survey_vector)\n\n# Specify the levels of factor_survey_vector\nlevels(factor_survey_vector) <-c(\"Female\", \"Male\")\n\nfactor_survey_vector\n\n#Summarizing a factor\n\n# Generate summary for factor_survey_vector\nsummary(factor_survey_vector)\n\n# Note: R returns NA when you try to compare values in a factor,\n\n# Create speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\n\n# Convert speed_vector to ordered factor vector\nfactor_speed_vector <-factor(speed_vector, ordered = TRUE, levels = c(\"slow\", \"medium\", \"fast\"))\n\n# Print factor_speed_vector\nfactor_speed_vector\nsummary(factor_speed_vector)\n\n\n# Create factor_speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\nfactor_speed_vector <- factor(speed_vector, ordered = TRUE, levels = c(\"slow\", \"medium\", \"fast\"))\n\n# Factor value for second data analyst\nda2 <- factor_speed_vector[2]\n\n# Factor value for fifth data analyst\nda5 <- factor_speed_vector[5]\n\n# Is data analyst 2 faster than data analyst 5?\nda2>da5\n\n\n\n6.1.4 Data Frame\n\nhead(mtcars)\nstr(mtcars)\n\n# Definition of vectors\nname <- c(\"Mercury\", \"Venus\", \"Earth\", \n          \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype <- c(\"Terrestrial planet\", \n          \"Terrestrial planet\", \n          \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\", \"Gas giant\")\ndiameter <- c(0.382, 0.949, 1, 0.532, \n              11.209, 9.449, 4.007, 3.883)\nrotation <- c(58.64, -243.02, 1, 1.03, \n              0.41, 0.43, -0.72, 0.67)\nrings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n# Create a data frame from the vectors\nplanets_df <-data.frame(name, type, diameter, rotation, rings)\nplanets_df\n\n# Print out diameter of Mercury (row 1, column 3)\nplanets_df[1,3]\n\n# Print out data for Mars (entire fourth row)\nplanets_df[4,]\n\n# Select first 5 values of diameter column\nplanets_df[1:5, 3]\n\n# Select an entire column\n\nplanets_df[,3]\nplanets_df[,\"diameter\"]\nplanets_df$diameter\n\n\n# Select planets with diameter < 1\nsubset(planets_df, subset = diameter<1)\n\n# Use order() to create positions\npositions <- order(planets_df$diameter)\n\n# Use positions to sort planets_df\nplanets_df[positions, ]\npwd"
  },
  {
    "objectID": "Rintro.html#introduction-to-sampling",
    "href": "Rintro.html#introduction-to-sampling",
    "title": "6  R for Data Analyst",
    "section": "6.2 Introduction to sampling",
    "text": "6.2 Introduction to sampling\n\n6.2.1 Point estimates\n\n\n6.2.2 Random number generation\n\nView(dataset)\n# Generate random numbers from ...\nrandoms <- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform = runif(n_numbers, min = -3, max = 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\nNotice how the histograms almost take the flat and bell curve shapes of the uniform and normal distributions, but there is a bit of random noise.\nSetting the seed to a particular value means that subsequent random code that generates random numbers will have the same answer each time you run it.\n\n\n6.2.3 Bootstrapping\nThe bootstrapping workflow is to generate\n\na resample of the same size as the population,\ncalculate a summary statistic,\nthen repeat this to get a distribution of summary statistics.\n\nThe key to deciding whether to sample without or with replacement is whether or not your dataset is best thought of as being the whole population or not.\nTo make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.\nFrom the smaller sample of Spotify songs, we can estimate the mean danceability statistic in the population. Since we have a distribution of statistics, we can even quantify how accurate our estimate is.\nIf the sample is not closely representative of the population, then the mean of the bootstrap distribution will not be representative of the population mean. This is less of a problem for standard errors.\n\n# Generate a sampling distribution\nmean_sampling_dist <- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the population\n    spotify_population %>% \n      # Sample 500 rows without replacement\n      slice_sample(n = 500) %>% \n      # Calculate the mean popularity as mean_popularity\n      summarise(mean_popularity = mean(popularity)) %>% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# See the result\nmean_popularity_2000_samp\n\n\n# Generate a bootstrap distribution\nmean_bootstrap_dist <- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the sample\n    spotify_sample %>% \n      # Sample same number of rows with replacement\n      slice_sample(prop = 1, replace = TRUE) %>% \n      # Calculate the mean popularity\n      summarise(mean_popularity = mean(popularity)) %>% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# Store the resamples in a tibble\nbootstrap_distn <- tibble(\n  resample_mean = mean_danceability_1000\n)\n\n# Draw a histogram of the resample means with binwidth 0.002\nggplot(bootstrap_distn, aes(resample_mean))+\n  geom_histogram(binwidth=0.002)\n\nThe sampling distribution mean is the best estimate of the true population mean; the bootstrap distribution mean is closest to the original sample mean.\nThe sampling distribution mean can be used to estimate the population mean, but that is not the case with the boostrap distribution.\n\n# Calculate std from sampling_distribution\n\nsamp_distn_sd <- sampling_distribution %>% \n  summarize(sd(sample_mean) * sqrt(500))\n\n# Calculate std from bootstrap_distribution\n\nboot_distn_sd <- bootstrap_distribution %>% \n  summarize(sd(resample_mean) * sqrt(500))\n\n# See the results\nc(sam_distn = samp_distn_sd, boot_distn = boot_distn_sd)\n\nWhen you don’t have all the values from the true population, you can use bootstrapping to get a good estimate of the population standard deviation\n\n\n6.2.4 Confidence interval\nWhen reporting results, it is common to provide a confidence interval alongside an estimate.\nWhat does that confidence interval provide?\n\nA range of plausible values for an unknown quantity.\n\nConfidence intervals account for uncertainty in our estimate of a population parameter by providing a range of possible values. We are confident that the true value lies somewhere in the interval specified by that range.\nThe standard error method for calculating the confidence interval assumes that the bootstrap distribution is normal. This assumption should hold if the sample size and number of replicates are sufficiently large.\n\n# Generate a 95% confidence interval using the quantile method\nconf_int_quantile <- bootstrap_distribution %>% \n  summarize(\n    lower = quantile(resample_mean, 0.025),\n    upper = quantile(resample_mean, 0.975)\n  )\n\n# See the result\nconf_int_quantile\n\n# Generate a 95% confidence interval using the std error method\nconf_int_std_error <- bootstrap_distribution %>% \n  summarize(\n    point_estimate = mean(resample_mean),\n    standard_error = sd(resample_mean),\n    lower = qnorm(0.025, point_estimate, standard_error),\n    upper = qnorm(0.975, point_estimate, standard_error)\n  )\n\n# See the result\nconf_int_std_error\n\n\nthe standard deviation of a bootstrap distribution statistic is a good approximation for the standard error of the sampling distribution.\nyou calculated confidence intervals for statistics using both the quantile method and the standard error method, and they gave very similar answers. That means that the normal distribution is a good approximation for the bootstrap distribution.\n\n\n\n6.2.5 t test\nHypothesis testing workflow for the one sample case where you compared a sample mean to a hypothesized value, and the two sample case where you compared two sample means. In both cases, the workflow follows this format.\n\n\n\n6.2.6 Two sample mean test statistic\nThe hypothesis test for determining if there is a difference between the means of two populations uses t-scores, and can be calculated from three values from each sample using this equation.\nWhy is t needed?\nThe process for calculating p-values is\n1. to start with the sample statistic,\n2. standardize it to get a test statistic,\n3. then transform it via a cumulative distribution function (CDF).\nIn Chapter 1, that final transformation was denoted z, and the CDF transformation used the (standard normal) z-distribution.\nIn the last video, the test statistic was denoted t, and the transformation used the t-distribution.\nUsing a sample standard deviation to estimate the standard error is computationally easier than using bootstrapping. However, to correct for the approximation, you need to use a t-distribution when transforming the test statistic to get the p-value.\n\n# Calculate the numerator of the test statistic\nnumerator <- xbar_no - xbar_yes\n\n# Calculate the denominator of the test statistic\ndenominator <- sqrt(s_no^2/n_no + s_yes^2/n_yes)\n\n# Calculate the test statistic\nt_stat <- numerator/denominator\n\n# See the result\nt_stat\n# Calculate the degrees of freedom\ndegrees_of_freedom <- n_no+n_yes-2\n\n# Calculate the p-value from the test stat\np_value <- pt(t_stat, df = degrees_of_freedom, lower.tail = TRUE)\n\n# See the result\np_value"
  },
  {
    "objectID": "Rintro.html#hypothesis-testing-in-r",
    "href": "Rintro.html#hypothesis-testing-in-r",
    "title": "6  R for Data Analyst",
    "section": "6.3 Hypothesis testing in R",
    "text": "6.3 Hypothesis testing in R\n\n6.3.1 Calculating a z-score\nSince variables have arbitrary ranges and units, we need to standardize them. For example, it would be silly if a hypothesis test gave a different answer if your variables were in Euros instead of US dollars. Standardization avoids that.\nOne standardized value of interest in a hypothesis test is called a z-score. To calculate it, we need three numbers:\n\nSample statistic (point estimate),\nHypothesized statistic,\nStandard error of the statistic (which we estimate from the bootstrap distribution).\n\n\n# View the late_shipments dataset\nView(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp <- late_shipments %>% \n  summarize(prop_late_shipments = mean(late == \"Yes\")) %>% \n  pull(prop_late_shipments)\n\n# See the results\nlate_prop_samp\n\n# Hypothesize that the proportion is 6%\nlate_prop_hyp <- 0.06\n\n# Calculate the standard error\nstd_error <- late_shipments_boot_distn %>% \n  summarize(sd_late_prop = sd(late_prop)) %>% \n  pull(sd_late_prop)\n\n# Find z-score of late_prop_samp\nz_score <- (late_prop_samp - late_prop_hyp) / std_error\n\n# See the results\nz_score\n\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic.\n\n\n6.3.2 p value\nA hypothesis is a statement about a population parameter. We don’t know the true value of this population parameter; we can only make inferences about it from the data. Hypothesis tests compare two competing hypotheses.\nRather than saying we accept the alternative hypothesis, the verdicts are rejecting the null hypothesis, or failing to reject the null hypothesis.\nThe hypothes is testing equivalent of “beyond a reasonable doubt” is known as the significance level.\nThe tails of the distribution that are relevant depend on whether the alternative hypothesis refers to “greater than”, “less than”, or “differences between”.\n\n# Calculate the p-value\np_value <- pnorm(z_score, lower.tail = FALSE)\n                 \n# See the result\np_value   \n\nThe p-value is calculated by transforming the z-score with the standard normal cumulative distribution function (CDF).\n\n\n6.3.3 Statistical significance\nWhat defines the cutoff point between a small p-value and a large one?\nSignificance level\nThe cutoff point is known as the significance level,\\(\\alpha\\). The appropriate significance level depends on the dataset and the discipline you are working in. Five percent is the most common choice, but ten percent and one percent are also popular. The significance level gives us a decision process for which hypothesis to support.\nIf the p-value is low, H0 must go (reject H0 )\nIf the p-value is high, H0 must fly (fail to reject H0)\nIt’s important that you decide what the appropriate significance level should be before you run your test. Otherwise there is a temptation to decide on a significance level that lets you choose the hypothesis you want.\n\n\n6.3.4 Confidence intervals\nTo get a sense as to potential values of the population parameter, it’s common to choose a confidence interval of one minus the significance level. For a significance level of 0.05 e, we’d use a 95% confidance interval. Here’s the calculation using the quantile method.\n\n# Calculate 95% confidence interval using quantile method\nconf_int_quantile <- late_shipments_boot_distn %>%\n  summarize(\n    lower = quantile(prop_late_shipments, 0.025),\n    upper = quantile(prop_late_shipments, 0.975)\n  )\n\n# See the result\nconf_int_quantile\n\nThe interval runs from 0.369 to 0.407 giving a range of plausible values for the proportion of data scientists starting programming as children.\nCalculating confidence intervals\nIf you give a single estimate of a sample statistic, you are bound to be wrong by some amount. For example, the hypothesized proportion of late shipments was 6%. Even if evidence suggests the null hypothesis that the proportion of late shipments is equal to this, for any new sample of shipments, the proportion is likely to be a little different. Consequently, it’s a good idea to state a confidence interval. That is, you say “we are 95% ‘confident’ the proportion of late shipments is between A and B” (for some value of A and B).\nSampling in R demonstrated two methods for calculating confidence intervals. Here, you’ll use quantiles of the bootstrap distribution to calculate the confidence interval.\nWhen you have a confidence interval width equal to one minus the significance level, if the hypothesized population parameter is within the confidence interval, you should fail to reject the null hypothesis.\n\n\n6.3.5 Proportion Tests\nThe t-test is needed for tests of mean(s) since you are estimating two unknown quantities, which leads to more variability.\n\n\n6.3.6 Non-parametric tests\nAssumptions in hypothesis testing\nEach hypothesis test makes assumptions about the data. It’s only when these assumptions are met that it is appropriate to use that hypothesis test.\nRandomness\nWhether it uses one or two samples, every hypothesis test assumes that each sample is randomly sourced from its population. If you don’t have a random sample, then it won’t be representative of the population. To check this assumption, you need to know where your data came from. There are no statistical or coding tests you can perform to check this. If in doubt, ask the people involved in collecting the data, or a domain expert that understands the population being sampled.\nIndependence of observations\nTests also assume that each observation is independent. There are some special cases like paired t-tests where dependencies between two samples are allowed, but these change the calculations so you need to understand where such dependencies occur. As you saw with the paired t-test, not accounting for dependencies results in an increased chance of false negative and false positive errors. This is also a difficult problem to diagnose after you have the data. It needs to be discussed before data collection.\nLarge sample size\nHypothesis tests also assume that your sample is big enough. Smaller samples incur greater uncertainty, and mean that the Central Limit Theorem doesn’t apply, which in turn means that the sampling distribution might not be normally distributed. The increased uncertainty means you get wider confidence intervals on the parameter you are trying to estimate. The Central Limit Theorem not applying means the calculations on the sample could be nonsense, which increases the chance of false negative and positive errors. The check for “big enough” depends on the test and that’s where we’ll head next.\nLarge sample size: t-test\nFor one sample t-tests, a popular heuristic is that you need at least thirty observations in your sample. For the two sample case or ANOVA, you need thirty observations in each. That means you can’t compensate for one small group sample by making the other one bigger. In the paired case, you need thirty pairs of observations.\nLarge sample size: proportion tests\nFor one sample proportion tests, the sample is considered big enough if it contains at least ten successes and ten failures. Notice that if the probability of success is close to zero or close to one, then you need a bigger sample. In the two sample case the size requirements apply to each sample separately.\nLarge sample size: chi-square tests\nThe chi-square test is slightly more forgiving and only requires five successes and failures in each group rather than ten.\nSanity check\nOne more check you can perform is to calculate a bootstrap distribution and visualize it with a histogram. If you don’t see a bell-shaped normal curve, then one of the assumptions hasn’t been met. In that case, you should revisit the data collection process, and see if any of the three assumptions of randomness, independence, and sample size do not hold."
  },
  {
    "objectID": "Rintro.html#foundations-of-inference",
    "href": "Rintro.html#foundations-of-inference",
    "title": "6  R for Data Analyst",
    "section": "6.4 Foundations of Inference",
    "text": "6.4 Foundations of Inference\n\n6.4.1 NHANES home owner’s data\n\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(infer)\n\nlibrary(NHANES)\n# What are the variables in the NHANES dataset?\n\ncolnames(NHANES)\n\nlet’s investigate the relationship between gender and home ownership calculate the original observed statistic\n\nhomes <- NHANES %>% \n  select(Gender, HomeOwn) %>% \n  filter(HomeOwn %in% c(\"Own\", \"Rent\"))\nhomes\n\n\ndiff_orig <- homes %>% \n  group_by(Gender) %>% \n  summarise(prop_org = mean(HomeOwn == \"Own\")) %>% \n  summarise(diff(prop_org)) %>% \n  pull()\n\nRandomized data under null model of independence\n\nhomeown_perm <- homes %>% \n  specify(HomeOwn ~ Gender, success = \"Own\") %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n#the distribution of randomized statistics\nggplot(homeown_perm, aes(x = stat))+\n  geom_dotplot(binwidth = 0.001)\n\ngeom density\nDo the data come from the population?\n\nggplot(homeown_perm, aes(x = stat))+\n  geom_density()+\n  geom_vline(aes(xintercept = diff_orig), color = \"red\")\n\nCompare permuted differences to observed difference\n\nhomeown_perm %>%\n  summarize(n_perm_le_obs = sum(stat <= diff_orig))\n\n223 permuted differences are more extreme than the observed difference. This only represents 21.2% of the null statistics, so you can conclude that the observed difference is consistent with the permuted distribution.\n\n\n6.4.2 Gender Discrimination\n\nlibrary(openintro)\n\ndisc <- sex_discrimination\n\ndisc %>% \n  count(sex, decision)\n\ndisc_orig <- disc %>% \n  group_by(sex) %>% \n  summarise(promoted_prop = mean(decision == \"promoted\")) %>% \n  summarise(diff(promoted_prop)) %>% \n  pull()\ndisc_orig\n\nAs the first step of any analysis, you should look at and summarize the data. Categorical variables are often summarized using proportions, and it is always important to understand the denominator of the proportion.\n\ndisc_perm <- disc %>% \n  specify(decision ~ sex, success = \"promoted\") %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\ndisc_perm\n\n\nggplot(disc_perm, aes(x = stat))+\n  geom_histogram(binwidth = 0.01)+\n  geom_vline(aes(xintercept = disc_orig), color = \"red\")\n\np-value measures the degree of disagreement between the data and the null hypothesis. here, you’re only interested in the one-sided hypothesis test here. That is, you’re trying to answer the question, “Are men more likely to be promoted than women?”\n\ndisc_perm %>% \n  visualise(obs_stat = disc_orig, direction = \"greater\")\n\ndisc_perm %>% \n  get_p_value(obs_stat = disc_orig, direction = \"greater\" )\n\nCalculating p values\n\ndisc_perm %>%\n  summarize(p_value = mean(disc_orig >= stat))\n\nhow the p-value is computed here. First we identify permuted differences that are larger than or equal to the observed statistic and label those situations with a 1, all other permutations receiving a zero. By averaging the 0s and 1s, the mean gives the proportion of times the permuted difference is larger than or equal to the observed difference. Because (point) 03 is less than (point) 05, we reject the null hypothesis and claim that men are promoted at a higher rate than women. That is, we conclude that it was not simply random variability which led to a higher proportion of men being promoted. A p-value of (point) 03 is reasonably close to (point) 05 which means we should be somewhat careful in making strong claims.\n\n\n6.4.3 Opportunity Cost\n\nopportunity_cost %>% \n  count(group, decision)\n\nopportunity_cost %>% \n  group_by(group) %>% \n  summarise(buy_prop = mean(decision == \"buy video\"))\n\nThe barplot better displays the results from the study. The treatment seems like it might have had an effect.\n\n# Plot group, filled by decision\nggplot(opportunity_cost, aes(x = group, fill = decision)) + \n  # Add a bar layer, with position \"fill\"\n  geom_bar(position = \"fill\")\n\n\n# Point estimate\n\n# Calculate the observed difference in purchase rate\ndiff_obs <- opportunity_cost %>%\n  # Group by group\n  group_by(group) %>%\n  # Calculate proportion deciding to buy a DVD\n  summarise(prop_buy = mean(decision == \"buy video\")) %>%\n  # Calculate difference between groups\n  summarise(stat = diff(prop_buy)) %>% \n  pull()\ndiff_obs\n\n\n# Create data frame of permuted differences in purchase rates\nopp_perm <- opportunity_cost %>%\n  # Specify decision vs. group, where success is buying a DVD\n  specify(decision ~ group, success = \"buy video\") %>%\n  # Set the null hypothesis to independence\n  hypothesise(null = \"independence\") %>%\n  # Generate 1000 reps of type permute\n  generate(reps = 1000, type = \"permute\") %>%\n  # Calculate the summary stat difference in proportions\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\"))\n    \n# Review the result\nopp_perm\n\n\n# Using the permuation data, plot stat\nggplot(opp_perm, aes(x = stat)) + \n  # Add a histogram layer with binwidth 0.005\n  geom_histogram(binwidth = 0.005) +\n  # Add a vline layer with intercept diff_obs\n  geom_vline(aes(xintercept = diff_obs), color = \"red\")\n\nyou’ll calculate the p-value to judge if the difference in proportions permuted is consistent with the observed difference.\nNow that you’ve created the randomization distribution, you’ll use it to assess whether the observed difference in proportions is consistent with the null difference. You will measure this consistency (or lack thereof) with a p-value, or the proportion of permuted differences less than or equal to the observed difference.\nvisualize and get_p_value using the built in infer functions. Remember that the null statistics are above the original difference, so the p-value (which represents how often a null value is more extreme) is calculated by counting the number of null values which are less than the original difference.\n\n# Visualize the statistic \nlibrary(infer)\nopp_perm %>%\n  visualize()+\n  shade_p_value(obs_stat =diff_obs, direction = \"left\" )\n\n# Calculate the p-value using `get_p_value`\nopp_perm %>%\n  get_p_value(obs_stat = diff_obs, direction = \"left\")\n\n# Calculate the p-value using `summarize`\nopp_perm %>%\n  summarize(p_value = mean(stat <= diff_obs))\n\nThe small p-value indicates that the observed data are inconsistent with the null hypothesis. We should reject the null claim and conclude that financial advice does affect the likelihood of purchase.\n\n\n6.4.4 Error in hypothesis testing\nthere are two possible decisions to make in hypothesis testing. 1. Either the observed data are inconsistent with the null hypothesis, in which case the null hypothesis is rejected.\n\nOr the observed data are consistent with the null hypothesis,in which case the null hypothesis is not rejected and no conclusion is made about a larger population.\n\nThere are also two possible “truth” states: either the null hypothesis is true or the alternative hypothesis is true.\n\nthe goal of the scientific study is to be in the bottom box where the alternative hypothesis is true and the data provide convincing evidence to reject the null hypothesis.\nHowever, any of the other three boxes are also possible. We cannot know which row has resulted, but we do know which conclusion has been made, thereby specifying the column.\nWhich is to say, if the null hypothesis is rejected, then either the science is correct or a type I error has been made. If the null hypothesis is not rejected, it has either been done so correctly or a type II error has been made. Recall that the decision being made controls the type I error rate, that is the false positive rate, at, for example, (point) 05, for both mathematical and historical reasons.\n\n\n6.4.5 Parameters and Confidence Interval\nUntil now, the research question at hand has been a question of comparison. What if, instead, the research question is one of estimation?\nFor example, “under which diet plan will participants lose more weight on average” is a comparative question and we use a hypothesis test. “How much should participants expect to lose on average” is an estimation question for which we use confidence intervals. Or\nanother example is the comparative question: “which of two car manufacturers are drivers more likely to recommend to their friends?” Hypothesis testing is used to analyze that question. But: “what percent of users are likely to recommend Subaru to their friends” is an estimation problem and we use confidence intervals to answer that question.\nOne more, the comparative question: “are education level and average income linearly related” is addressed with a hypothesis test. The estimation question: “for each additional year of education, what is the predicted average income” uses a confidence interval. OK, you see the pattern.\nParameter\nFor each of the estimation problems, we need to understand what a parameter is. A parameter is a numerical value from the population.\nSo in the first example, the parameter is the true average amount that all dieters will lose on a particular program. In the second example, the parameter is the proportion of individuals in the population who recommend Subaru cars. And the last parameter is the average income of all individuals in the population with a particular education level.\nConfidence interval\nA confidence interval is a range of numbers that hopefully captures the true parameter value of interest.\nFor example, at the end of the course, we’ll be able to make conclusions along the lines of “we are 95% confident that somewhere between 12% and 34% of the entire population recommends Subarus.” That is, the goal in creating a confidence interval is to calculate a range of plausible values for the parameter of interest.\n\n# Compute p-hat for each poll\nex1_props <- all_polls %>% \n  # Group by poll\n  group_by(poll) %>% \n  # Calculate proportion of yes votes\n  summarize(stat = mean(vote == \"yes\"))\n  \n# Review the result\nex1_props"
  },
  {
    "objectID": "Rintro.html#introduction-to-regression-with-r",
    "href": "Rintro.html#introduction-to-regression-with-r",
    "title": "6  R for Data Analyst",
    "section": "6.5 Introduction to Regression with R",
    "text": "6.5 Introduction to Regression with R\nRegression models are a class of statistical models that let you explore the relationship between a response variable and some explanatory variables.\nLinear regression is used when the response variable is numeric, like in the motor insurance dataset. Logistic regression is used when the response variable is logical. That is, it takes TRUE or FALSE values. We’ll limit the scope further to only consider simple linear regression and simple logistic regression. This means you only have a single explanatory variable.\nit’s a good idea to visualize your dataset. To visualize the relationship between two numeric variables, you can use a scatter plot.\nadd a trend line to the scatter plot. A trend line means fitting a line that follows the data points. In ggplot, trend lines are added using geom_smooth(). Setting the method argument to “lm”, for “linear model” gives a trend line calculated with a linear regression.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(fst)\n\ntaiwan_real_estate <- read.fst(\"data/taiwan_real_estate.fst\")\n\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq))+\n  geom_point(alpha = 0.5)+\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n\nStraight lines are completely defined by two properties. The intercept is the y value when x is zero. The slope is the steepness of the line, equal to the amount y increases if you increase x by one. \\[ y = intercept + slope * x \\]\n\nmdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\nThe intercept is positive, so a house with no convenience stores nearby still has a positive price. The coefficient for convenience stores is also positive, so as the number of nearby convenience stores increases, so does the price of the house.\n\n6.5.1 Categorical explanatory variables\nTo visualize the data, scatter plots aren’t ideal because species is categorical. Instead, we can draw a histogram for each of the species.\nLet’s calculate some summary statistics. First we group by species then we summarize to calculate their mean masses.\n\n# Using taiwan_real_estate, plot price_twd_msq\nggplot(taiwan_real_estate, aes(price_twd_msq)) +\n  # Make it a histogram with 10 bins\n  geom_histogram(bins = 10) +\n  # Facet the plot so each house age group gets its own panel\n  facet_wrap(~ house_age_years)\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\nCalculating means by category\n\nsummary_stats <- taiwan_real_estate %>% \n  # Group by house age\n  group_by(house_age_years) %>% \n  # Summarize to calculate the mean house price/area\n  summarize(mean_by_group = mean(price_twd_msq))\n\n# See the result\nsummary_stats\n\nlm() with a categorical explanatory variable\n\n# Run a linear regression of price_twd_msq vs. house_age_years\nmdl_price_vs_age <- lm(\n  price_twd_msq ~ house_age_years, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_age\n\n\n# Update the model formula to remove the intercept\nmdl_price_vs_age_no_intercept <- lm(\n  price_twd_msq ~ house_age_years +0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_age_no_intercept\n\nThis is a reassuringly boring result. When you only have a single, categorical explanatory variable, the linear regression coefficients are the means of each category.\n\n\n6.5.2 Predictions\nThe big benefit of running models rather than simply calculating descriptive statistics is that models let you make predictions. Before we can make predictions, we need a model.\nThe principle behind predicting is to ask questions of the form “if I set the explanatory variables to these values, what value would the response variable have?”.\n\nlibrary(tibble)\n# From previous steps\nexplanatory_data <- tibble(\n  n_convenience = 0:10\n)\n\n# Edit this, so predictions are stored in prediction_data\npredict(mdl_price_vs_conv, explanatory_data)\n\n# See the result\nprediction_data <- explanatory_data %>%\nmutate(\n  price_twd_msq = predict(mdl_price_vs_conv, explanatory_data)\n)\n\nHaving the predictions in a data frame will make it easier to visualize them.\n\n# Add to the plot\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add a point layer of prediction data, colored yellow\n  geom_point(\n    data = prediction_data, color = \"yellow\"\n  )\n\n\nsummary(mdl_price_vs_conv)\n\n\n\n6.5.3 Using Broom Package\n\nlibrary(broom)\ntidy(mdl_price_vs_conv)\naugment(mdl_price_vs_conv)\nglance(mdl_price_vs_conv)\n\n\n\n6.5.4 Assessing model fit\nQuantifyig model fit\n\nr-squared or the coefficient of determination\n\nIt is defined as the proportion of the variance in the response variable that is predictable from the explanatory variable.\nglance from broom and call r-squared\nFor simple linear regression, the interpretation of the coefficient of determination is straightforward. It is simply the correlation between the explanatory and response variables, squared.\n\nResidual standard error (RSE)\n\nThe RSE is, very roughly speaking, a measure of the typical size of the residuals. That is, how much the predictions are typically wrong by. It has the same unit as the response variable.\nResidual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it’s a measure of how badly wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.\n\nmdl_price_vs_conv %>% \n  glance() %>% \n  pull(sigma, r.squared)\n# sigma is RSE\n\n\nRoot-mean-square error (RMSE)\n\nYou need to be aware that RMSE exists, but typically you should use RSE instead.\n\nad_conversion <- read.fst(\"data/ad_conversion.fst\")\n\nmdl_nclick_impression <- lm(n_impressions ~ n_clicks, data = ad_conversion)\n\nmdl_nclick_impression %>% \n  glance() %>% \n  pull(sigma)\n\n\nlibrary(ggfortify)\nautoplot(mdl_nclick_impression, which= 1:3, nrow = 3, ncol =1)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Keizer, R J, M O Karlsson, and A Hooker. 2013. “Modeling and\nSimulation Workbench for NONMEM: Tutorial on Pirana, PsN, and\nXpose.” CPT: Pharmacometrics & Systems Pharmacology\n2 (6): e50. https://doi.org/10.1038/psp.2013.24.\n\n\nKeutzer, Lina, Huifang You, Ali Farnoud, Joakim Nyberg, Sebastian G.\nWicha, Gareth Maher-Edwards, Georgios Vlasakakis, et al. 2022.\n“Machine Learning and Pharmacometrics for Prediction of\nPharmacokinetic Data: Differences, Similarities and Challenges\nIllustrated with Rifampicin.” Pharmaceutics 14 (8):\n1530. https://doi.org/10.3390/pharmaceutics14081530."
  }
]